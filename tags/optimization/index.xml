<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Optimization on Practical ML</title>
    <link>https://arikpoz.github.io/tags/optimization/</link>
    <description>Recent content in Optimization on Practical ML</description>
    <image>
      <title>Practical ML</title>
      <url>https://arikpoz.github.io/images/papermod-cover.png</url>
      <link>https://arikpoz.github.io/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- 0.147.2</generator>
    <language>en</language>
    <lastBuildDate>Wed, 07 May 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://arikpoz.github.io/tags/optimization/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Faster Models with Graph Fusion: How Deep Learning Frameworks Optimize Your Computation</title>
      <link>https://arikpoz.github.io/posts/2025-05-07-faster-models-with-graph-fusion-how-deep-learning-frameworks-optimize-your-computation/</link>
      <pubDate>Wed, 07 May 2025 00:00:00 +0000</pubDate>
      <guid>https://arikpoz.github.io/posts/2025-05-07-faster-models-with-graph-fusion-how-deep-learning-frameworks-optimize-your-computation/</guid>
      <description>&lt;p&gt;&lt;img alt=&#34;&amp;ldquo;An illustration of graph fusion.&amp;rdquo;&#34; loading=&#34;lazy&#34; src=&#34;https://arikpoz.github.io/posts/2025-05-07-faster-models-with-graph-fusion-how-deep-learning-frameworks-optimize-your-computation/lead-image.jpg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Modern deep learning models are made up of hundreds or even thousands of operations. Each of these operations involves memory reads, computation, and memory writes, which when executed individually leads to substantial overhead. One of the most effective ways to cut down this overhead and boost performance is through &lt;strong&gt;graph fusion&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Graph fusion&lt;/strong&gt;, also known as operation fusion or kernel fusion, refers to the process of merging multiple operations into a single, more efficient kernel. By combining adjacent operations like a convolution followed by batch normalization and a ReLU activation into one fused unit, deep learning frameworks can avoid unnecessary memory access, reduce kernel launch overhead, and take better advantage of hardware capabilities.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Low-Rank Factorization in PyTorch: Compressing Neural Networks with Linear Algebra</title>
      <link>https://arikpoz.github.io/posts/2025-04-29-low-rank-factorization-in-pytorch-compressing-neural-networks-with-linear-algebra/</link>
      <pubDate>Tue, 29 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://arikpoz.github.io/posts/2025-04-29-low-rank-factorization-in-pytorch-compressing-neural-networks-with-linear-algebra/</guid>
      <description>&lt;p&gt;&lt;img alt=&#34;&amp;ldquo;Diagram of low-rank factorization compressing ResNet50, showing 93% size reduction, 10% lower latency, and 3.5% accuracy drop.&amp;rdquo;&#34; loading=&#34;lazy&#34; src=&#34;https://arikpoz.github.io/posts/2025-04-29-low-rank-factorization-in-pytorch-compressing-neural-networks-with-linear-algebra/lead-image.jpg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Can we shrink neural networks without sacrificing much accuracy?
Low-rank factorization is a powerful, often overlooked technique that compresses models by decomposing large weight matrices into smaller components.&lt;/p&gt;
&lt;p&gt;In this post, we&amp;rsquo;ll explain what low-rank factorization is, show how to apply it to a ResNet50 model in PyTorch, and evaluate the trade-offs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Knowledge Distillation in PyTorch: Shrinking Neural Networks the Smart Way</title>
      <link>https://arikpoz.github.io/posts/2025-04-24-knowledge-distillation-in-pytorch-shrinking-neural-networks-the-smart-way/</link>
      <pubDate>Thu, 24 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://arikpoz.github.io/posts/2025-04-24-knowledge-distillation-in-pytorch-shrinking-neural-networks-the-smart-way/</guid>
      <description>&lt;p&gt;&lt;img alt=&#34;&amp;ldquo;A glowing teacher neural network transferring knowledge to a smaller student model, with the title &amp;lsquo;Knowledge Distillation&amp;rsquo; overlaid.&amp;rdquo;&#34; loading=&#34;lazy&#34; src=&#34;https://arikpoz.github.io/posts/2025-04-24-knowledge-distillation-in-pytorch-shrinking-neural-networks-the-smart-way/lead-image.jpg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;What if your model could run twice as fast and use half the memory, without giving up much accuracy?&lt;/strong&gt;&lt;br&gt;
This is the promise of &lt;strong&gt;knowledge distillation&lt;/strong&gt;: training smaller, faster models to mimic larger, high-performing ones. In this post, we’ll walk through how to distill a powerful ResNet50 model into a lightweight ResNet18 and demonstrate a &lt;strong&gt;+5% boost in accuracy&lt;/strong&gt; compared to training the smaller model from scratch, all while cutting inference latency by over &lt;strong&gt;50%&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Neural Network Quantization in PyTorch</title>
      <link>https://arikpoz.github.io/posts/2025-04-16-neural-network-quantization-in-pytorch/</link>
      <pubDate>Wed, 16 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://arikpoz.github.io/posts/2025-04-16-neural-network-quantization-in-pytorch/</guid>
      <description>&lt;p&gt;&lt;img alt=&#34;image here&amp;quot;&#34; loading=&#34;lazy&#34; src=&#34;https://arikpoz.github.io/posts/2025-04-16-neural-network-quantization-in-pytorch/lead-image.jpg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This tutorial provides an introduction to quantization in PyTorch, covering both theory and practice. We’ll explore the different types of quantization, and apply both post training quantization (PTQ) and quantization aware training (QAT) on a simple example using CIFAR-10 and ResNet18. In the presented example we achieve a &lt;strong&gt;75% reduction in space&lt;/strong&gt; and &lt;strong&gt;16% reduction in GPU latency&lt;/strong&gt; with only 1% drop in accuracy.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-quantization&#34;&gt;What is Quantization?&lt;/h2&gt;
&lt;p&gt;Quantization is a model optimization technique that reduces the numerical precision used to represent weights and activations in deep learning models. Its primary benefits include:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Neural Network Pruning: How to Accelerate Inference with Minimal Accuracy Loss</title>
      <link>https://arikpoz.github.io/posts/2025-04-10-neural-network-pruning-how-to-accelerate-inference-with-minimal-accuracy-loss/</link>
      <pubDate>Thu, 10 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://arikpoz.github.io/posts/2025-04-10-neural-network-pruning-how-to-accelerate-inference-with-minimal-accuracy-loss/</guid>
      <description>&lt;p&gt;&lt;img alt=&#34;Cartoon of a person in an &amp;ldquo;NN Pruning&amp;rdquo; shirt trimming a large robot labeled &amp;ldquo;Neural Network&amp;rdquo; into a slim, fast robot labeled &amp;ldquo;Pruned Network.&amp;rdquo;&#34; loading=&#34;lazy&#34; src=&#34;https://arikpoz.github.io/posts/2025-04-10-neural-network-pruning-how-to-accelerate-inference-with-minimal-accuracy-loss/lead-image.jpg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this post, I will demonstrate how to use pruning to &lt;strong&gt;significantly reduce a model&amp;rsquo;s size and latency&lt;/strong&gt; while maintaining minimal accuracy loss. In the example, we achieve a &lt;strong&gt;90% reduction in model size&lt;/strong&gt; and &lt;strong&gt;5.5x faster inference time&lt;/strong&gt;, all while preserving the same level of accuracy.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fast Image Loading with NVIDIA nvImageCodec</title>
      <link>https://arikpoz.github.io/posts/2025-04-07-fast-image-loading-with-nvidia-nvimagecodec/</link>
      <pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://arikpoz.github.io/posts/2025-04-07-fast-image-loading-with-nvidia-nvimagecodec/</guid>
      <description>&lt;p&gt;&lt;img alt=&#34;OpenCV turtle loses the race with nvImageCodec rabbit&#34; loading=&#34;lazy&#34; src=&#34;https://arikpoz.github.io/posts/2025-04-07-fast-image-loading-with-nvidia-nvimagecodec/lead-image.jpg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In deep learning pipelines, especially those involving image data, &lt;strong&gt;data loading and preprocessing&lt;/strong&gt; often become major bottlenecks. Traditionally, image decoding is performed using libraries like &lt;a href=&#34;https://docs.opencv.org/4.x/index.html&#34;&gt;OpenCV&lt;/a&gt; or &lt;a href=&#34;https://pillow.readthedocs.io/en/stable/&#34;&gt;Pillow&lt;/a&gt;, which rely on CPU-based processing. After decoding, the data must be transferred to GPU memory for further operations. But what if the decoding process itself could be performed directly on the GPU? Could this lead to faster performance?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introduction to Model Compression: Why and How to Shrink Neural Networks for Speed</title>
      <link>https://arikpoz.github.io/posts/2025-04-02-introduction-to-model-compression-why-and-how-to-shrink-neural-networks-for-speed/</link>
      <pubDate>Wed, 02 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://arikpoz.github.io/posts/2025-04-02-introduction-to-model-compression-why-and-how-to-shrink-neural-networks-for-speed/</guid>
      <description>&lt;p&gt;&lt;img alt=&#34;Glowing digital brain shrinking in size, symbolizing faster, smaller neural networks&#34; loading=&#34;lazy&#34; src=&#34;https://arikpoz.github.io/posts/2025-04-02-introduction-to-model-compression-why-and-how-to-shrink-neural-networks-for-speed/lead-image.jpg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Deep learning models have grown increasingly large and complex, enabling state-of-the-art performance in tasks such as image recognition, natural language processing, and generative AI. However, these large models often come with high computational costs, making them slow to run on edge devices, embedded systems, or even in cloud environments with strict latency requirements.&lt;/p&gt;
&lt;p&gt;Model compression techniques aim to reduce the size and computational requirements of neural networks while maintaining their accuracy. This enables faster inference, lower power consumption, and better deployment flexibility. In this post, we’ll explore why model compression is essential and provide an overview of four key techniques: &lt;strong&gt;pruning, quantization, knowledge distillation, and low-rank factorization&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to Make Your Neural Network Run Faster: An Overview of Optimization Techniques</title>
      <link>https://arikpoz.github.io/posts/2025-03-30-how-to-make-your-neural-network-run-faster-an-overview-of-optimization-techniques/</link>
      <pubDate>Sun, 30 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://arikpoz.github.io/posts/2025-03-30-how-to-make-your-neural-network-run-faster-an-overview-of-optimization-techniques/</guid>
      <description>&lt;p&gt;&lt;img alt=&#34;Illustration of a neural network transforming into a faster, optimized version&#34; loading=&#34;lazy&#34; src=&#34;https://arikpoz.github.io/posts/2025-03-30-how-to-make-your-neural-network-run-faster-an-overview-of-optimization-techniques/lead-image.jpg&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Neural networks are becoming increasingly powerful, but speed remains a crucial factor in real-world applications. Whether you&amp;rsquo;re running models on the cloud, edge devices, or personal hardware, optimizing them for speed can lead to faster inference, lower latency, and reduced resource consumption.&lt;/p&gt;
&lt;p&gt;In this post, we&amp;rsquo;ll explore various techniques to accelerate neural networks, from model compression to hardware optimizations. This will serve as a foundation for future deep dives into each method.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
