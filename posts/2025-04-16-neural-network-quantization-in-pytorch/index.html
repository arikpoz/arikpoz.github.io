<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Neural Network Quantization in PyTorch | Practical ML</title>
<meta name="keywords" content="">
<meta name="description" content="
Introduction
This tutorial provides an introduction to quantization in PyTorch, covering both theory and practice. We’ll explore the different types of quantization, and apply both post training quantization (PTQ) and quantization aware training (QAT) on a simple example using CIFAR-10 and ResNet18. In the presented example we achieve a 75% reduction in space and 16% reduction in GPU latency with only 1% drop in accuracy.

What is Quantization?
Quantization is a model optimization technique that reduces the numerical precision used to represent weights and activations in deep learning models. Its primary benefits include:">
<meta name="author" content="Arik Poznanski">
<link rel="canonical" href="https://arikpoz.github.io/posts/2025-04-16-neural-network-quantization-in-pytorch/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css" integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF&#43;13Dyqob6ASlTrTye8=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://arikpoz.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://arikpoz.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://arikpoz.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://arikpoz.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://arikpoz.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://arikpoz.github.io/posts/2025-04-16-neural-network-quantization-in-pytorch/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<meta name="google-site-verification" content="W9Zr2fU6GN7Pj5eoP80QHjzCMltqYsT_ut_zg5S8FP8" />


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true},{left: '$', right: '$', display: false}]});">
</script>

      <script async src="https://www.googletagmanager.com/gtag/js?id=G-KDV9XDG1TJ"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-KDV9XDG1TJ');
        }
      </script>






</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://arikpoz.github.io/" accesskey="h" title="Practical ML (Alt + H)">Practical ML</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://arikpoz.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://arikpoz.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://arikpoz.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Neural Network Quantization in PyTorch
    </h1>
    <div class="post-meta"><span title='2025-04-16 00:00:00 +0000 UTC'>April 16, 2025</span>&nbsp;·&nbsp;16 min&nbsp;·&nbsp;Arik Poznanski

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#what-is-quantization" aria-label="What is Quantization?">What is Quantization?</a></li>
                <li>
                    <a href="#why-quantize-weights-and-activations" aria-label="Why Quantize Weights and Activations?">Why Quantize Weights and Activations?</a><ul>
                        
                <li>
                    <a href="#why-quantize-weights" aria-label="Why Quantize Weights?">Why Quantize Weights?</a></li>
                <li>
                    <a href="#why-quantize-activations" aria-label="Why Quantize Activations?">Why Quantize Activations?</a></li></ul>
                </li>
                <li>
                    <a href="#types-of-quantization" aria-label="Types of Quantization">Types of Quantization</a><ul>
                        
                <li>
                    <a href="#floating-point-quantization" aria-label="Floating-Point Quantization">Floating-Point Quantization</a></li>
                <li>
                    <a href="#integer-quantization" aria-label="Integer Quantization">Integer Quantization</a></li>
                <li>
                    <a href="#how-are-scale-and-zero-point-determined-calibration" aria-label="How Are Scale and Zero-Point Determined? (Calibration)">How Are Scale and Zero-Point Determined? (Calibration)</a></li>
                <li>
                    <a href="#tradeoffs" aria-label="Tradeoffs">Tradeoffs</a></li></ul>
                </li>
                <li>
                    <a href="#when-is-quantization-applied" aria-label="When Is Quantization Applied?">When Is Quantization Applied?</a><ul>
                        
                <li>
                    <a href="#post-training-quantization-ptq" aria-label="Post Training Quantization (PTQ)">Post Training Quantization (PTQ)</a></li>
                <li>
                    <a href="#quantization-aware-training-qat" aria-label="Quantization Aware Training (QAT)">Quantization Aware Training (QAT)</a></li></ul>
                </li>
                <li>
                    <a href="#code-walkthrough" aria-label="Code Walkthrough">Code Walkthrough</a><ul>
                        
                <li>
                    <a href="#shared-code" aria-label="Shared code">Shared code</a><ul>
                        
                <li>
                    <a href="#basic-setup" aria-label="Basic Setup">Basic Setup</a></li>
                <li>
                    <a href="#get-cifar-10-train-and-test-sets" aria-label="Get CIFAR-10 train and test sets">Get CIFAR-10 train and test sets</a></li>
                <li>
                    <a href="#adjust-resnet18-network-for-cifar-10-dataset" aria-label="Adjust ResNet18 network for CIFAR-10 dataset">Adjust ResNet18 network for CIFAR-10 dataset</a></li>
                <li>
                    <a href="#define-train-and-evaluate-functions" aria-label="Define Train and Evaluate functions">Define Train and Evaluate functions</a></li>
                <li>
                    <a href="#define-helper-functions-to-measure-latency" aria-label="Define helper functions to measure latency">Define helper functions to measure latency</a></li>
                <li>
                    <a href="#train-full-model" aria-label="Train full model">Train full model</a></li>
                <li>
                    <a href="#evaluate-full-model" aria-label="Evaluate full model">Evaluate full model</a></li></ul>
                </li>
                <li>
                    <a href="#post-training-quantization-ptq-1" aria-label="Post Training Quantization (PTQ)">Post Training Quantization (PTQ)</a><ul>
                        
                <li>
                    <a href="#evaluate-quantized-model" aria-label="Evaluate quantized model">Evaluate quantized model</a></li>
                <li>
                    <a href="#optimize-quantized-model-for-inference" aria-label="Optimize quantized model for inference">Optimize quantized model for inference</a></li>
                <li>
                    <a href="#evaluate-optimized-model" aria-label="Evaluate optimized model">Evaluate optimized model</a></li></ul>
                </li>
                <li>
                    <a href="#quantization-aware-training-qat-1" aria-label="Quantization Aware Training (QAT)">Quantization Aware Training (QAT)</a><ul>
                        
                <li>
                    <a href="#evaluate-quantized-model-1" aria-label="Evaluate quantized model">Evaluate quantized model</a></li>
                <li>
                    <a href="#optimize-quantized-model-for-inference-1" aria-label="Optimize quantized model for inference">Optimize quantized model for inference</a></li>
                <li>
                    <a href="#evaluate-optimized-model-1" aria-label="Evaluate optimized model">Evaluate optimized model</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#comparison-of-ptq-and-qat-results" aria-label="Comparison of PTQ and QAT Results">Comparison of PTQ and QAT Results</a></li>
                <li>
                    <a href="#summary" aria-label="Summary">Summary</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p><img alt="image here&quot;" loading="lazy" src="/posts/2025-04-16-neural-network-quantization-in-pytorch/lead-image.jpg"></p>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>This tutorial provides an introduction to quantization in PyTorch, covering both theory and practice. We’ll explore the different types of quantization, and apply both post training quantization (PTQ) and quantization aware training (QAT) on a simple example using CIFAR-10 and ResNet18. In the presented example we achieve a <strong>75% reduction in space</strong> and <strong>16% reduction in GPU latency</strong> with only 1% drop in accuracy.</p>
<hr>
<h2 id="what-is-quantization">What is Quantization?<a hidden class="anchor" aria-hidden="true" href="#what-is-quantization">#</a></h2>
<p>Quantization is a model optimization technique that reduces the numerical precision used to represent weights and activations in deep learning models. Its primary benefits include:</p>
<ol>
<li><strong>Model Compression</strong> - lowers memory usage and storage.</li>
<li><strong>Inference Acceleration</strong> - speeds up inference and reduces energy consumption.</li>
</ol>
<p>While quantization is most often used for deployment on edge devices (e.g., phones, embedded hardware), it can also reduce infrastructure costs for large-scale inference in the cloud.</p>
<h2 id="why-quantize-weights-and-activations">Why Quantize Weights and Activations?<a hidden class="anchor" aria-hidden="true" href="#why-quantize-weights-and-activations">#</a></h2>
<p>Quantization typically targets both <strong>weights</strong> and <strong>activations</strong>, and each serves a different purpose in optimizing model deployment:</p>
<h3 id="why-quantize-weights">Why Quantize Weights?<a hidden class="anchor" aria-hidden="true" href="#why-quantize-weights">#</a></h3>
<ul>
<li><strong>Storage savings</strong>: Weights are the learned parameters of a model and are saved to disk. Reducing their precision (e.g., from <code>float32</code> to <code>int8</code>) significantly shrinks the size of the model file.</li>
<li><strong>Faster model loading</strong>: Smaller weights reduce model loading time, which is especially useful for mobile and edge deployments.</li>
<li><strong>Reduced memory footprint</strong>: On-device memory use is lower, which allows running larger models or multiple models concurrently.</li>
</ul>
<h3 id="why-quantize-activations">Why Quantize Activations?<a hidden class="anchor" aria-hidden="true" href="#why-quantize-activations">#</a></h3>
<ul>
<li><strong>Runtime efficiency</strong>: Activations are the intermediate outputs of each layer computed during the forward pass. Lower-precision activations (e.g., <code>int8</code> instead of <code>float32</code>) require less memory bandwidth and compute.</li>
<li><strong>End-to-end low-precision execution</strong>: Quantizing both weights and activations enables optimized hardware kernels (e.g., <code>int8</code> × <code>int8</code> → <code>int32</code>) to be used throughout the network, maximizing speed and energy efficiency.</li>
<li><strong>Better cache locality</strong>: Smaller activation tensors are more cache-friendly, leading to faster inference.</li>
</ul>
<p>Quantizing only the weights can reduce model size but won’t deliver full runtime acceleration. Quantizing both weights and activations is essential to fully unlock the benefits of quantized inference on CPUs, mobile chips, and specialized accelerators.</p>
<h2 id="types-of-quantization">Types of Quantization<a hidden class="anchor" aria-hidden="true" href="#types-of-quantization">#</a></h2>
<p>The two most common approaches to quantization fall into these categories:</p>
<h3 id="floating-point-quantization">Floating-Point Quantization<a hidden class="anchor" aria-hidden="true" href="#floating-point-quantization">#</a></h3>
<p>Floating-point quantization reduces the bit-width of real-valued tensors, typically from 32-bit (<code>float32</code>) to 16-bit (<code>float16</code> or <code>bfloat16</code>). These formats use fewer bits for the exponent and mantissa, resulting in lower precision but maintaining the continuous range and general expressiveness of real numbers.</p>
<ul>
<li>Uses 16 bits instead of 32 (e.g., <code>float16</code>, <code>bfloat16</code>).</li>
<li>Preserves dynamic range and real-number structure.</li>
<li>Maintains relatively high accuracy.</li>
<li>Supported efficiently on modern hardware (e.g., GPUs, TPUs).</li>
</ul>
<p>The diagram below compares the internal bit layout of <code>float32</code> , <code>float16</code> , and <code>bfloat16</code> using color-coded segments for the sign, exponent, and mantissa bits:</p>
<p><img alt="bfloat16 layout&quot;" loading="lazy" src="/posts/2025-04-16-neural-network-quantization-in-pytorch/bfloat16-layout.png"></p>
<p><code>bfloat16</code> , developed by <a href="https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus">Google Brain</a>, is especially notable because it retains the full 8-bit exponent of <code>float32</code> , offering a wide dynamic range. While its 7-bit mantissa provides less precision, this makes it more numerically stable than <code>float16</code> , particularly for training deep networks</p>
<h3 id="integer-quantization">Integer Quantization<a hidden class="anchor" aria-hidden="true" href="#integer-quantization">#</a></h3>
<p>Integer quantization maps real-valued numbers to a discrete integer range using an affine transformation. This process enables efficient inference using low-precision arithmetic.</p>
<p><strong>Quantization:</strong> $q = \text{round}\left(\frac{x}{s}\right) + z$</p>
<p><strong>Dequantization:</strong> $x \approx s \cdot (q - z)$</p>
<p>Where:</p>
<ul>
<li><code>x</code> is the original float</li>
<li><code>q</code> is the quantized integer</li>
<li><code>s</code> is the scale (a float)</li>
<li><code>z</code> is the zero-point (an int)</li>
</ul>
<p>These mappings let the model operate primarily with integers during inference, reducing memory usage and enabling faster execution on integer-optimized hardware.</p>
<h3 id="how-are-scale-and-zero-point-determined-calibration">How Are Scale and Zero-Point Determined? (Calibration)<a hidden class="anchor" aria-hidden="true" href="#how-are-scale-and-zero-point-determined-calibration">#</a></h3>
<p>The scale and zero-point are calculated based on the distribution of float values in a tensor. Typically:</p>
<ul>
<li>
<p><strong>Scale (</strong><code>s</code> <strong>)</strong> is derived from the min and max float values of the tensor, and the min and max values of the quantized range (0-255 for <code>uint8</code> or -128 to 127 for <code>int8</code>)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">s = (x_max - x_min) / (q_max - q_min)
</span></span></code></pre></div></li>
<li>
<p><strong>Zero-point (</strong><code>z</code> <strong>)</strong> ensures that 0 is exactly representable:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">z = round(q_min - x_min / s)
</span></span></code></pre></div></li>
</ul>
<p>This process of determining the appropriate scale and zero-point by observing real-valued data flowing through the model is known as <strong>calibration</strong>. It is especially important for static quantization, where activation ranges are fixed based on representative input data.</p>
<p>These parameters are then stored along with each quantized tensor. There are two main approaches:</p>
<ul>
<li><strong>Per-tensor quantization</strong>: One scale and zero-point for the entire tensor.</li>
<li><strong>Per-channel quantization</strong>: Separate scale and zero-point per output channel (commonly used for weights in convolutional layers).</li>
</ul>
<p>During inference, these values are used to convert between quantized and real representations efficiently. Some characteristics: </p>
<ul>
<li>Aggressive memory/computation savings.</li>
<li>May introduce more quantization error.</li>
<li>Commonly used in edge-optimized frameworks like TensorFlow Lite and PyTorch Mobile.</li>
</ul>
<h3 id="tradeoffs">Tradeoffs<a hidden class="anchor" aria-hidden="true" href="#tradeoffs">#</a></h3>
<p>Quantization enables efficient inference but can degrade accuracy, especially if done post training without calibration. To minimize this, modern techniques like Quantization Aware Training (QAT) are used, see below.</p>
<hr>
<h2 id="when-is-quantization-applied">When Is Quantization Applied?<a hidden class="anchor" aria-hidden="true" href="#when-is-quantization-applied">#</a></h2>
<p>Quantization can be applied at different stages in the model lifecycle. The two primary approaches are Post Training Quantization (PTQ) and Quantization Aware Training (QAT), each with its own benefits and tradeoffs.</p>
<h3 id="post-training-quantization-ptq">Post Training Quantization (PTQ)<a hidden class="anchor" aria-hidden="true" href="#post-training-quantization-ptq">#</a></h3>
<p>PTQ is applied to a fully trained model without requiring any retraining. It’s simple and quick to implement, but may cause some degradation in model accuracy, especially when using aggressive quantization like <code>int8</code>.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li>Easy to integrate into existing workflows</li>
<li>No need to modify training code</li>
<li>Can dramatically reduce model size and inference cost</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Accuracy may drop, especially for sensitive models or tasks</li>
<li>Works best on models that are already robust to small numeric changes</li>
</ul>
<p><strong>Variants:</strong></p>
<ul>
<li>
<p><strong>Dynamic Quantization</strong>:</p>
<ul>
<li><strong>When?</strong> After training.</li>
<li><strong>What?</strong> Only weights are quantized and stored in <code>int8</code>. Activations remain in float and are quantized dynamically during inference.</li>
<li><strong>How?</strong> No calibration needed. Activation ranges are computed on-the-fly at runtime.</li>
<li><strong>Pros:</strong> Easy to apply; works well for models with large <code>nn.Linear</code> layers (e.g., NLP).</li>
<li><strong>Cons:</strong> Some operations still use float intermediates; less efficient than static quantization.</li>
</ul>
</li>
<li>
<p><strong>Static Quantization</strong>:</p>
<ul>
<li><strong>When?</strong> After training.</li>
<li><strong>What?</strong> Both weights and activations are quantized to <code>int8</code>.</li>
<li><strong>How?</strong> Requires calibration, passing representative data through the model to collect activation stats.</li>
<li><strong>Pros:</strong> Enables full integer inference; maximizes performance.</li>
<li><strong>Cons:</strong> Slightly more setup due to calibration requirement.</li>
</ul>
</li>
<li>
<p><strong>Weight-only Quantization</strong>:</p>
<ul>
<li><strong>When?</strong> After training.</li>
<li><strong>What?</strong> Only weights are quantized; activations remain <code>float32</code>.</li>
<li><strong>How?</strong> No activation quantization, so no calibration needed.</li>
<li><strong>Pros:</strong> Reduces model size.</li>
<li><strong>Cons:</strong> Limited inference speedup since activations are still float. Only weights are quantized; activations remain in float. Saves memory, but yields limited inference speedup.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="quantization-aware-training-qat">Quantization Aware Training (QAT)<a hidden class="anchor" aria-hidden="true" href="#quantization-aware-training-qat">#</a></h3>
<p>QAT introduces quantization effects during the training process by simulating them using fake quantization operations. These operations emulate the behavior of quantization during the forward pass (e.g., rounding, clamping) while still allowing gradients to flow through in full precision during the backward pass. This enables the model to learn to be robust to quantization effects while maintaining effective training dynamics.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li>Highest accuracy among quantized models</li>
<li>Especially useful for smaller or sensitive models that suffer from PTQ degradation</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Requires retraining or fine-tuning</li>
<li>Slightly slower training due to added quantization simulation steps</li>
</ul>
<p>QAT is particularly effective for compact architectures like MobileNet or for models deployed on edge devices where low-precision inference is essential and even small drops in accuracy can be problematic. (like MobileNet) or models deployed in latency-sensitive, low-precision environments (e.g., mobile or embedded devices).</p>
<hr>
<h2 id="code-walkthrough">Code Walkthrough<a hidden class="anchor" aria-hidden="true" href="#code-walkthrough">#</a></h2>
<p>In this section I will provide a complete example of applying both <strong>Post Training Quantization (PTQ)</strong> and <strong>Quantization Aware Training (QAT)</strong> to a ResNet18 model adjusted for CIFAR-10 dataset. The code was tested to work on PyTorch 2.4 through 2.8 (nightly build) using both X86 Quantizer for CPU deployments and XNNPACK Quantizer used for mobile and edge devices. You can find the full self-contained jupyter notebooks below, or in the <a href="https://github.com/arikpoz/neural-network-optimization">Neural Network Optimization GitHub repository</a>.</p>
<ul>
<li><a href="https://github.com/arikpoz/neural-network-optimization/blob/main/Quantization%20-%20PTQ%20using%20PyTorch%202%20Export%20Quantization%20and%20X86%20Backend.ipynb">Quantization - PTQ using PyTorch 2 Export Quantization and X86 Backend</a></li>
<li><a href="https://github.com/arikpoz/neural-network-optimization/blob/main/Quantization%20-%20QAT%20using%20PyTorch%202%20Export%20Quantization%20and%20X86%20Backend.ipynb">Quantization - QAT using PyTorch 2 Export Quantization and X86 Backend</a></li>
<li><a href="https://github.com/arikpoz/neural-network-optimization/blob/main/Quantization%20-%20PTQ%20using%20PyTorch%202%20Export%20Quantization%20and%20XNNPACK%20Quantizer.ipynb">Quantization - PTQ using PyTorch 2 Export Quantization and XNNPACK Quantizer</a></li>
<li><a href="https://github.com/arikpoz/neural-network-optimization/blob/main/Quantization%20-%20QAT%20using%20PyTorch%202%20Export%20Quantization%20and%20XNNPACK%20Quantizer.ipynb">Quantization - QAT using PyTorch 2 Export Quantization and XNNPACK Quantizer</a></li>
</ul>
<p>Below I will go over the code for PTQ and QAT for the X86 scenario, as the edge device case is very similar.</p>
<h3 id="shared-code">Shared code<a hidden class="anchor" aria-hidden="true" href="#shared-code">#</a></h3>
<p>We start with defining some code to get the CIFAR-10 dataset, adjust the ResNet18 model, and define training and evaluation functions to measure the model&rsquo;s size, accuracy, and latency.
We end this section with the training and evaluation of the baseline model, before quantization.</p>
<h4 id="basic-setup">Basic Setup<a hidden class="anchor" aria-hidden="true" href="#basic-setup">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">time</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">warnings</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">packaging</span> <span class="kn">import</span> <span class="n">version</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span><span class="p">,</span> <span class="n">models</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.quantization</span> <span class="kn">import</span> <span class="n">quantize_dynamic</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.ao.quantization</span> <span class="kn">import</span> <span class="n">get_default_qconfig</span><span class="p">,</span> <span class="n">QConfigMapping</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.ao.quantization.quantize_fx</span> <span class="kn">import</span> <span class="n">prepare_fx</span><span class="p">,</span> <span class="n">convert_fx</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Subset</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># ignores irrelevant warning, see: https://github.com/pytorch/pytorch/issues/149829</span>
</span></span><span class="line"><span class="cl"><span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&#34;ignore&#34;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&#34;.*TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support.*&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># ignores irrelevant warning, see: https://github.com/tensorflow/tensorflow/issues/77293</span>
</span></span><span class="line"><span class="cl"><span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&#34;ignore&#34;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&#34;.*erase_node(.*) on an already erased node.*&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;PyTorch Version: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;cuda&#34;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&#34;cpu&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Device used: </span><span class="si">{</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">skip_cpu</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># change to True to skip the slow checks on CPU</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Should skip CPU evaluations: </span><span class="si">{</span><span class="n">skip_cpu</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="get-cifar-10-train-and-test-sets">Get CIFAR-10 train and test sets<a hidden class="anchor" aria-hidden="true" href="#get-cifar-10-train-and-test-sets">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,))</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&#34;./data&#34;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&#34;./data&#34;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&#34;./data&#34;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&#34;./data&#34;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">calibration_dataset</span> <span class="o">=</span> <span class="n">Subset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">256</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">calibration_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">calibration_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="adjust-resnet18-network-for-cifar-10-dataset">Adjust ResNet18 network for CIFAR-10 dataset<a hidden class="anchor" aria-hidden="true" href="#adjust-resnet18-network-for-cifar-10-dataset">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_resnet18_for_cifar10</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns a ResNet-18 model adjusted for CIFAR-10:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - 3x3 conv with stride 1
</span></span></span><span class="line"><span class="cl"><span class="s2">    - No max pooling
</span></span></span><span class="line"><span class="cl"><span class="s2">    - 10 output classes
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model_to_quantize</span> <span class="o">=</span> <span class="n">get_resnet18_for_cifar10</span><span class="p">()</span>
</span></span></code></pre></div><h4 id="define-train-and-evaluate-functions">Define Train and Evaluate functions<a hidden class="anchor" aria-hidden="true" href="#define-train-and-evaluate-functions">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">save_path</span><span class="o">=</span><span class="s2">&#34;model.pth&#34;</span><span class="p">,</span> <span class="n">silent</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Trains a model with SGD and cross-entropy loss.
</span></span></span><span class="line"><span class="cl"><span class="s2">    Loads from save_path if it exists.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">except</span> <span class="ne">NotImplementedError</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">move_exported_model_to_train</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">save_path</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">silent</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Model already trained. Loading from </span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">save_path</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># no saved model found. training from given model state</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">silent</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: loss=</span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&#34;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">except</span> <span class="ne">NotImplementedError</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">move_exported_model_to_train</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">save_path</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">save_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">silent</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Training complete. Model saved to </span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tag</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Evaluates the model on test_loader and prints accuracy.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">except</span> <span class="ne">NotImplementedError</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">move_exported_model_to_eval</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">correct</span> <span class="o">=</span> <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">preds</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">total</span> <span class="o">+=</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Accuracy (</span><span class="si">{</span><span class="n">tag</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">accuracy</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&#34;</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="define-helper-functions-to-measure-latency">Define helper functions to measure latency<a hidden class="anchor" aria-hidden="true" href="#define-helper-functions-to-measure-latency">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Timer</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    A simple timer utility for measuring elapsed time in milliseconds.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Supports both GPU and CPU timing:
</span></span></span><span class="line"><span class="cl"><span class="s2">    - If CUDA is available, uses torch.cuda.Event for accurate GPU timing.
</span></span></span><span class="line"><span class="cl"><span class="s2">    - Otherwise, falls back to wall-clock CPU timing via time.time().
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Methods:
</span></span></span><span class="line"><span class="cl"><span class="s2">        start(): Start the timer.
</span></span></span><span class="line"><span class="cl"><span class="s2">        stop(): Stop the timer and return the elapsed time in milliseconds.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">starter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ender</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">starter</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">stop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ender</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">starter</span><span class="o">.</span><span class="n">elapsed_time</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ender</span><span class="p">)</span>  <span class="c1"># ms</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_time</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>  <span class="c1"># ms</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">estimate_latency</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">,</span> <span class="n">repetitions</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns avg and std inference latency (ms) over given runs.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">timer</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">timings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">repetitions</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># warm-up</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">example_inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">rep</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">repetitions</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">timer</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">example_inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">elapsed</span> <span class="o">=</span> <span class="n">timer</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">timings</span><span class="p">[</span><span class="n">rep</span><span class="p">]</span> <span class="o">=</span> <span class="n">elapsed</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">timings</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">timings</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">estimate_latency_full</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tag</span><span class="p">,</span> <span class="n">skip_cpu</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Prints model latency on GPU and (optionally) CPU.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># estimate latency on CPU</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">skip_cpu</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">example_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">latency_mu</span><span class="p">,</span> <span class="n">latency_std</span> <span class="o">=</span> <span class="n">estimate_latency</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">example_input</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Latency (</span><span class="si">{</span><span class="n">tag</span><span class="si">}</span><span class="s2">, on CPU): </span><span class="si">{</span><span class="n">latency_mu</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">latency_std</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ms&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># estimate latency on GPU</span>
</span></span><span class="line"><span class="cl">    <span class="n">example_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">latency_mu</span><span class="p">,</span> <span class="n">latency_std</span> <span class="o">=</span> <span class="n">estimate_latency</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">example_input</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Latency (</span><span class="si">{</span><span class="n">tag</span><span class="si">}</span><span class="s2">, on GPU): </span><span class="si">{</span><span class="n">latency_mu</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">latency_std</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ms&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">print_size_of_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Prints model size (MB).
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">&#34;temp.p&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">size_mb_full</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">getsize</span><span class="p">(</span><span class="s2">&#34;temp.p&#34;</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1e6</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Size (</span><span class="si">{</span><span class="n">tag</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">size_mb_full</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&#34;temp.p&#34;</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="train-full-model">Train full model<a hidden class="anchor" aria-hidden="true" href="#train-full-model">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">train</span><span class="p">(</span><span class="n">model_to_quantize</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">save_path</span><span class="o">=</span><span class="s2">&#34;full_model.pth&#34;</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="evaluate-full-model">Evaluate full model<a hidden class="anchor" aria-hidden="true" href="#evaluate-full-model">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># get full model size</span>
</span></span><span class="line"><span class="cl"><span class="n">print_size_of_model</span><span class="p">(</span><span class="n">model_to_quantize</span><span class="p">,</span> <span class="s2">&#34;full&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># evaluate full accuracy</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracy_full</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model_to_quantize</span><span class="p">,</span> <span class="s1">&#39;full&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># estimate full model latency</span>
</span></span><span class="line"><span class="cl"><span class="n">estimate_latency_full</span><span class="p">(</span><span class="n">model_to_quantize</span><span class="p">,</span> <span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="n">skip_cpu</span><span class="p">)</span>
</span></span></code></pre></div><p>Results:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">Size (full): 44.77 MB
</span></span><span class="line"><span class="cl">Accuracy (full): 80.53%
</span></span><span class="line"><span class="cl">Latency (full, on CPU): 804.16 ± 57.55 ms
</span></span><span class="line"><span class="cl">Latency (full, on GPU): 16.39 ± 0.30 ms
</span></span></code></pre></div><h3 id="post-training-quantization-ptq-1">Post Training Quantization (PTQ)<a hidden class="anchor" aria-hidden="true" href="#post-training-quantization-ptq-1">#</a></h3>
<p>The basic flow is as follow:</p>
<ol>
<li>Export the model to to a stable, backend-agnostic format that&rsquo;s suitable for transformations, optimizations, and deployment.</li>
<li>Define the quantizer that will prepare the model for quantization. Here I used the X86 for CPU deployments, but there is a simple variant that works better for mobile and edge devices working on ARM CPUs.</li>
<li>Preparing the model for quantization. For example, folding batch-norm into preceding conv2d operators, and inserting observers in appropriate places to collect activation statistics needed for calibration.</li>
<li>Running inference on calibration data to collect activation statistics</li>
<li>Converts calibrated model to a quantized model. While the quantized model already takes less space, it is not yet optimized for the final deployment.</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.ao.quantization.quantize_pt2e</span> <span class="kn">import</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">prepare_pt2e</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">convert_pt2e</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.ao.quantization.quantizer.x86_inductor_quantizer</span> <span class="k">as</span> <span class="nn">xiq</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.ao.quantization.quantizer.x86_inductor_quantizer</span> <span class="kn">import</span> <span class="n">X86InductorQuantizer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># batch of 128 images, each with 3 color channels and 32x32 resolution (CIFAR-10)</span>
</span></span><span class="line"><span class="cl"><span class="n">example_inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># export the model to a standardized format before quantization</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s2">&#34;2.5&#34;</span><span class="p">):</span> <span class="c1"># for pytorch 2.5+</span>
</span></span><span class="line"><span class="cl">    <span class="n">exported_model</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export_for_training</span><span class="p">(</span><span class="n">model_to_quantize</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">)</span><span class="o">.</span><span class="n">module</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span> <span class="c1"># for pytorch 2.4</span>
</span></span><span class="line"><span class="cl">    <span class="kn">from</span> <span class="nn">torch._export</span> <span class="kn">import</span> <span class="n">capture_pre_autograd_graph</span>
</span></span><span class="line"><span class="cl">    <span class="n">exported_model</span> <span class="o">=</span> <span class="n">capture_pre_autograd_graph</span><span class="p">(</span><span class="n">model_to_quantize</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># quantization setup for X86 Inductor Quantizer</span>
</span></span><span class="line"><span class="cl"><span class="n">quantizer</span> <span class="o">=</span> <span class="n">X86InductorQuantizer</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">quantizer</span><span class="o">.</span><span class="n">set_global</span><span class="p">(</span><span class="n">xiq</span><span class="o">.</span><span class="n">get_default_x86_inductor_quantization_config</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># preparing for PTQ by folding batch-norm into preceding conv2d operators, and inserting observers in appropriate places</span>
</span></span><span class="line"><span class="cl"><span class="n">prepared_model</span> <span class="o">=</span> <span class="n">prepare_pt2e</span><span class="p">(</span><span class="n">exported_model</span><span class="p">,</span> <span class="n">quantizer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># run inference on calibration data to collect activation stats needed for activation quantization</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">calibrate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">move_exported_model_to_eval</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">image</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">model</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">calibrate</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">,</span> <span class="n">calibration_loader</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># converts calibrated model to a quantized model</span>
</span></span><span class="line"><span class="cl"><span class="n">quantized_model</span> <span class="o">=</span> <span class="n">convert_pt2e</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># export again to remove unused weights after quantization</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s2">&#34;2.5&#34;</span><span class="p">):</span> <span class="c1"># for pytorch 2.5+</span>
</span></span><span class="line"><span class="cl">    <span class="n">quantized_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export_for_training</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">)</span><span class="o">.</span><span class="n">module</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span> <span class="c1"># for pytorch 2.4</span>
</span></span><span class="line"><span class="cl">    <span class="n">quantized_model</span> <span class="o">=</span> <span class="n">capture_pre_autograd_graph</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="evaluate-quantized-model">Evaluate quantized model<a hidden class="anchor" aria-hidden="true" href="#evaluate-quantized-model">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># get quantized model size</span>
</span></span><span class="line"><span class="cl"><span class="n">print_size_of_model</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span> <span class="s2">&#34;quantized&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># evaluate quantized accuracy</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracy_full</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span> <span class="s1">&#39;quantized&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># estimate quantized model latency</span>
</span></span><span class="line"><span class="cl"><span class="n">estimate_latency_full</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span> <span class="s1">&#39;quantized&#39;</span><span class="p">,</span> <span class="n">skip_cpu</span><span class="p">)</span>
</span></span></code></pre></div><p>Results:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">Size (quantized): 11.26 MB
</span></span><span class="line"><span class="cl">Accuracy (quantized): 80.45%
</span></span><span class="line"><span class="cl">Latency (quantized, on CPU): 1982.11 ± 229.35 ms
</span></span><span class="line"><span class="cl">Latency (quantized, on GPU): 37.15 ± 0.08 ms
</span></span></code></pre></div><p>Notice the space dropped by 75%, but CPU and GPU latency more than doubled. This is because the model while quantized is not optimized yet to run on the specific device. This will happen in the next section.</p>
<h4 id="optimize-quantized-model-for-inference">Optimize quantized model for inference<a hidden class="anchor" aria-hidden="true" href="#optimize-quantized-model-for-inference">#</a></h4>
<p>Here we do the final optimization to squeeze the performance. This uses C++ wrapper which reduces the Python overhead</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># enable the use of the C++ wrapper for TorchInductor which reduces Python overhead</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch._inductor.config</span> <span class="k">as</span> <span class="nn">config</span>
</span></span><span class="line"><span class="cl"><span class="n">config</span><span class="o">.</span><span class="n">cpp_wrapper</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># compiles quantized model to generate optimized model</span>
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimized_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="evaluate-optimized-model">Evaluate optimized model<a hidden class="anchor" aria-hidden="true" href="#evaluate-optimized-model">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># get optimized model size</span>
</span></span><span class="line"><span class="cl"><span class="n">print_size_of_model</span><span class="p">(</span><span class="n">optimized_model</span><span class="p">,</span> <span class="s2">&#34;optimized&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># evaluate optimized accuracy</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracy_full</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">optimized_model</span><span class="p">,</span> <span class="s1">&#39;optimized&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># estimate optimized model latency</span>
</span></span><span class="line"><span class="cl"><span class="n">estimate_latency_full</span><span class="p">(</span><span class="n">optimized_model</span><span class="p">,</span> <span class="s1">&#39;optimized&#39;</span><span class="p">,</span> <span class="n">skip_cpu</span><span class="p">)</span>
</span></span></code></pre></div><p>Results:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">Size (optimized): 11.26 MB
</span></span><span class="line"><span class="cl">Accuracy (optimized): 79.53%
</span></span><span class="line"><span class="cl">Latency (optimized, on CPU): 782.53 ± 51.36 ms
</span></span><span class="line"><span class="cl">Latency (optimized, on GPU): 13.80 ± 0.28 ms
</span></span></code></pre></div><p>Notably, it achieves a <strong>75% reduction in space</strong>, <strong>reduces GPU latency by 16%</strong> and 3% on CPU, with only a 1% drop in accuracy.</p>
<h3 id="quantization-aware-training-qat-1">Quantization Aware Training (QAT)<a hidden class="anchor" aria-hidden="true" href="#quantization-aware-training-qat-1">#</a></h3>
<p>In QAT the basic flow is very similiar to PTQ, the main difference is the replacement of the calibration step that collects activation statistics with a much longer fine-tuning step which fine-tunes the model considering the quantization constraints. The collection of activation statistics also happens, as part of the fine-tuning process.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.ao.quantization.quantize_pt2e</span> <span class="kn">import</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">prepare_qat_pt2e</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">convert_pt2e</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.ao.quantization.quantizer.x86_inductor_quantizer</span> <span class="k">as</span> <span class="nn">xiq</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.ao.quantization.quantizer.x86_inductor_quantizer</span> <span class="kn">import</span> <span class="n">X86InductorQuantizer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># batch of 128 images, each with 3 color channels and 32x32 resolution (CIFAR-10)</span>
</span></span><span class="line"><span class="cl"><span class="n">example_inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># export the model to a standardized format before quantization</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s2">&#34;2.5&#34;</span><span class="p">):</span> <span class="c1"># for pytorch 2.5+</span>
</span></span><span class="line"><span class="cl">    <span class="n">exported_model</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export_for_training</span><span class="p">(</span><span class="n">model_to_quantize</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">)</span><span class="o">.</span><span class="n">module</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span> <span class="c1"># for pytorch 2.4</span>
</span></span><span class="line"><span class="cl">    <span class="kn">from</span> <span class="nn">torch._export</span> <span class="kn">import</span> <span class="n">capture_pre_autograd_graph</span>
</span></span><span class="line"><span class="cl">    <span class="n">exported_model</span> <span class="o">=</span> <span class="n">capture_pre_autograd_graph</span><span class="p">(</span><span class="n">model_to_quantize</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">)</span>    
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># quantization setup for X86 Inductor Quantizer</span>
</span></span><span class="line"><span class="cl"><span class="n">quantizer</span> <span class="o">=</span> <span class="n">X86InductorQuantizer</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">quantizer</span><span class="o">.</span><span class="n">set_global</span><span class="p">(</span><span class="n">xiq</span><span class="o">.</span><span class="n">get_default_x86_inductor_quantization_config</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># inserts fake quantizes in appropriate places in the model and performs the fusions, like conv2d + batch-norm</span>
</span></span><span class="line"><span class="cl"><span class="n">prepared_model</span> <span class="o">=</span> <span class="n">prepare_qat_pt2e</span><span class="p">(</span><span class="n">exported_model</span><span class="p">,</span> <span class="n">quantizer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># fine-tune with quantization constraints</span>
</span></span><span class="line"><span class="cl"><span class="n">train</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">save_path</span><span class="o">=</span><span class="s2">&#34;qat_model_x86.pth&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># converts calibrated model to a quantized model</span>
</span></span><span class="line"><span class="cl"><span class="n">quantized_model</span> <span class="o">=</span> <span class="n">convert_pt2e</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># export again to remove unused weights after quantization</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s2">&#34;2.5&#34;</span><span class="p">):</span> <span class="c1"># for pytorch 2.5+</span>
</span></span><span class="line"><span class="cl">    <span class="n">quantized_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export_for_training</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">)</span><span class="o">.</span><span class="n">module</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span> <span class="c1"># for pytorch 2.4</span>
</span></span><span class="line"><span class="cl">    <span class="n">quantized_model</span> <span class="o">=</span> <span class="n">capture_pre_autograd_graph</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="evaluate-quantized-model-1">Evaluate quantized model<a hidden class="anchor" aria-hidden="true" href="#evaluate-quantized-model-1">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># get quantized model size</span>
</span></span><span class="line"><span class="cl"><span class="n">print_size_of_model</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span> <span class="s2">&#34;quantized&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># evaluate quantized accuracy</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracy_full</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span> <span class="s1">&#39;quantized&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># estimate quantized model latency</span>
</span></span><span class="line"><span class="cl"><span class="n">estimate_latency_full</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span> <span class="s1">&#39;quantized&#39;</span><span class="p">,</span> <span class="n">skip_cpu</span><span class="p">)</span>
</span></span></code></pre></div><p>Results:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">Size (quantized): 11.26 MB
</span></span><span class="line"><span class="cl">Accuracy (quantized): 80.57%
</span></span><span class="line"><span class="cl">Latency (quantized, on CPU): 1617.82 ± 158.67 ms
</span></span><span class="line"><span class="cl">Latency (quantized, on GPU): 33.62 ± 0.16 ms
</span></span></code></pre></div><h4 id="optimize-quantized-model-for-inference-1">Optimize quantized model for inference<a hidden class="anchor" aria-hidden="true" href="#optimize-quantized-model-for-inference-1">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># enable the use of the C++ wrapper for TorchInductor which reduces Python overhead</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch._inductor.config</span> <span class="k">as</span> <span class="nn">config</span>
</span></span><span class="line"><span class="cl"><span class="n">config</span><span class="o">.</span><span class="n">cpp_wrapper</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># compiles quantized model to generate optimized model</span>
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimized_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="evaluate-optimized-model-1">Evaluate optimized model<a hidden class="anchor" aria-hidden="true" href="#evaluate-optimized-model-1">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># get optimized model size</span>
</span></span><span class="line"><span class="cl"><span class="n">print_size_of_model</span><span class="p">(</span><span class="n">optimized_model</span><span class="p">,</span> <span class="s2">&#34;optimized&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># evaluate optimized accuracy</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracy_full</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">optimized_model</span><span class="p">,</span> <span class="s1">&#39;optimized&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># estimate optimized model latency</span>
</span></span><span class="line"><span class="cl"><span class="n">estimate_latency_full</span><span class="p">(</span><span class="n">optimized_model</span><span class="p">,</span> <span class="s1">&#39;optimized&#39;</span><span class="p">,</span> <span class="n">skip_cpu</span><span class="p">)</span>
</span></span></code></pre></div><p>Results:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">Size (optimized): 11.26 MB
</span></span><span class="line"><span class="cl">Accuracy (optimized): 79.54%
</span></span><span class="line"><span class="cl">Latency (optimized, on CPU): 831.76 ± 39.63 ms
</span></span><span class="line"><span class="cl">Latency (optimized, on GPU): 13.71 ± 0.24 ms
</span></span></code></pre></div><p>While in this small-scale model the results of QAT are very similar to PTQ, it is suggested that for larger models QAT has an opportunity to provide higher accuracy than the PTQ variant.</p>
<h2 id="comparison-of-ptq-and-qat-results">Comparison of PTQ and QAT Results<a hidden class="anchor" aria-hidden="true" href="#comparison-of-ptq-and-qat-results">#</a></h2>
<p>Below is a summary table comparing the baseline model with the Post Training Quantization (PTQ) and Quantization Aware Training (QAT) results based on our CIFAR-10 ResNet18 experiments:</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Model Size</th>
          <th>Accuracy</th>
          <th>GPU Latency (ms)</th>
          <th>CPU Latency (ms)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Baseline (no quantization)</td>
          <td>44.77 MB</td>
          <td>80.53%</td>
          <td>16.39 ± 0.30 ms</td>
          <td>804.16 ± 57.55 ms</td>
      </tr>
      <tr>
          <td>Post Training Quantization</td>
          <td>11.26 MB</td>
          <td>79.53%</td>
          <td>13.80 ± 0.28 ms</td>
          <td>782.53 ± 51.36 ms</td>
      </tr>
      <tr>
          <td>Quantization Aware Training</td>
          <td>11.26 MB</td>
          <td>79.54%</td>
          <td>13.71 ± 0.24 ms</td>
          <td>831.76 ± 39.63 ms</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="summary">Summary<a hidden class="anchor" aria-hidden="true" href="#summary">#</a></h2>
<p>Quantization is a powerful technique for compressing and accelerating deep learning models by lowering numerical precision. PyTorch provides flexible APIs for applying both Post Training Quantization (PTQ) and Quantization Aware Training (QAT).</p>
<ul>
<li>Use <strong>PTQ</strong> when simplicity and speed are key, and you can tolerate some loss in accuracy.</li>
<li>Use <strong>QAT</strong> when you need the best possible performance from quantized models, especially for smaller or sensitive models.</li>
</ul>
<p>With good calibration and training strategies, quantization can reduce model size and inference time significantly with minimal impact on performance.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>For additional details, the following sources were helpful when preparing this post.</p>
<ol>
<li><a href="https://pytorch.org/docs/stable/quantization.html">PyTorch Documentation: Quantization</a></li>
<li><a href="https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html">PyTorch Documentation: PyTorch 2 Export Post Training Quantization</a></li>
<li><a href="https://pytorch.org/tutorials/prototype/pt2e_quant_qat.html">PyTorch Documentation: PyTorch 2 Export Quantization-Aware Training (QAT)</a></li>
<li><a href="https://pytorch.org/tutorials/prototype/pt2e_quant_x86_inductor.html">PyTorch Documentation: PyTorch 2 Export Quantization with X86 Backend through Inductor</a></li>
<li><a href="https://dev-discuss.pytorch.org/t/torchinductor-update-6-cpu-backend-performance-update-and-new-features-in-pytorch-2-1/1514">PyTorch Dev Discussions: TorchInductor Update 6: CPU backend performance update and new features in PyTorch 2.1</a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="https://arikpoz.github.io/posts/2025-04-10-neural-network-pruning-how-to-accelerate-inference-with-minimal-accuracy-loss/">
    <span class="title">Next »</span>
    <br>
    <span>Neural Network Pruning: How to Accelerate Inference with Minimal Accuracy Loss</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network Quantization in PyTorch on x"
            href="https://x.com/intent/tweet/?text=Neural%20Network%20Quantization%20in%20PyTorch&amp;url=https%3a%2f%2farikpoz.github.io%2fposts%2f2025-04-16-neural-network-quantization-in-pytorch%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network Quantization in PyTorch on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2farikpoz.github.io%2fposts%2f2025-04-16-neural-network-quantization-in-pytorch%2f&amp;title=Neural%20Network%20Quantization%20in%20PyTorch&amp;summary=Neural%20Network%20Quantization%20in%20PyTorch&amp;source=https%3a%2f%2farikpoz.github.io%2fposts%2f2025-04-16-neural-network-quantization-in-pytorch%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network Quantization in PyTorch on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2farikpoz.github.io%2fposts%2f2025-04-16-neural-network-quantization-in-pytorch%2f&title=Neural%20Network%20Quantization%20in%20PyTorch">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network Quantization in PyTorch on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2farikpoz.github.io%2fposts%2f2025-04-16-neural-network-quantization-in-pytorch%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network Quantization in PyTorch on whatsapp"
            href="https://api.whatsapp.com/send?text=Neural%20Network%20Quantization%20in%20PyTorch%20-%20https%3a%2f%2farikpoz.github.io%2fposts%2f2025-04-16-neural-network-quantization-in-pytorch%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network Quantization in PyTorch on telegram"
            href="https://telegram.me/share/url?text=Neural%20Network%20Quantization%20in%20PyTorch&amp;url=https%3a%2f%2farikpoz.github.io%2fposts%2f2025-04-16-neural-network-quantization-in-pytorch%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network Quantization in PyTorch on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Neural%20Network%20Quantization%20in%20PyTorch&u=https%3a%2f%2farikpoz.github.io%2fposts%2f2025-04-16-neural-network-quantization-in-pytorch%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://arikpoz.github.io/">Practical ML</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
