[{"content":"\nIntroduction This tutorial provides an introduction to quantization in PyTorch, covering both theory and practice. We’ll explore the different types of quantization, and apply both post training quantization (PTQ) and quantization aware training (QAT) on a simple example using CIFAR-10 and ResNet18. In the presented example we achieve a 75% reduction in space and 16% reduction in GPU latency with only 1% drop in accuracy.\nWhat is Quantization? Quantization is a model optimization technique that reduces the numerical precision used to represent weights and activations in deep learning models. Its primary benefits include:\nModel Compression - lowers memory usage and storage. Inference Acceleration - speeds up inference and reduces energy consumption. While quantization is most often used for deployment on edge devices (e.g., phones, embedded hardware), it can also reduce infrastructure costs for large-scale inference in the cloud.\nWhy Quantize Weights and Activations? Quantization typically targets both weights and activations, and each serves a different purpose in optimizing model deployment:\nWhy Quantize Weights? Storage savings: Weights are the learned parameters of a model and are saved to disk. Reducing their precision (e.g., from float32 to int8) significantly shrinks the size of the model file. Faster model loading: Smaller weights reduce model loading time, which is especially useful for mobile and edge deployments. Reduced memory footprint: On-device memory use is lower, which allows running larger models or multiple models concurrently. Why Quantize Activations? Runtime efficiency: Activations are the intermediate outputs of each layer computed during the forward pass. Lower-precision activations (e.g., int8 instead of float32) require less memory bandwidth and compute. End-to-end low-precision execution: Quantizing both weights and activations enables optimized hardware kernels (e.g., int8 × int8 → int32) to be used throughout the network, maximizing speed and energy efficiency. Better cache locality: Smaller activation tensors are more cache-friendly, leading to faster inference. Quantizing only the weights can reduce model size but won’t deliver full runtime acceleration. Quantizing both weights and activations is essential to fully unlock the benefits of quantized inference on CPUs, mobile chips, and specialized accelerators.\nTypes of Quantization The two most common approaches to quantization fall into these categories:\nFloating-Point Quantization Floating-point quantization reduces the bit-width of real-valued tensors, typically from 32-bit (float32) to 16-bit (float16 or bfloat16). These formats use fewer bits for the exponent and mantissa, resulting in lower precision but maintaining the continuous range and general expressiveness of real numbers.\nUses 16 bits instead of 32 (e.g., float16, bfloat16). Preserves dynamic range and real-number structure. Maintains relatively high accuracy. Supported efficiently on modern hardware (e.g., GPUs, TPUs). The diagram below compares the internal bit layout of float32 , float16 , and bfloat16 using color-coded segments for the sign, exponent, and mantissa bits:\nbfloat16 , developed by Google Brain, is especially notable because it retains the full 8-bit exponent of float32 , offering a wide dynamic range. While its 7-bit mantissa provides less precision, this makes it more numerically stable than float16 , particularly for training deep networks\nInteger Quantization Integer quantization maps real-valued numbers to a discrete integer range using an affine transformation. This process enables efficient inference using low-precision arithmetic.\nQuantization: $q = \\text{round}\\left(\\frac{x}{s}\\right) + z$\nDequantization: $x \\approx s \\cdot (q - z)$\nWhere:\nx is the original float q is the quantized integer s is the scale (a float) z is the zero-point (an int) These mappings let the model operate primarily with integers during inference, reducing memory usage and enabling faster execution on integer-optimized hardware.\nHow Are Scale and Zero-Point Determined? (Calibration) The scale and zero-point are calculated based on the distribution of float values in a tensor. Typically:\nScale (s ) is derived from the min and max float values of the tensor, and the min and max values of the quantized range (0-255 for uint8 or -128 to 127 for int8)\ns = (x_max - x_min) / (q_max - q_min) Zero-point (z ) ensures that 0 is exactly representable:\nz = round(q_min - x_min / s) This process of determining the appropriate scale and zero-point by observing real-valued data flowing through the model is known as calibration. It is especially important for static quantization, where activation ranges are fixed based on representative input data.\nThese parameters are then stored along with each quantized tensor. There are two main approaches:\nPer-tensor quantization: One scale and zero-point for the entire tensor. Per-channel quantization: Separate scale and zero-point per output channel (commonly used for weights in convolutional layers). During inference, these values are used to convert between quantized and real representations efficiently. Some characteristics: Aggressive memory/computation savings. May introduce more quantization error. Commonly used in edge-optimized frameworks like TensorFlow Lite and PyTorch Mobile. Tradeoffs Quantization enables efficient inference but can degrade accuracy, especially if done post training without calibration. To minimize this, modern techniques like Quantization Aware Training (QAT) are used, see below.\nWhen Is Quantization Applied? Quantization can be applied at different stages in the model lifecycle. The two primary approaches are Post Training Quantization (PTQ) and Quantization Aware Training (QAT), each with its own benefits and tradeoffs.\nPost Training Quantization (PTQ) PTQ is applied to a fully trained model without requiring any retraining. It’s simple and quick to implement, but may cause some degradation in model accuracy, especially when using aggressive quantization like int8.\nAdvantages:\nEasy to integrate into existing workflows No need to modify training code Can dramatically reduce model size and inference cost Limitations:\nAccuracy may drop, especially for sensitive models or tasks Works best on models that are already robust to small numeric changes Variants:\nDynamic Quantization:\nWhen? After training. What? Only weights are quantized and stored in int8. Activations remain in float and are quantized dynamically during inference. How? No calibration needed. Activation ranges are computed on-the-fly at runtime. Pros: Easy to apply; works well for models with large nn.Linear layers (e.g., NLP). Cons: Some operations still use float intermediates; less efficient than static quantization. Static Quantization:\nWhen? After training. What? Both weights and activations are quantized to int8. How? Requires calibration, passing representative data through the model to collect activation stats. Pros: Enables full integer inference; maximizes performance. Cons: Slightly more setup due to calibration requirement. Weight-only Quantization:\nWhen? After training. What? Only weights are quantized; activations remain float32. How? No activation quantization, so no calibration needed. Pros: Reduces model size. Cons: Limited inference speedup since activations are still float. Only weights are quantized; activations remain in float. Saves memory, but yields limited inference speedup. Quantization Aware Training (QAT) QAT introduces quantization effects during the training process by simulating them using fake quantization operations. These operations emulate the behavior of quantization during the forward pass (e.g., rounding, clamping) while still allowing gradients to flow through in full precision during the backward pass. This enables the model to learn to be robust to quantization effects while maintaining effective training dynamics.\nAdvantages:\nHighest accuracy among quantized models Especially useful for smaller or sensitive models that suffer from PTQ degradation Limitations:\nRequires retraining or fine-tuning Slightly slower training due to added quantization simulation steps QAT is particularly effective for compact architectures like MobileNet or for models deployed on edge devices where low-precision inference is essential and even small drops in accuracy can be problematic. (like MobileNet) or models deployed in latency-sensitive, low-precision environments (e.g., mobile or embedded devices).\nCode Walkthrough In this section I will provide a complete example of applying both Post Training Quantization (PTQ) and Quantization Aware Training (QAT) to a ResNet18 model adjusted for CIFAR-10 dataset. The code was tested to work on PyTorch 2.4 through 2.8 (nightly build) using both X86 Quantizer for CPU deployments and XNNPACK Quantizer used for mobile and edge devices. You can find the full self-contained jupyter notebooks below, or in the Neural Network Optimization GitHub repository.\nQuantization - PTQ using PyTorch 2 Export Quantization and X86 Backend Quantization - QAT using PyTorch 2 Export Quantization and X86 Backend Quantization - PTQ using PyTorch 2 Export Quantization and XNNPACK Quantizer Quantization - QAT using PyTorch 2 Export Quantization and XNNPACK Quantizer Below I will go over the code for PTQ and QAT for the X86 scenario, as the edge device case is very similar.\nShared code We start with defining some code to get the CIFAR-10 dataset, adjust the ResNet18 model, and define training and evaluation functions to measure the model\u0026rsquo;s size, accuracy, and latency. We end this section with the training and evaluation of the baseline model, before quantization.\nBasic Setup import os import time import warnings from packaging import version import numpy as np import torch import torch.nn as nn from torchvision import datasets, transforms, models from torch.quantization import quantize_dynamic from torch.ao.quantization import get_default_qconfig, QConfigMapping from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx from torch.utils.data import DataLoader, Subset # ignores irrelevant warning, see: https://github.com/pytorch/pytorch/issues/149829 warnings.filterwarnings(\u0026#34;ignore\u0026#34;, message=\u0026#34;.*TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support.*\u0026#34;) # ignores irrelevant warning, see: https://github.com/tensorflow/tensorflow/issues/77293 warnings.filterwarnings(\u0026#34;ignore\u0026#34;, message=\u0026#34;.*erase_node(.*) on an already erased node.*\u0026#34;) print(f\u0026#34;PyTorch Version: {torch.__version__}\u0026#34;) device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) print(f\u0026#34;Device used: {device.type}\u0026#34;) skip_cpu = False # change to True to skip the slow checks on CPU print(f\u0026#34;Should skip CPU evaluations: {skip_cpu}\u0026#34;) Get CIFAR-10 train and test sets transform = transforms.Compose([ transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ]) train_dataset = datasets.CIFAR10(root=\u0026#34;./data\u0026#34;, train=True, download=True, transform=transform) train_loader = DataLoader( datasets.CIFAR10(root=\u0026#34;./data\u0026#34;, train=True, download=True, transform=transform), batch_size=128, shuffle=True ) test_dataset = datasets.CIFAR10(root=\u0026#34;./data\u0026#34;, train=False, download=True, transform=transform) test_loader = DataLoader( datasets.CIFAR10(root=\u0026#34;./data\u0026#34;, train=False, download=True, transform=transform), batch_size=128, shuffle=False, num_workers=2, drop_last=True, ) calibration_dataset = Subset(train_dataset, range(256)) calibration_loader = DataLoader(calibration_dataset, batch_size=128, shuffle=False) Adjust ResNet18 network for CIFAR-10 dataset def get_resnet18_for_cifar10(): \u0026#34;\u0026#34;\u0026#34; Returns a ResNet-18 model adjusted for CIFAR-10: - 3x3 conv with stride 1 - No max pooling - 10 output classes \u0026#34;\u0026#34;\u0026#34; model = models.resnet18(weights=None, num_classes=10) model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False) model.maxpool = nn.Identity() return model.to(device) model_to_quantize = get_resnet18_for_cifar10() Define Train and Evaluate functions def train(model, loader, epochs, lr=0.01, save_path=\u0026#34;model.pth\u0026#34;, silent=False): \u0026#34;\u0026#34;\u0026#34; Trains a model with SGD and cross-entropy loss. Loads from save_path if it exists. \u0026#34;\u0026#34;\u0026#34; try: model.train() except NotImplementedError: torch.ao.quantization.move_exported_model_to_train(model) if os.path.exists(save_path): if not silent: print(f\u0026#34;Model already trained. Loading from {save_path}\u0026#34;) model.load_state_dict(torch.load(save_path)) return # no saved model found. training from given model state criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9) for epoch in range(epochs): for x, y in loader: x, y = x.to(device), y.to(device) optimizer.zero_grad() loss = criterion(model(x), y) loss.backward() optimizer.step() if not silent: print(f\u0026#34;Epoch {epoch+1}: loss={loss.item():.4f}\u0026#34;) evaluate(model, f\u0026#34;Epoch {epoch+1}\u0026#34;) try: model.train() except NotImplementedError: torch.ao.quantization.move_exported_model_to_train(model) if save_path: torch.save(model.state_dict(), save_path) if not silent: print(f\u0026#34;Training complete. Model saved to {save_path}\u0026#34;) def evaluate(model, tag): \u0026#34;\u0026#34;\u0026#34; Evaluates the model on test_loader and prints accuracy. \u0026#34;\u0026#34;\u0026#34; try: model.eval() except NotImplementedError: model = torch.ao.quantization.move_exported_model_to_eval(model) model.to(device) correct = total = 0 with torch.no_grad(): for x, y in test_loader: x, y = x.to(device), y.to(device) preds = model(x).argmax(1) correct += (preds == y).sum().item() total += y.size(0) accuracy = correct / total print(f\u0026#34;Accuracy ({tag}): {accuracy*100:.2f}%\u0026#34;) Define helper functions to measure latency class Timer: \u0026#34;\u0026#34;\u0026#34; A simple timer utility for measuring elapsed time in milliseconds. Supports both GPU and CPU timing: - If CUDA is available, uses torch.cuda.Event for accurate GPU timing. - Otherwise, falls back to wall-clock CPU timing via time.time(). Methods: start(): Start the timer. stop(): Stop the timer and return the elapsed time in milliseconds. \u0026#34;\u0026#34;\u0026#34; def __init__(self): self.use_cuda = torch.cuda.is_available() if self.use_cuda: self.starter = torch.cuda.Event(enable_timing=True) self.ender = torch.cuda.Event(enable_timing=True) def start(self): if self.use_cuda: self.starter.record() else: self.start_time = time.time() def stop(self): if self.use_cuda: self.ender.record() torch.cuda.synchronize() return self.starter.elapsed_time(self.ender) # ms else: return (time.time() - self.start_time) * 1000 # ms def estimate_latency(model, example_inputs, repetitions=50): \u0026#34;\u0026#34;\u0026#34; Returns avg and std inference latency (ms) over given runs. \u0026#34;\u0026#34;\u0026#34; timer = Timer() timings = np.zeros((repetitions, 1)) # warm-up for _ in range(5): _ = model(example_inputs) with torch.no_grad(): for rep in range(repetitions): timer.start() _ = model(example_inputs) elapsed = timer.stop() timings[rep] = elapsed return np.mean(timings), np.std(timings) def estimate_latency_full(model, tag, skip_cpu): \u0026#34;\u0026#34;\u0026#34; Prints model latency on GPU and (optionally) CPU. \u0026#34;\u0026#34;\u0026#34; # estimate latency on CPU if not skip_cpu: example_input = torch.rand(128, 3, 32, 32).cpu() model.cpu() latency_mu, latency_std = estimate_latency(model, example_input) print(f\u0026#34;Latency ({tag}, on CPU): {latency_mu:.2f} ± {latency_std:.2f} ms\u0026#34;) # estimate latency on GPU example_input = torch.rand(128, 3, 32, 32).cuda() model.cuda() latency_mu, latency_std = estimate_latency(model, example_input) print(f\u0026#34;Latency ({tag}, on GPU): {latency_mu:.2f} ± {latency_std:.2f} ms\u0026#34;) def print_size_of_model(model, tag=\u0026#34;\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Prints model size (MB). \u0026#34;\u0026#34;\u0026#34; torch.save(model.state_dict(), \u0026#34;temp.p\u0026#34;) size_mb_full = os.path.getsize(\u0026#34;temp.p\u0026#34;) / 1e6 print(f\u0026#34;Size ({tag}): {size_mb_full:.2f} MB\u0026#34;) os.remove(\u0026#34;temp.p\u0026#34;) Train full model train(model_to_quantize, train_loader, epochs=15, save_path=\u0026#34;full_model.pth\u0026#34;) Evaluate full model # get full model size print_size_of_model(model_to_quantize, \u0026#34;full\u0026#34;) # evaluate full accuracy accuracy_full = evaluate(model_to_quantize, \u0026#39;full\u0026#39;) # estimate full model latency estimate_latency_full(model_to_quantize, \u0026#39;full\u0026#39;, skip_cpu) Results:\nSize (full): 44.77 MB Accuracy (full): 80.53% Latency (full, on CPU): 804.16 ± 57.55 ms Latency (full, on GPU): 16.39 ± 0.30 ms Post Training Quantization (PTQ) The basic flow is as follow:\nExport the model to to a stable, backend-agnostic format that\u0026rsquo;s suitable for transformations, optimizations, and deployment. Define the quantizer that will prepare the model for quantization. Here I used the X86 for CPU deployments, but there is a simple variant that works better for mobile and edge devices working on ARM CPUs. Preparing the model for quantization. For example, folding batch-norm into preceding conv2d operators, and inserting observers in appropriate places to collect activation statistics needed for calibration. Running inference on calibration data to collect activation statistics Converts calibrated model to a quantized model. While the quantized model already takes less space, it is not yet optimized for the final deployment. from torch.ao.quantization.quantize_pt2e import ( prepare_pt2e, convert_pt2e, ) import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq from torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer # batch of 128 images, each with 3 color channels and 32x32 resolution (CIFAR-10) example_inputs = (torch.rand(128, 3, 32, 32).to(device),) # export the model to a standardized format before quantization if version.parse(torch.__version__) \u0026gt;= version.parse(\u0026#34;2.5\u0026#34;): # for pytorch 2.5+ exported_model = torch.export.export_for_training(model_to_quantize, example_inputs).module() else: # for pytorch 2.4 from torch._export import capture_pre_autograd_graph exported_model = capture_pre_autograd_graph(model_to_quantize, example_inputs) # quantization setup for X86 Inductor Quantizer quantizer = X86InductorQuantizer() quantizer.set_global(xiq.get_default_x86_inductor_quantization_config()) # preparing for PTQ by folding batch-norm into preceding conv2d operators, and inserting observers in appropriate places prepared_model = prepare_pt2e(exported_model, quantizer) # run inference on calibration data to collect activation stats needed for activation quantization def calibrate(model, data_loader): torch.ao.quantization.move_exported_model_to_eval(model) with torch.no_grad(): for image, target in data_loader: model(image.to(device)) calibrate(prepared_model, calibration_loader) # converts calibrated model to a quantized model quantized_model = convert_pt2e(prepared_model) # export again to remove unused weights after quantization if version.parse(torch.__version__) \u0026gt;= version.parse(\u0026#34;2.5\u0026#34;): # for pytorch 2.5+ quantized_model = torch.export.export_for_training(quantized_model, example_inputs).module() else: # for pytorch 2.4 quantized_model = capture_pre_autograd_graph(quantized_model, example_inputs) Evaluate quantized model # get quantized model size print_size_of_model(quantized_model, \u0026#34;quantized\u0026#34;) # evaluate quantized accuracy accuracy_full = evaluate(quantized_model, \u0026#39;quantized\u0026#39;) # estimate quantized model latency estimate_latency_full(quantized_model, \u0026#39;quantized\u0026#39;, skip_cpu) Results:\nSize (quantized): 11.26 MB Accuracy (quantized): 80.45% Latency (quantized, on CPU): 1982.11 ± 229.35 ms Latency (quantized, on GPU): 37.15 ± 0.08 ms Notice the space dropped by 75%, but CPU and GPU latency more than doubled. This is because the model while quantized is not optimized yet to run on the specific device. This will happen in the next section.\nOptimize quantized model for inference Here we do the final optimization to squeeze the performance. This uses C++ wrapper which reduces the Python overhead\n# enable the use of the C++ wrapper for TorchInductor which reduces Python overhead import torch._inductor.config as config config.cpp_wrapper = True # compiles quantized model to generate optimized model with torch.no_grad(): optimized_model = torch.compile(quantized_model) Evaluate optimized model # get optimized model size print_size_of_model(optimized_model, \u0026#34;optimized\u0026#34;) # evaluate optimized accuracy accuracy_full = evaluate(optimized_model, \u0026#39;optimized\u0026#39;) # estimate optimized model latency estimate_latency_full(optimized_model, \u0026#39;optimized\u0026#39;, skip_cpu) Results:\nSize (optimized): 11.26 MB Accuracy (optimized): 79.53% Latency (optimized, on CPU): 782.53 ± 51.36 ms Latency (optimized, on GPU): 13.80 ± 0.28 ms Notably, it achieves a 75% reduction in space, reduces GPU latency by 16% and 3% on CPU, with only a 1% drop in accuracy.\nQuantization Aware Training (QAT) In QAT the basic flow is very similiar to PTQ, the main difference is the replacement of the calibration step that collects activation statistics with a much longer fine-tuning step which fine-tunes the model considering the quantization constraints. The collection of activation statistics also happens, as part of the fine-tuning process.\nfrom torch.ao.quantization.quantize_pt2e import ( prepare_qat_pt2e, convert_pt2e, ) import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq from torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer # batch of 128 images, each with 3 color channels and 32x32 resolution (CIFAR-10) example_inputs = (torch.rand(128, 3, 32, 32).to(device),) # export the model to a standardized format before quantization if version.parse(torch.__version__) \u0026gt;= version.parse(\u0026#34;2.5\u0026#34;): # for pytorch 2.5+ exported_model = torch.export.export_for_training(model_to_quantize, example_inputs).module() else: # for pytorch 2.4 from torch._export import capture_pre_autograd_graph exported_model = capture_pre_autograd_graph(model_to_quantize, example_inputs) # quantization setup for X86 Inductor Quantizer quantizer = X86InductorQuantizer() quantizer.set_global(xiq.get_default_x86_inductor_quantization_config()) # inserts fake quantizes in appropriate places in the model and performs the fusions, like conv2d + batch-norm prepared_model = prepare_qat_pt2e(exported_model, quantizer) # fine-tune with quantization constraints train(prepared_model, train_loader, epochs=3, save_path=\u0026#34;qat_model_x86.pth\u0026#34;) # converts calibrated model to a quantized model quantized_model = convert_pt2e(prepared_model) # export again to remove unused weights after quantization if version.parse(torch.__version__) \u0026gt;= version.parse(\u0026#34;2.5\u0026#34;): # for pytorch 2.5+ quantized_model = torch.export.export_for_training(quantized_model, example_inputs).module() else: # for pytorch 2.4 quantized_model = capture_pre_autograd_graph(quantized_model, example_inputs) Evaluate quantized model # get quantized model size print_size_of_model(quantized_model, \u0026#34;quantized\u0026#34;) # evaluate quantized accuracy accuracy_full = evaluate(quantized_model, \u0026#39;quantized\u0026#39;) # estimate quantized model latency estimate_latency_full(quantized_model, \u0026#39;quantized\u0026#39;, skip_cpu) Results:\nSize (quantized): 11.26 MB Accuracy (quantized): 80.57% Latency (quantized, on CPU): 1617.82 ± 158.67 ms Latency (quantized, on GPU): 33.62 ± 0.16 ms Optimize quantized model for inference # enable the use of the C++ wrapper for TorchInductor which reduces Python overhead import torch._inductor.config as config config.cpp_wrapper = True # compiles quantized model to generate optimized model with torch.no_grad(): optimized_model = torch.compile(quantized_model) Evaluate optimized model # get optimized model size print_size_of_model(optimized_model, \u0026#34;optimized\u0026#34;) # evaluate optimized accuracy accuracy_full = evaluate(optimized_model, \u0026#39;optimized\u0026#39;) # estimate optimized model latency estimate_latency_full(optimized_model, \u0026#39;optimized\u0026#39;, skip_cpu) Results:\nSize (optimized): 11.26 MB Accuracy (optimized): 79.54% Latency (optimized, on CPU): 831.76 ± 39.63 ms Latency (optimized, on GPU): 13.71 ± 0.24 ms While in this small-scale model the results of QAT are very similar to PTQ, it is suggested that for larger models QAT has an opportunity to provide higher accuracy than the PTQ variant.\nComparison of PTQ and QAT Results Below is a summary table comparing the baseline model with the Post Training Quantization (PTQ) and Quantization Aware Training (QAT) results based on our CIFAR-10 ResNet18 experiments:\nMethod Model Size Accuracy GPU Latency (ms) CPU Latency (ms) Baseline (no quantization) 44.77 MB 80.53% 16.39 ± 0.30 ms 804.16 ± 57.55 ms Post Training Quantization 11.26 MB 79.53% 13.80 ± 0.28 ms 782.53 ± 51.36 ms Quantization Aware Training 11.26 MB 79.54% 13.71 ± 0.24 ms 831.76 ± 39.63 ms Summary Quantization is a powerful technique for compressing and accelerating deep learning models by lowering numerical precision. PyTorch provides flexible APIs for applying both Post Training Quantization (PTQ) and Quantization Aware Training (QAT).\nUse PTQ when simplicity and speed are key, and you can tolerate some loss in accuracy. Use QAT when you need the best possible performance from quantized models, especially for smaller or sensitive models. With good calibration and training strategies, quantization can reduce model size and inference time significantly with minimal impact on performance.\nReferences For additional details, the following sources were helpful when preparing this post.\nPyTorch Documentation: Quantization PyTorch Documentation: PyTorch 2 Export Post Training Quantization PyTorch Documentation: PyTorch 2 Export Quantization-Aware Training (QAT) PyTorch Documentation: PyTorch 2 Export Quantization with X86 Backend through Inductor PyTorch Dev Discussions: TorchInductor Update 6: CPU backend performance update and new features in PyTorch 2.1 ","permalink":"https://arikpoz.github.io/posts/2025-04-16-neural-network-quantization-in-pytorch/","summary":"\u003cp\u003e\u003cimg alt=\"image here\u0026quot;\" loading=\"lazy\" src=\"/posts/2025-04-16-neural-network-quantization-in-pytorch/lead-image.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis tutorial provides an introduction to quantization in PyTorch, covering both theory and practice. We’ll explore the different types of quantization, and apply both post training quantization (PTQ) and quantization aware training (QAT) on a simple example using CIFAR-10 and ResNet18. In the presented example we achieve a \u003cstrong\u003e75% reduction in space\u003c/strong\u003e and \u003cstrong\u003e16% reduction in GPU latency\u003c/strong\u003e with only 1% drop in accuracy.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"what-is-quantization\"\u003eWhat is Quantization?\u003c/h2\u003e\n\u003cp\u003eQuantization is a model optimization technique that reduces the numerical precision used to represent weights and activations in deep learning models. Its primary benefits include:\u003c/p\u003e","title":"Neural Network Quantization in PyTorch"},{"content":"\nIntroduction In this post, I will demonstrate how to use pruning to significantly reduce a model\u0026rsquo;s size and latency while maintaining minimal accuracy loss. In the example, we achieve a 90% reduction in model size and 5.5x faster inference time, all while preserving the same level of accuracy.\nWe will begin with a brief explanation of what pruning is and why it is important. Then, I’ll provide a hands-on demonstration of applying pruning to a PyTorch model.\nOverview of Pruning Neural network pruning involves removing less important weights, channels, or neurons from a neural network to make it smaller and faster. The goal is to reduce computational costs (such as latency and memory usage) without significantly affecting model accuracy.\nDeep neural networks often contain a lot of redundancy. This redundancy arises because models are typically overparameterized to ensure high accuracy and generalization. During training, many parameters become co-dependent or have little impact on the final output. For example, multiple neurons may learn similar features, or certain filters may remain underutilized. This redundancy makes models robust but also bloated. Pruning helps streamline these models by eliminating parts that contribute the least to the output, resulting in a more efficient network that is easier to deploy on edge devices or in latency-sensitive applications.\nThere are two main types of pruning:\nUnstructured Pruning: Removes individual weights regardless of their position. While it can achieve high sparsity, it often requires specialized hardware or libraries to fully utilize the sparsity. Zeroing out individual weights typically does not improve latency because standard deep learning libraries use dense matrix multiplication regardless of how many weights are zeroed out. To benefit from sparsity, the model must be converted into a sparse format, which is often not well-supported by commodity hardware. In fact, these sparse representations can sometimes be slower than dense operations due to less optimized memory access patterns and lack of hardware acceleration. As a result, unstructured pruning offers theoretical compression but not always practical speedups unless carefully integrated into the deployment pipeline.\nStructured Pruning: Removes entire filters, channels, or layers, leading to real improvements in inference speed on standard hardware. Unlike unstructured pruning, which retains the original dense structure and thus doesn’t alter compute patterns, structured pruning directly reduces the dimensionality of tensors and layers. This means fewer floating-point operations (FLOPs) and less memory access, as the actual matrices involved in convolutions and linear operations are physically smaller. As a result, inference is faster and more efficient on standard hardware using optimized dense kernels, with no need for specialized sparse computation support.\nPyTorch Pruning API PyTorch provides a built-in pruning utility under torch.nn.utils.prune. This API supports both unstructured pruning (zeroing individual weights by magnitude or custom metrics) and structured pruning (removing entire channels or neurons). The PyTorch pruning tutorial offers a solid introduction using iterative magnitude pruning. However, it is important to note that the PyTorch pruning API does not result in real inference speedups out-of-the-box. This is because it primarily focuses on zeroing out weights rather than removing them. For unstructured pruning, it does not convert the model to a sparse representation, which is necessary to leverage computational gains. For structured pruning, it does not automatically modify the architecture to remove entire channels or filters, which means the computational graph remains unchanged.\nThat said, the PyTorch pruning API is a flexible and useful tool for experimenting with pruning strategies. It provides a simple interface to apply custom pruning criteria, evaluate sparsity effects, and implement iterative pruning and retraining loops. It is especially helpful for research and prototyping where exact hardware efficiency is less critical than functional model behavior.\nWhy Use Torch-Pruning? Structured pruning isn’t trivial. Removing a channel in one layer often requires modifying downstream layers. Structured pruning often involves complex inter-layer dependencies. For example, if you prune an output channel from a convolutional layer, any layer that consumes its output, such as a batch normalization layer or subsequent convolution, must also be updated to match the new shape. Managing these changes across many layers can be error-prone and tedious when done manually. Torch-Pruning solves this by introducing a graph-based algorithm called DepGraph, which automatically analyzes the model\u0026rsquo;s computation graph, identifies dependencies, and organizes pruning into safe and consistent execution plans.\nPractical Usage Example: Pruning ResNet-18 in PyTorch Let’s walk through pruning a ResNet-18 model step-by-step using torch-pruning. We\u0026rsquo;ll do this in Google Colab, so you can follow along easily. This example is adapted from the official README of Torch-Pruning.\nRun this code in Google Colab to try it yourself.\nSetup First, install the required library:\n!pip install torch-pruning Then, define the required imports:\nimport os import time import copy import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from torchvision import datasets, transforms, models from torch.utils.data import DataLoader import torch_pruning as tp device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) print(f\u0026#34;{device=}\u0026#34;) Get CIFAR-10 Train and Test Sets transform = transforms.Compose([ transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ]) train_loader = DataLoader( datasets.CIFAR10(root=\u0026#34;./data\u0026#34;, train=True, download=True, transform=transform), batch_size=128, shuffle=True ) test_loader = DataLoader( datasets.CIFAR10(root=\u0026#34;./data\u0026#34;, train=False, download=True, transform=transform), batch_size=256 ) Adjust ResNet-18 Network for CIFAR-10 Dataset def get_resnet18_for_cifar10(): model = models.resnet18(weights=None, num_classes=10) model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False) model.maxpool = nn.Identity() return model.to(device) full_model = get_resnet18_for_cifar10() Define Train and Evaluate Functions def train(model, loader, epochs, lr=0.01, save_path=\u0026#34;model.pth\u0026#34;, silent=False): if os.path.exists(save_path): if not silent: print(f\u0026#34;Model already trained. Loading from {save_path}\u0026#34;) model.load_state_dict(torch.load(save_path)) return criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9) model.train() for epoch in range(epochs): for x, y in loader: x, y = x.to(device), y.to(device) optimizer.zero_grad() loss = criterion(model(x), y) loss.backward() optimizer.step() if not silent: print(f\u0026#34;Epoch {epoch+1}: loss={loss.item():.4f}\u0026#34;) torch.save(model.state_dict(), save_path) if not silent: print(f\u0026#34;Training complete. Model saved to {save_path}\u0026#34;) def evaluate(model): model.eval() correct = total = 0 with torch.no_grad(): for x, y in test_loader: x, y = x.to(device), y.to(device) preds = model(x).argmax(1) correct += (preds == y).sum().item() total += y.size(0) return correct / total Define Helper Functions to Measure Latency class Timer: def __init__(self): self.use_cuda = torch.cuda.is_available() if self.use_cuda: self.starter = torch.cuda.Event(enable_timing=True) self.ender = torch.cuda.Event(enable_timing=True) def start(self): if self.use_cuda: self.starter.record() else: self.start_time = time.time() def stop(self): if self.use_cuda: self.ender.record() torch.cuda.synchronize() return self.starter.elapsed_time(self.ender) # ms else: return (time.time() - self.start_time) * 1000 # ms def estimate_latency(model, example_inputs, repetitions=50): timer = Timer() timings = np.zeros((repetitions, 1)) # Warm-up for _ in range(5): _ = model(example_inputs) with torch.no_grad(): for rep in range(repetitions): timer.start() _ = model(example_inputs) elapsed = timer.stop() timings[rep] = elapsed return np.mean(timings), np.std(timings) Train and Evaluate the Full Model train(full_model, train_loader, epochs=10, save_path=\u0026#34;full_model.pth\u0026#34;) accuracy_full = evaluate(full_model) example_input = torch.rand(128, 3, 32, 32).to(device) macs, parameters = tp.utils.count_ops_and_params(full_model, example_input) latency_mu, latency_std = estimate_latency(full_model, example_input) print(f\u0026#34;[full model] \\t\\tMACs: {macs/1e9:.2f} G, \\tParameters: {parameters/1e6:.2f} M, \\tLatency: {latency_mu:.2f} ± {latency_std:.2f} ms \\tAccuracy: {accuracy_full*100:.2f}%\u0026#34;) To save you some time, here are the results for the fully trained model:\n[full model] MACs: 0.56 G, Parameters: 11.17 M, Latency: 16.52 ± 0.03 ms Accuracy: 76.85% Prune by L2 Magnitude # Clone full model before pruning pruned_model = copy.deepcopy(full_model) pruned_model = pruned_model.to(device) # Set which layers to skip pruning. Important to keep the final classifier layer ignored_layers = [] for m in pruned_model.modules(): if isinstance(m, torch.nn.Linear) and m.out_features == 10: ignored_layers.append(m) # Iterative pruning iterative_steps = 20 pruner = tp.pruner.MagnitudePruner( model=pruned_model, example_inputs=example_input, importance=tp.importance.MagnitudeImportance(p=2), pruning_ratio=1, iterative_steps=iterative_steps, ignored_layers=ignored_layers, round_to=2, ) for iter in range(iterative_steps): # Prune pruner.step() # Evaluate after pruning acc_before = evaluate(pruned_model) # Fine-tune pruned model train(pruned_model, train_loader, epochs=1, save_path=f\u0026#34;pruned_model_{iter}.pth\u0026#34;, silent=True) # Evaluate after fine-tuning acc_after = evaluate(pruned_model) # Count MACs and parameters macs, parameters = tp.utils.count_ops_and_params(pruned_model, example_input) latency_mu, latency_std = estimate_latency(pruned_model, example_input) current_pruning_ratio = 1 / iterative_steps * (iter + 1) print(f\u0026#34;[pruned model] \\tPruning ratio: {current_pruning_ratio:.2f}, \\tMACs: {macs/1e9:.2f} G, \\tParameters: {parameters/1e6:.2f} M, \\tLatency: {latency_mu:.2f} ± {latency_std:.2f} ms \\tAccuracy pruned: {acc_before*100:.2f}%\\tFinetuned: {acc_after*100:.2f}%\u0026#34;) The pruning results show the model\u0026rsquo;s accuracy immediately after pruning and again after fine-tuning the smaller, pruned model. While accuracy initially drops following pruning, it recovers significantly after just one epoch of fine-tuning.\n[pruned model] Pruning ratio: 0.05, MACs: 0.49 G, Parameters: 10.03 M, Latency: 17.64 ± 0.04 ms Accuracy pruned: 63.60%\tFinetuned: 72.17% [pruned model] Pruning ratio: 0.10, MACs: 0.44 G, Parameters: 9.00 M, Latency: 16.12 ± 0.04 ms Accuracy pruned: 44.51%\tFinetuned: 76.51% [pruned model] Pruning ratio: 0.15, MACs: 0.40 G, Parameters: 8.01 M, Latency: 16.40 ± 0.04 ms Accuracy pruned: 66.98%\tFinetuned: 75.18% [pruned model] Pruning ratio: 0.20, MACs: 0.35 G, Parameters: 7.09 M, Latency: 16.33 ± 0.04 ms Accuracy pruned: 51.83%\tFinetuned: 74.64% [pruned model] Pruning ratio: 0.25, MACs: 0.31 G, Parameters: 6.29 M, Latency: 14.40 ± 0.05 ms Accuracy pruned: 63.51%\tFinetuned: 76.73% [pruned model] Pruning ratio: 0.30, MACs: 0.27 G, Parameters: 5.44 M, Latency: 14.07 ± 0.03 ms Accuracy pruned: 49.36%\tFinetuned: 74.64% [pruned model] Pruning ratio: 0.35, MACs: 0.23 G, Parameters: 4.69 M, Latency: 12.27 ± 0.03 ms Accuracy pruned: 58.74%\tFinetuned: 77.56% [pruned model] Pruning ratio: 0.40, MACs: 0.20 G, Parameters: 3.98 M, Latency: 12.28 ± 0.03 ms Accuracy pruned: 63.98%\tFinetuned: 78.29% [pruned model] Pruning ratio: 0.45, MACs: 0.16 G, Parameters: 3.34 M, Latency: 11.41 ± 0.02 ms Accuracy pruned: 45.66%\tFinetuned: 78.58% [pruned model] Pruning ratio: 0.50, MACs: 0.14 G, Parameters: 2.80 M, Latency: 7.06 ± 0.03 ms Accuracy pruned: 49.91%\tFinetuned: 72.77% [pruned model] Pruning ratio: 0.55, MACs: 0.11 G, Parameters: 2.24 M, Latency: 6.82 ± 0.05 ms Accuracy pruned: 38.72%\tFinetuned: 76.13% [pruned model] Pruning ratio: 0.60, MACs: 0.09 G, Parameters: 1.77 M, Latency: 5.96 ± 0.05 ms Accuracy pruned: 42.84%\tFinetuned: 79.09% [pruned model] Pruning ratio: 0.65, MACs: 0.07 G, Parameters: 1.34 M, Latency: 4.88 ± 0.09 ms Accuracy pruned: 33.88%\tFinetuned: 75.54% [pruned model] Pruning ratio: 0.70, MACs: 0.05 G, Parameters: 0.99 M, Latency: 4.17 ± 0.01 ms Accuracy pruned: 22.50%\tFinetuned: 75.60% [pruned model] Pruning ratio: 0.75, MACs: 0.04 G, Parameters: 0.70 M, Latency: 2.96 ± 0.08 ms Accuracy pruned: 34.23%\tFinetuned: 78.91% [pruned model] Pruning ratio: 0.80, MACs: 0.02 G, Parameters: 0.44 M, Latency: 2.70 ± 0.02 ms Accuracy pruned: 15.91%\tFinetuned: 75.55% [pruned model] Pruning ratio: 0.85, MACs: 0.01 G, Parameters: 0.25 M, Latency: 2.69 ± 0.04 ms Accuracy pruned: 14.16%\tFinetuned: 75.01% [pruned model] Pruning ratio: 0.90, MACs: 0.01 G, Parameters: 0.11 M, Latency: 2.63 ± 0.01 ms Accuracy pruned: 10.00%\tFinetuned: 68.87% [pruned model] Pruning ratio: 0.95, MACs: 0.00 G, Parameters: 0.03 M, Latency: 2.59 ± 0.02 ms Accuracy pruned: 10.00%\tFinetuned: 53.36% [pruned model] Pruning ratio: 1.00, MACs: 0.00 G, Parameters: 0.03 M, Latency: 2.57 ± 0.01 ms Accuracy pruned: 53.36%\tFinetuned: 54.91% Note that one of the final models achives same accuracy (even higher, 78.91%) while having 15x less parameters (0.7M vs. 11.17M), and is 5.5x faster than original (2.96 ms vs. 16.52 ms).\nSummary Pruning is a powerful technique to make deep networks lighter and faster. In this blog post, we:\nExplored what pruning is and why it matters Compared the native PyTorch pruning API with Torch-Pruning Used torch-pruning to prune a ResNet-18 model in PyTorch Evaluated model size, inference latency, and top-1 prediction accuracy using CIFAR-10 data By applying structured pruning, you can make your models more efficient with minimal impact on performance, a valuable step in any model optimization workflow.\n","permalink":"https://arikpoz.github.io/posts/2025-04-10-neural-network-pruning-how-to-accelerate-inference-with-minimal-accuracy-loss/","summary":"\u003cp\u003e\u003cimg alt=\"Cartoon of a person in an \u0026ldquo;NN Pruning\u0026rdquo; shirt trimming a large robot labeled \u0026ldquo;Neural Network\u0026rdquo; into a slim, fast robot labeled \u0026ldquo;Pruned Network.\u0026rdquo;\" loading=\"lazy\" src=\"/posts/2025-04-10-neural-network-pruning-how-to-accelerate-inference-with-minimal-accuracy-loss/lead-image.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this post, I will demonstrate how to use pruning to \u003cstrong\u003esignificantly reduce a model\u0026rsquo;s size and latency\u003c/strong\u003e while maintaining minimal accuracy loss. In the example, we achieve a \u003cstrong\u003e90% reduction in model size\u003c/strong\u003e and \u003cstrong\u003e5.5x faster inference time\u003c/strong\u003e, all while preserving the same level of accuracy.\u003c/p\u003e","title":"Neural Network Pruning: How to Accelerate Inference with Minimal Accuracy Loss"},{"content":"\nIntroduction In deep learning pipelines, especially those involving image data, data loading and preprocessing often become major bottlenecks. Traditionally, image decoding is performed using libraries like OpenCV or Pillow, which rely on CPU-based processing. After decoding, the data must be transferred to GPU memory for further operations. But what if the decoding process itself could be performed directly on the GPU? Could this lead to faster performance?\nIn this post, I demonstrate how to use nvImageCodec to achieve a 2.18x speedup in JPEG loading by decoding directly on the GPU. Learn more about nvImageCodec in its documentation or on GitHub.\n🔍 What is nvImageCodec? nvImageCodec is a high-performance image codec optimized for GPU acceleration. It is designed for scenarios like model training and batch inference, where decoding thousands of images quickly is critical. The library supports decoding (bytes to pixels) and encoding (pixels to bytes) for various common image formats. However, not all formats are fully supported on the GPU. Some, like PNG and WebP, fall back to CPU-based decoding. Below is a summary of supported formats:\n✅ Format Support: Format GPU Decode GPU Encode Notes JPEG ✅ Yes ✅ Yes Fastest, hardware-accelerated JPEG 2000 ✅ Yes ✅ Yes TIFF ✅ Yes ❌ No (planned) CUDA decoder PNG ❌ No (planned) ❌ No (planned) CPU only WebP ❌ No ❌ No CPU only 🌟 What was Benchmarked? We compared the performance of:\nOpenCV: CPU-based decoding followed by PIL transformations. nvImageCodec: GPU-based decoding with tensor transformations. Benchmark Details: Dataset: 1000 JPEG images from the ImageNet Sample Images dataset (credit: Eli Schwartz). Model: ResNet18 for inference. Transform Pipeline: Resize and crop applied to all images. Each benchmark was run 10 times (plus 1 warmup iteration), and the average times were recorded for:\n🧪 Loading: Decoding, resizing, and tensor conversion. ⚡ Inference: Model forward pass. ⏱️ Total: Combined loading and inference time. All benchmarks were conducted in Google Colab using a T4 GPU instance.\nRun this code in Google Colab to try it yourself.\n🛠️ Setup in Colab Install Dependencies and Load Dataset !pip install nvidia-nvimgcodec-cu11 opencv-python-headless !git clone https://github.com/EliSchwartz/imagenet-sample-images.git Prepare the Images import os, shutil from pathlib import Path source_dir = Path(\u0026#34;imagenet-sample-images\u0026#34;) dest_dir = Path(\u0026#34;benchmark_images\u0026#34;) dest_dir.mkdir(exist_ok=True) all_images = list(source_dir.glob(\u0026#34;*.JPEG\u0026#34;)) for img in all_images: shutil.copy(img, dest_dir / img.name) image_paths = sorted(list(dest_dir.glob(\u0026#34;*.JPEG\u0026#34;))) print(f\u0026#34;Prepared {len(image_paths)} images.\u0026#34;) Define Model and Preprocessing import torch import torchvision.transforms as transforms import torchvision.models as models from torchvision.transforms import Resize, CenterCrop device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = models.resnet18(pretrained=True).to(device).eval() transform = transforms.Compose([ Resize(256), CenterCrop(224), ]) 🧲 Benchmark Functions (10x Repeated Runs) OpenCV Benchmark def run_opencv_inference(image_paths, runs=10): import time, numpy as np from PIL import Image load_times, infer_times = [], [] for run_idx in range(runs + 1): imgs = [] t0 = time.time() for path in image_paths: img = cv2.imread(str(path)) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img = Image.fromarray(img) img = transform(img) img = transforms.ToTensor()(img) imgs.append(img) batch = torch.stack(imgs).to(device) load_time = time.time() - t0 t1 = time.time() with torch.no_grad(): model(batch) infer_time = time.time() - t1 if run_idx == 0: print(f\u0026#34;Run {run_idx + 1}: Warmup iteration (not included in mean). Loading Time = {load_time:.4f}s, Inference Time = {infer_time:.4f}s\u0026#34;) else: load_times.append(load_time) infer_times.append(infer_time) print(f\u0026#34;Run {run_idx + 1}: Loading Time = {load_time:.4f}s, Inference Time = {infer_time:.4f}s\u0026#34;) return np.mean(load_times), np.mean(infer_times) opencv_load, opencv_infer = run_opencv_inference(image_paths) nvImageCodec Benchmark def run_nvimagecodec_inference(image_paths, runs=10): import time, numpy as np decoder = nvimgcodec.Decoder(device_id=0) load_times, infer_times = [], [] for run_idx in range(runs + 1): imgs = [] t0 = time.time() for path in image_paths: with open(path, \u0026#39;rb\u0026#39;) as f: data = f.read() nv_img = decoder.decode(data) img = torch.as_tensor(nv_img.cuda()).permute(2, 0, 1).float().div(255) img = transform(img) imgs.append(img) batch = torch.stack(imgs).to(device) load_time = time.time() - t0 t1 = time.time() with torch.no_grad(): model(batch) infer_time = time.time() - t1 if run_idx == 0: print(f\u0026#34;Run {run_idx + 1}: Warmup iteration (not included in mean). Loading Time = {load_time:.4f}s, Inference Time = {infer_time:.4f}s\u0026#34;) else: load_times.append(load_time) infer_times.append(infer_time) print(f\u0026#34;Run {run_idx + 1}: Loading Time = {load_time:.4f}s, Inference Time = {infer_time:.4f}s\u0026#34;) return np.mean(load_times), np.mean(infer_times) nv_load, nv_infer = run_nvimagecodec_inference(image_paths) 📊 Results \u0026amp; Visualization import pandas as pd import matplotlib.pyplot as plt results = pd.DataFrame({ \u0026#34;Method\u0026#34;: [\u0026#34;OpenCV\u0026#34;, \u0026#34;nvImageCodec\u0026#34;], \u0026#34;Loading Time (s)\u0026#34;: [opencv_load, nv_load], \u0026#34;Inference Time (s)\u0026#34;: [opencv_infer, nv_infer], \u0026#34;Total Time (s)\u0026#34;: [ opencv_load + opencv_infer, nv_load + nv_infer ], }) print(results) results.plot(x=\u0026#34;Method\u0026#34;, y=[\u0026#34;Loading Time (s)\u0026#34;, \u0026#34;Inference Time (s)\u0026#34;, \u0026#34;Total Time (s)\u0026#34;], kind=\u0026#34;bar\u0026#34;, figsize=(10, 6)) plt.title(\u0026#34;OpenCV vs. nvImageCodec on 1000 ImageNet JPEGs (10-run average)\u0026#34;) plt.ylabel(\u0026#34;Seconds\u0026#34;) plt.grid(True) plt.show() ✅ Summary Method Loading Time (s) Inference Time (s) Total Time (s) OpenCV 6.08343 0.00349 6.08693 nvImageCodec 2.78262 0.00323 2.78585 By leveraging the T4 GPU, nvImageCodec achieves a 2.18x speedup in JPEG loading times by performing decoding directly on the GPU. This eliminates CPU bottlenecks and enables a more efficient data pipeline.\nFor workflows heavily reliant on JPEGs, integrating nvImageCodec into your training or inference pipeline can deliver substantial performance improvements with minimal effort.\nTip: Before integrating, ensure that loading time is indeed a bottleneck in your pipeline. For example, test by preloading a single image or skipping loading altogether to simulate random data. In training pipelines, prefetching images in parallel with GPU processing is also a common optimization strategy.\n","permalink":"https://arikpoz.github.io/posts/2025-04-07-fast-image-loading-with-nvidia-nvimagecodec/","summary":"\u003cp\u003e\u003cimg alt=\"OpenCV turtle loses the race with nvImageCodec rabbit\" loading=\"lazy\" src=\"/posts/2025-04-07-fast-image-loading-with-nvidia-nvimagecodec/lead-image.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eIn deep learning pipelines, especially those involving image data, \u003cstrong\u003edata loading and preprocessing\u003c/strong\u003e often become major bottlenecks. Traditionally, image decoding is performed using libraries like \u003ca href=\"https://docs.opencv.org/4.x/index.html\"\u003eOpenCV\u003c/a\u003e or \u003ca href=\"https://pillow.readthedocs.io/en/stable/\"\u003ePillow\u003c/a\u003e, which rely on CPU-based processing. After decoding, the data must be transferred to GPU memory for further operations. But what if the decoding process itself could be performed directly on the GPU? Could this lead to faster performance?\u003c/p\u003e","title":"Fast Image Loading with NVIDIA nvImageCodec"},{"content":"\nIntroduction Deep learning models have grown increasingly large and complex, enabling state-of-the-art performance in tasks such as image recognition, natural language processing, and generative AI. However, these large models often come with high computational costs, making them slow to run on edge devices, embedded systems, or even in cloud environments with strict latency requirements.\nModel compression techniques aim to reduce the size and computational requirements of neural networks while maintaining their accuracy. This enables faster inference, lower power consumption, and better deployment flexibility. In this post, we’ll explore why model compression is essential and provide an overview of four key techniques: pruning, quantization, knowledge distillation, and low-rank factorization.\nWhy Compress Neural Networks? Compression is not just about saving memory - it significantly improves inference speed and enables deployment on a wider range of hardware. Here are some key benefits:\nFaster Inference: Smaller models require fewer computations, reducing latency in real-time applications. Lower Memory Footprint: Compressed models take up less storage, making them ideal for mobile and edge devices. Reduced Power Consumption: Lower computation means lower energy usage, which is critical for battery-powered devices. Easier Deployment: Efficient models can be deployed on a broader range of hardware, including microcontrollers and IoT devices. Cost Savings: Running optimized models on lower-end hardware reduces infrastructure costs in cloud-based AI applications. Now, let\u0026rsquo;s explore the four primary methods of model compression.\n1. Pruning: Cutting Down Redundant Weights What is Pruning? Pruning removes unnecessary weights or neurons from a neural network, reducing its size without significantly impacting performance. The idea is that many parameters contribute little to the final output, and eliminating them can speed up computation.\nTypes of Pruning: Unstructured Pruning: Removes individual weights that have minimal impact on the network. Structured Pruning: Removes entire neurons, channels, or layers for a more hardware-friendly compression. Global vs. Layer-wise Pruning: Some methods prune across the entire model, while others prune within each layer independently. Use Cases and Benefits: Works well for over-parameterized models. Can be applied iteratively during training or post-training. Reduces memory usage and speeds up inference. 2. Quantization: Reducing Precision for Faster Computation What is Quantization? Quantization lowers the precision of a model’s weights and activations, reducing memory usage and enabling faster execution, particularly on specialized hardware like GPUs, TPUs, and mobile processors.\nTypes of Quantization: Post-Training Quantization: Converts a trained FP32 model to a lower precision (e.g., INT8) after training. Quantization-Aware Training: Trains the model with quantization effects simulated to minimize accuracy loss. Dynamic vs. Static Quantization: Determines whether quantization is applied per batch dynamically or precomputed for inference. Use Cases and Benefits: Significant speedup on hardware optimized for lower precision (TensorRT, OpenVINO, TFLite). Works well for inference-time optimization. Reduces model size while maintaining accuracy in many cases. 3. Knowledge Distillation: Training Small Models Using Large Models What is Knowledge Distillation? Knowledge distillation trains a smaller “student” model to mimic the behavior of a larger “teacher” model. Instead of learning directly from labeled data, the student learns from the teacher’s output distribution, capturing nuanced knowledge that direct training may miss.\nTypes of Knowledge Distillation: Logit-based Distillation: The student learns from the softened output probabilities of the teacher. Feature-based Distillation: The student mimics intermediate feature representations from the teacher. Self-Distillation: A single model is trained in stages, where later iterations learn from earlier iterations. Use Cases and Benefits: Enables smaller models to achieve near teacher-level accuracy. Useful for transferring knowledge from large pretrained models (e.g., BERT → DistilBERT). Can be used in conjunction with other compression techniques. 4. Low-Rank Factorization: Decomposing Weights for Efficiency What is Low-Rank Factorization? Low-rank factorization techniques decompose large weight matrices into smaller ones that approximate the original matrix, reducing computational cost without major accuracy loss.\nMethods of Factorization: Singular Value Decomposition (SVD): Breaks down weight matrices into simpler components. Tensor Decomposition: Extends matrix factorization to multi-dimensional tensors for convolutional layers. Factorized Convolutions: Reduces convolutional kernel complexity (e.g., depthwise separable convolutions in MobileNet). Use Cases and Benefits: Particularly useful for CNNs and Transformer models. Reduces FLOPs (floating point operations) in matrix multiplications. Can be combined with pruning and quantization for additional gains. Choosing the Right Compression Method Each compression technique has trade-offs, and the best choice depends on your target hardware and application:\nCompression Method Best For Key Benefit Potential Drawback Pruning Over-parameterized models Reduces model size May require fine-tuning Quantization Hardware acceleration Significant speedup Some accuracy loss possible Knowledge Distillation Efficient small models Retains knowledge from large models Requires a good teacher model Low-Rank Factorization CNNs, Transformers Reduces computation Approximation can impact accuracy Conclusion Model compression is a critical step in optimizing deep learning models for speed and efficiency. Each method: pruning, quantization, knowledge distillation, and low-rank factorization, offers unique advantages depending on the application.\n","permalink":"https://arikpoz.github.io/posts/2025-04-02-introduction-to-model-compression-why-and-how-to-shrink-neural-networks-for-speed/","summary":"\u003cp\u003e\u003cimg alt=\"Glowing digital brain shrinking in size, symbolizing faster, smaller neural networks\" loading=\"lazy\" src=\"/posts/2025-04-02-introduction-to-model-compression-why-and-how-to-shrink-neural-networks-for-speed/lead-image.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eDeep learning models have grown increasingly large and complex, enabling state-of-the-art performance in tasks such as image recognition, natural language processing, and generative AI. However, these large models often come with high computational costs, making them slow to run on edge devices, embedded systems, or even in cloud environments with strict latency requirements.\u003c/p\u003e\n\u003cp\u003eModel compression techniques aim to reduce the size and computational requirements of neural networks while maintaining their accuracy. This enables faster inference, lower power consumption, and better deployment flexibility. In this post, we’ll explore why model compression is essential and provide an overview of four key techniques: \u003cstrong\u003epruning, quantization, knowledge distillation, and low-rank factorization\u003c/strong\u003e.\u003c/p\u003e","title":"Introduction to Model Compression: Why and How to Shrink Neural Networks for Speed"},{"content":"\nIntroduction Neural networks are becoming increasingly powerful, but speed remains a crucial factor in real-world applications. Whether you\u0026rsquo;re running models on the cloud, edge devices, or personal hardware, optimizing them for speed can lead to faster inference, lower latency, and reduced resource consumption.\nIn this post, we\u0026rsquo;ll explore various techniques to accelerate neural networks, from model compression to hardware optimizations. This will serve as a foundation for future deep dives into each method.\n1. Model Compression: Shrinking the Network Without Losing Power One of the most effective ways to speed up a neural network is by reducing its size while maintaining performance. This can be achieved through:\nPruning – Removing unnecessary weights and neurons that contribute little to the model’s output. This reduces the number of computations needed during inference, improving speed without significantly affecting accuracy. Techniques include structured and unstructured pruning, where entire neurons or just individual weights are removed.\nQuantization – Lowering the precision of weights and activations, typically from 32-bit floating point (FP32) to 16-bit (FP16) or even 8-bit integers (INT8). Since lower precision numbers require fewer bits to store and process, inference can be significantly accelerated, especially on hardware optimized for integer operations like NVIDIA TensorRT or TensorFlow Lite.\nKnowledge Distillation – Training a smaller \u0026ldquo;student\u0026rdquo; model to mimic a larger \u0026ldquo;teacher\u0026rdquo; model. The student model learns to approximate the output of the more complex model, reducing computational overhead while maintaining accuracy. This is particularly useful for deploying models on edge devices or mobile applications.\nLow-Rank Factorization – Decomposing large weight matrices into smaller, more efficient representations. By breaking down convolutions and fully connected layers into simpler operations, low-rank factorization can reduce the number of multiplications required, speeding up inference while preserving most of the original model\u0026rsquo;s expressiveness.\n2. Graph \u0026amp; Operator Optimization: Speeding Up Computation Many deep learning frameworks support graph optimizations that fuse or restructure operations for efficiency. These techniques make computations more efficient by reducing redundant operations:\nGraph Fusion – Merging multiple operations into a single, optimized kernel. For example, in deep learning frameworks like TensorFlow and PyTorch, a convolution followed by a batch normalization operation can be fused into a single computation step, reducing memory access overhead and speeding up execution.\nONNX \u0026amp; TorchScript Optimization – Converting models into an optimized intermediate representation like ONNX (Open Neural Network Exchange) or TorchScript can allow further graph-level optimizations and compatibility with efficient runtime engines like ONNX Runtime and TensorRT.\nXLA (Accelerated Linear Algebra) – An optimization framework used in TensorFlow and JAX that compiles deep learning models into highly efficient computation graphs, enabling faster execution by reducing redundant operations and improving memory locality.\n3. Hardware Acceleration: Making the Most of Your Device Neural networks can be significantly accelerated by optimizing for specific hardware capabilities. This involves choosing the right computing resources and leveraging hardware-specific optimizations:\nUsing Specialized Libraries – Libraries like NVIDIA\u0026rsquo;s cuDNN, Intel’s MKL-DNN, and OneDNN optimize matrix multiplications and convolutions to run efficiently on specific hardware. These backends take advantage of SIMD (Single Instruction, Multiple Data) and GPU tensor cores to maximize throughput.\nChoosing the Right Hardware – Depending on your workload, selecting the right processing unit can make a huge difference. GPUs excel at parallelized matrix computations, TPUs (Tensor Processing Units) are optimized for deep learning workloads, and CPUs can still be efficient for low-latency applications, especially with vectorized instructions.\nParallelization – Splitting computations across multiple processing units to improve efficiency. Data parallelism (splitting batches across devices), model parallelism (splitting layers across devices), and tensor parallelism (splitting tensors across devices) are all used in large-scale training and inference.\n4. Efficient Inference Engines: Deploying Models Faster Deep learning frameworks are often designed for flexibility, which can lead to inefficiencies during inference. Using optimized inference engines helps streamline execution:\nTensorRT – NVIDIA’s high-performance deep learning inference engine that applies layer fusion, precision calibration, and kernel tuning to maximize speed on GPUs. It’s widely used in production AI deployments, from self-driving cars to cloud AI.\nOpenVINO – Intel’s optimization framework designed for CPUs and specialized accelerators. It converts models into an intermediate representation optimized for low-latency inference, making it a good choice for deploying models on Intel hardware, including edge devices.\nTVM – An open-source deep learning compiler that enables automatic optimization of deep learning models across different hardware backends. It applies transformations like operator fusion and memory reuse to accelerate inference without modifying the original model.\nTFLite \u0026amp; ONNX Runtime – TensorFlow Lite is optimized for mobile and embedded devices, while ONNX Runtime provides accelerated inference for models converted to the ONNX format. These are crucial for deploying models on lightweight environments with constrained resources.\n5. Batch \u0026amp; Pipeline Optimization: Handling Data Efficiently Beyond optimizing the model itself, efficiently managing input data and execution pipelines is essential for real-time applications:\nDynamic vs. Static Batching – Static batching processes fixed-size input batches, which is faster but less flexible. Dynamic batching, on the other hand, groups incoming requests into batches in real-time, optimizing performance in production settings.\nPreloading \u0026amp; Caching – Data loading can become a bottleneck in high-performance systems. Using data caching and preloading techniques (e.g., TensorFlow\u0026rsquo;s tf.data API or PyTorch’s DataLoader) ensures that the model is never waiting for input data.\nMulti-threaded Execution – Running inference using multiple CPU or GPU threads allows models to process multiple requests in parallel, improving throughput. Frameworks like TensorFlow Serving and TorchServe optimize request handling using these techniques.\nConclusion Optimizing neural networks for speed involves a combination of compression, graph restructuring, hardware tuning, inference engine selection, and data pipeline optimizations. By applying these techniques, you can significantly accelerate inference time, reduce memory footprint, and deploy models efficiently across different platforms.\n","permalink":"https://arikpoz.github.io/posts/2025-03-30-how-to-make-your-neural-network-run-faster-an-overview-of-optimization-techniques/","summary":"\u003cp\u003e\u003cimg alt=\"Illustration of a neural network transforming into a faster, optimized version\" loading=\"lazy\" src=\"/posts/2025-03-30-how-to-make-your-neural-network-run-faster-an-overview-of-optimization-techniques/lead-image.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eNeural networks are becoming increasingly powerful, but speed remains a crucial factor in real-world applications. Whether you\u0026rsquo;re running models on the cloud, edge devices, or personal hardware, optimizing them for speed can lead to faster inference, lower latency, and reduced resource consumption.\u003c/p\u003e\n\u003cp\u003eIn this post, we\u0026rsquo;ll explore various techniques to accelerate neural networks, from model compression to hardware optimizations. This will serve as a foundation for future deep dives into each method.\u003c/p\u003e","title":"How to Make Your Neural Network Run Faster: An Overview of Optimization Techniques"}]