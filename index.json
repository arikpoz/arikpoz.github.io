[{"content":"\nIntroduction Modern deep learning models are made up of hundreds or even thousands of operations. Each of these operations involves memory reads, computation, and memory writes, which when executed individually leads to substantial overhead. One of the most effective ways to cut down this overhead and boost performance is through graph fusion.\nGraph fusion, also known as operation fusion or kernel fusion, refers to the process of merging multiple operations into a single, more efficient kernel. By combining adjacent operations like a convolution followed by batch normalization and a ReLU activation into one fused unit, deep learning frameworks can avoid unnecessary memory access, reduce kernel launch overhead, and take better advantage of hardware capabilities.\nThis optimization is applied under the hood by compilers and runtime engines like PyTorch’s TorchScript or ONNX Runtime’s graph transformers. For the end user, the result is faster model execution, with no changes needed to the model’s architecture.\nIn this post, we’ll explore how graph fusion works, what types of operations can be fused, and how different frameworks apply it. We’ll also walk through a concrete PyTorch example and examine when fusion offers the biggest benefits, and when it doesn’t.\nWhat is Graph Fusion? At its core, graph fusion is a compiler optimization technique that merges multiple adjacent operations in a computational graph into a single, more efficient operation. Instead of executing each operation independently, each with its own memory reads, computation, and memory writes, fusion allows these steps to be combined into one pass, reducing overhead and improving performance.\nThink of it like combining multiple assembly lines into a single, streamlined process. For instance, a common sequence in neural networks, convolution → batch normalization → ReLU activation, can be fused into one operation that does all three steps at once. This avoids writing intermediate results to memory, launching multiple GPU kernels, or repeatedly switching contexts.\nGraph fusion can occur at various levels of abstraction:\nHigh-level graph fusion, where entire layers or ops are merged during graph transformations (e.g., fusing a Conv + BatchNorm during model export or optimization). Low-level kernel fusion, where the fused operations are implemented as a single CUDA kernel. Dynamic fusion, where operations are fused at runtime based on input shapes and execution context. The benefits of graph fusion are particularly important in deep learning workloads, where models are large, and even small inefficiencies can add up. Frameworks like PyTorch, TensorFlow, and ONNX Runtime all include backend compilers and execution engines that perform graph fusion under the hood to improve both training and inference performance.\nMotivation for Fusion To understand why graph fusion matters, we need to look at the inefficiencies in how deep learning models are typically executed.\nMost neural networks are expressed as computational graphs where each node (operation) is executed independently: a convolution runs, writes its output to memory; batch normalization reads that output, processes it, and writes its own output; then ReLU does the same, and so on. Each step involves a memory read, computation, and memory write, plus a kernel launch on the GPU or CPU.\nThis process leads to several key performance bottlenecks:\n1. Memory Bandwidth Bottlenecks\nModern accelerators like GPUs are extremely fast at computation, but they’re often limited by memory bandwidth. Writing intermediate results to memory and reading them back in the next op can consume more time than the computation itself. Fusing operations keeps intermediate values in registers or shared memory, drastically reducing memory traffic.\n2. Kernel Launch Overhead\nEach operation, especially on GPUs, requires launching a separate kernel. These launches aren’t free, they involve CPU-side scheduling, driver overhead, and context switches. By combining multiple operations into a single kernel, fusion minimizes launch overhead and improves throughput.\n3. Better Cache and Register Utilization\nFusion keeps data closer to the compute units. Instead of flushing intermediate results to global memory (where latency is high), fused kernels can use registers or local memory, resulting in better locality and faster execution.\n4. Reduced Latency and Improved Throughput\nUltimately, graph fusion speeds up inference and training. It’s especially important in latency-sensitive applications (e.g., real-time inference on edge devices), but also valuable at scale for reducing compute costs in the cloud.\nCommon Fusion Patterns While in theory many operations can be fused, in practice, fusion works best when operations are adjacent, stateless, and element-wise or mathematically composable. Most frameworks and compilers include pattern-matching passes that look for common subgraphs that can be merged. Here are some of the most frequently fused patterns in modern deep learning:\nConv + BatchNorm (+ ReLU) One of the most impactful fusion patterns. BatchNorm can be mathematically folded into Conv’s weights and bias, and ReLU can be appended as an activation. This reduces multiple operations into a single fused convolution kernel.\nLet’s break this down and see how it can be fused into a single, optimized operation.\n1. The Convolution Layer\nA standard 2D convolution outputs:\n$$ z = W * x + b $$\n$W$: convolution weights\n$b$: bias\n$*$: convolution operation\n$x$: input tensor\n$z$: output feature map\n2. The Batch Normalization Layer\nBatchNorm, applied per channel, normalizes the output of the conv layer:\n$$ \\text{BN}(z) = \\gamma \\cdot \\frac{z - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta $$\n$\\mu$, $\\sigma^2$: running mean and variance (from training or inference stats)\n$\\gamma$, $\\beta$: learned affine parameters\n$\\epsilon$: small constant for numerical stability\n3. Combine Conv and BatchNorm\nWe substitute $z = W * x + b$ into the BN expression: $$ \\text{BN}(W * x + b) = \\gamma \\cdot \\frac{W * x + b - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta $$\nLet’s define: $$ \\alpha = \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}}, \\quad \\beta\u0026rsquo; = \\beta - \\alpha \\cdot \\mu $$\nThen: $$ \\text{BN}(W * x + b) = \\alpha \\cdot (W * x + b) + \\beta - \\alpha \\cdot \\mu = \\alpha \\cdot W * x + (\\alpha \\cdot b + \\beta\u0026rsquo;) $$\nSo we can precompute: $$ W\u0026rsquo; = \\alpha \\cdot W, \\quad b\u0026rsquo; = \\alpha \\cdot b + \\beta' $$\nResulting in a single convolution with adjusted weights and biases: $$ y = W\u0026rsquo; * x + b' $$\n4. Add ReLU\nReLU is a pointwise operation: $$ y = \\text{ReLU}(W\u0026rsquo; * x + b\u0026rsquo;) $$\nBecause ReLU has no trainable parameters and is stateless, it can be appended directly to the fused operation, resulting in a fused Conv-BN-ReLU kernel.\n5. Summary\nBy folding the BatchNorm parameters into the Conv weights and biases, and applying ReLU in-place, we eliminate:\nThe need to store the intermediate result after Conv.\nOne or two extra kernel launches.\nRedundant memory bandwidth usage.\nThis fusion is both exact (no approximation) and cheap to compute, and is widely applied in inference for CNNs like ResNet, MobileNet, and EfficientNet.\nMatMul + Bias + Activation Fully connected layers often follow a matrix multiplication with a bias addition and an activation function like ReLU or GELU. These can be fused into a single GEMM (General Matrix Multiply) kernel.\nExample pattern:\nMatMul → Add (bias) → ReLU ↓ FusedLinearReLU Chained Pointwise Ops Operations like Add, Multiply, Sigmoid, Tanh, ReLU, etc., that operate element-wise on tensors can often be fused together into one kernel. This is especially helpful in transformers and MLP blocks where many such operations are chained.\nExample pattern:\nAdd → Multiply → ReLU → Dropout ↓ FusedPointwiseKernel Residual Blocks (Add + Activation) In ResNet-style architectures, skip connections end with an element-wise Add followed by an activation. These two steps are often fused in inference for lower latency.\nExample pattern:\nAdd → ReLU ↓ FusedAddReLU Normalization + Activation In transformer models, layer normalization followed by an activation like GELU is a candidate for fusion, especially in inference with fixed sequence lengths.\nFusion in Practice: Framework-Specific Techniques While the concept of graph fusion is universal, its implementation varies across deep learning frameworks. Each framework has its own compiler stack, optimization passes, and APIs to expose or trigger fusion. Here\u0026rsquo;s how fusion is handled in the most widely used ecosystems:\nPyTorch PyTorch supports several forms of fusion, mostly applied at the graph level using TorchScript or FX:\ntorch.jit.trace / torch.jit.script: These convert Python-based models into a static computation graph. During tracing or scripting, PyTorch can detect common patterns and apply operator fusion automatically.\nfuse_modules(): Common in quantization pipelines, this utility can explicitly fuse submodules like Conv2d + BatchNorm2d + ReLU. It\u0026rsquo;s used during model preparation for quantized or optimized inference.\ntorch.quantization.fuse_modules(model, [[\u0026#34;conv\u0026#34;, \u0026#34;bn\u0026#34;, \u0026#34;relu\u0026#34;]], inplace=True) FX + TorchInductor: PyTorch 2.0 introduces a new compiler stack where models are transformed with FX (Functional Transformations) and lowered to optimized kernels using TorchInductor. This system performs aggressive fusion of pointwise ops, matrix multiplies, and even fuses with custom backends like Triton.\nAOTAutograd + Triton: For training workloads, the AOTAutograd backend allows splitting the forward and backward graphs and fusing them across boundaries. Combined with Triton, it enables high-performance fused kernels.\nTensorFlow Fusion in TensorFlow is primarily handled by its optimization and compilation tools:\nGrappler: TensorFlow\u0026rsquo;s default graph optimizer. It includes fusion passes for folding BatchNorm into Conv, combining pointwise ops, and eliminating redundant ops.\nXLA (Accelerated Linear Algebra): A just-in-time compiler for TensorFlow graphs. When enabled (@tf.function(jit_compile=True)), it lowers TensorFlow ops into a highly optimized fused kernel representation using HLO (High-Level Optimizer) IR.\n@tf.function(jit_compile=True) def fused_fn(x): return tf.nn.relu(tf.nn.batch_normalization(tf.nn.conv2d(x, ...))) TF Lite \u0026amp; Edge TPU: TensorFlow Lite applies fusion passes when converting models for inference. It can fuse Conv+BN+Activation and quantize them into fused integer ops for efficient edge inference. ONNX Runtime ONNX Runtime performs fusion both during export and at runtime:\nGraph Transformers: Optimization passes like ConvBNFusion, GemmActivationFusion, and LayerNormFusion are run automatically when loading the graph, or via manual optimization scripts.\nExecution Providers (EPs): Fusion is often backend-dependent. For instance, the TensorRT EP or OpenVINO EP will apply hardware-specific fusion passes to accelerate execution.\nPretrained Model Optimizer Tools: ONNX Runtime provides CLI and Python tools (optimizer.optimize_model) that allow you to export a fused version of your model for deployment.\nExample: Fusing Conv + BatchNorm in PyTorch To see graph fusion in action, let’s walk through a practical example of fusing a convolution and batch normalization layer in PyTorch. This pattern appears frequently in convolutional neural networks like ResNet and MobileNet, and fusing it can improve inference efficiency with no loss in accuracy.\nStep 1: Define an Unfused Model\nWe’ll define a simple model with a convolution followed by batch normalization and ReLU:\nimport torch import torch.nn as nn class UnfusedConvBNReLU(nn.Module): def __init__(self): super().__init__() self.conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=True) self.bn = nn.BatchNorm2d(16) self.relu = nn.ReLU() def forward(self, x): return self.relu(self.bn(self.conv(x))) Step 2: Fuse the Layers\nPyTorch provides a convenient method to fuse layers, especially useful when preparing a model for quantization or inference optimization:\nmodel = UnfusedConvBNReLU().eval() fused = torch.quantization.fuse_modules(model, [[\u0026#34;conv\u0026#34;, \u0026#34;bn\u0026#34;, \u0026#34;relu\u0026#34;]], inplace=False).eval() Step 3: Verify Correctness\nYou can check that the fused model behaves identically (within floating point tolerance):\ninput_tensor = torch.randn(1, 3, 224, 224) with torch.no_grad(): output1 = model(input_tensor) output2 = fused(input_tensor) print(torch.allclose(output1, output2, atol=1e-5)) # Should print: True Step 4: Benchmark the Difference\nLet’s compare inference time using torch.cuda.Event for timing:\nimport time model = model.cuda() fused = fused.cuda() input_tensor = input_tensor.cuda() def benchmark(model, name): model.eval() with torch.no_grad(): # Warmup for _ in range(100): model(input_tensor) # Timing start = time.time() for _ in range(1000): model(input_tensor) torch.cuda.synchronize() end = time.time() print(f\u0026#34;{name}: {(end - start)*1000:.2f} ms\u0026#34;) benchmark(model, \u0026#34;Unfused\u0026#34;) benchmark(fused, \u0026#34;Fused\u0026#34;) Output (run on google colab\u0026rsquo;s T4):\nUnfused: 564.98 ms Fused: 226.02 ms You’ll often see small but meaningful improvements in runtime, especially for models with many such blocks.\nWhat Actually Happened?\nThe fused model doesn\u0026rsquo;t introduce new behavior, it just rewrites the operations into a more efficient execution pattern. During export or ahead-of-time compilation (e.g., TorchScript + TorchInductor), the compiler can now emit a single kernel for the entire fused block, saving memory and execution time.\nWhen Graph Fusion Helps (and When It Doesn’t) Graph fusion can dramatically improve the performance of deep learning models, but its effectiveness depends on the structure of your model, the execution environment, and the framework/compiler being used. Here’s when it delivers the most benefit, and when its impact may be limited.\nWhen Graph Fusion Helps 1. Inference on Edge Devices or CPUs\nDevices like phones, microcontrollers, and Raspberry Pi have limited memory bandwidth and compute power. Fusion reduces kernel launches and memory access, which is crucial on such constrained hardware.\n2. Large Models with Repeated Blocks\nModels like ResNet, MobileNet, or ViT use many repeatable blocks (Conv → BN → ReLU). Fusion applies uniformly across these patterns, compounding the performance benefit.\n3. Pointwise Operation Chains\nTransformers and MLPs often contain sequences of element-wise ops. Fusing them into a single kernel reduces overhead and avoids materializing unnecessary intermediate tensors.\n4. Exported or Compiled Models\nIf you export your model using TorchScript, ONNX, or TensorFlow Lite, fusion is often applied as part of the optimization pass, making deployment faster without any model changes.\n5. Latency-Critical Applications\nIn real-time systems (e.g., robotics, AR, recommendation engines), shaving off even milliseconds of latency matters. Fusion can provide quick wins without redesigning the model.\nWhen Fusion Doesn’t Help (Much) 1. Dynamic Control Flow\nIf your model includes if/while statements or data-dependent logic, fusion may not be applied. Compilers often require static graphs to match fusion patterns reliably.\n2. Already-Bound Memory Bottlenecks\nIf your model’s performance is limited by I/O, disk access, or network latency (e.g., in large-scale distributed inference), fusion might not make a noticeable dent.\n3. Small Models with Few Ops\nFor tiny models (e.g., simple MLPs with 2–3 layers), the overhead that fusion eliminates is already minimal. Gains may be negligible.\n4. Training with Frequent Weight Updates\nIn training mode, batch norm uses live batch statistics, and some fused operations (especially with quantization) may not be numerically identical. Fusion is usually more aggressive in inference.\n5. Ops with Side Effects\nCertain operations like Dropout or custom loss functions can’t always be fused, especially if they have randomness or state.\nConclusion Graph fusion is one of the most impactful, low-effort ways to optimize deep learning models. By merging multiple adjacent operations into a single fused kernel, it reduces memory access, kernel launch overhead, and runtime latency, often with zero changes to model accuracy or behavior.\nWhile fusion happens mostly under the hood, understanding how it works, and when it applies, can help you build more efficient models and make smarter deployment decisions. Whether you’re exporting a model for inference, compiling for edge devices, or optimizing training with PyTorch’s AOTAutograd, fusion plays a central role in turning your model into a high-performance executable.\nAs frameworks and compilers evolve, fusion is becoming more dynamic, hardware-aware, and integrated with other optimization techniques like quantization, pruning, and code generation. It\u0026rsquo;s no longer just a backend trick, it’s a key part of how modern deep learning systems scale.\n","permalink":"https://arikpoz.github.io/posts/2025-05-07-faster-models-with-graph-fusion-how-deep-learning-frameworks-optimize-your-computation/","summary":"\u003cp\u003e\u003cimg alt=\"\u0026ldquo;An illustration of graph fusion.\u0026rdquo;\" loading=\"lazy\" src=\"/posts/2025-05-07-faster-models-with-graph-fusion-how-deep-learning-frameworks-optimize-your-computation/lead-image.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eModern deep learning models are made up of hundreds or even thousands of operations. Each of these operations involves memory reads, computation, and memory writes, which when executed individually leads to substantial overhead. One of the most effective ways to cut down this overhead and boost performance is through \u003cstrong\u003egraph fusion\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eGraph fusion\u003c/strong\u003e, also known as operation fusion or kernel fusion, refers to the process of merging multiple operations into a single, more efficient kernel. By combining adjacent operations like a convolution followed by batch normalization and a ReLU activation into one fused unit, deep learning frameworks can avoid unnecessary memory access, reduce kernel launch overhead, and take better advantage of hardware capabilities.\u003c/p\u003e","title":"Faster Models with Graph Fusion: How Deep Learning Frameworks Optimize Your Computation"},{"content":"\nIntroduction Can we shrink neural networks without sacrificing much accuracy? Low-rank factorization is a powerful, often overlooked technique that compresses models by decomposing large weight matrices into smaller components.\nIn this post, we\u0026rsquo;ll explain what low-rank factorization is, show how to apply it to a ResNet50 model in PyTorch, and evaluate the trade-offs.\nIn our example, we achieved a 93% reduction in model size (from 94.38 MB down to 6.9 MB), cut inference latency by 10%, and incurred only a 3.5% drop in accuracy, a compelling trade-off for many practical applications!\nWhat Is Low-Rank Factorization? Low-rank factorization refers to the process of approximating a matrix with the product of two smaller matrices. If a weight matrix W has shape (m, n) and approximate rank r, we can write:\n$$ W ≈ U \\cdot V $$ where $$ U ∈ R^{m×r} , V ∈ R^{r×n} $$\nThis decomposition reduces the number of parameters from m*n to r*(m + n), which is a big win if r \u0026lt;\u0026lt; min(m, n).\nIn deep learning, many weight matrices, especially in linear and convolutional layers, are highly redundant and can be approximated well by low-rank versions. This insight is closely tied to the idea that neural networks overparameterize, and that some of this redundancy can be removed with minimal accuracy loss.\nLow-rank factorization is typically achieved by applying Singular Value Decomposition (SVD) to the weight matrix. SVD decomposes a matrix into three components, U, S, and Vᵀ, capturing its principal components. By keeping only the top singular values and corresponding vectors, we can approximate the original matrix with lower rank while preserving most of its information.\nHowever, after factorization, the model usually suffers a drop in accuracy. Fine-tuning the compressed model is typically required to regain most of the original performance.\nWhere Can We Apply It? Low-rank factorization is most applicable in the following cases:\nFully connected layers: These are easiest to factorize, since their weights are already 2D matrices.\nConvolutional layers: These need to be reshaped to 2D first (e.g., [out_channels, in_channels * kernel_h * kernel_w]), then factored, then reshaped back into two sequential convolutional layers.\nEmbedding layers: Can also benefit from factorization in NLP tasks.\nIt works best on larger layers with high-dimensional weight matrices. You can often skip very small layers or those with strong structural constraints.\nHeuristics for where to apply:\nStart with the largest layers, typically toward the end of the model Use the SVD spectrum to decide which layers are compressible, a steep drop-off in singular values suggests high redundancy PyTorch Example: Applying Low-Rank Factorization to ResNet50 In this section, we will apply low-rank factorization to a ResNet50 baseline model, trained on CIFAR-10. We will use various compression ratio to evaluate the accuracy-size-latency trade-offs.\nFor a complete Jupyter notebook implementation, visit this link. I also added a utils notebook to keep some commonly used functions. Both are part of my Neural Network Optimization GitHub repository.\nTraining Baseline Model The code below begins by adapting and training a ResNet50 model on the CIFAR-10 dataset to establish a strong baseline. To maximize the accuracy of both the baseline model and the fine-tuned compressed models, I have enhanced the train function with several advanced features:\nLearning Rate Scheduling: Automatically reduces the learning rate when the validation loss plateaus. Early Stopping: Halts training when the validation loss stops improving, preventing overfitting. Model Checkpointing: Saves the best-performing model during training for later use. Gradient Clipping: Mitigates the risk of exploding gradients by capping the gradient values during backpropagation. These improvements ensure a robust training process, yielding optimal results for both the original and compressed models.\ndevice = get_device() train_loader, val_loader, test_loader = get_cifar10_loaders() model = get_resnet50_for_cifar10(device) optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) criterion = torch.nn.CrossEntropyLoss() scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\u0026#39;min\u0026#39;, patience=3, factor=0.5) train( model, train_loader, val_loader, optimizer, criterion, device, epochs=50, scheduler=scheduler, grad_clip=1.0, save_path=\u0026#34;full_model_resnet50_best_model.pt\u0026#34;, early_stopping_patience=5, resume=True, ) Training Helper Functions Note: To facilitate reuse and maintain clarity, the commonly used functions have been relocated to a dedicated utils notebook. They are included here for reference and completeness.\ndef get_device(silent=False): \u0026#34;\u0026#34;\u0026#34; Returns the device to be used for PyTorch operations. If a GPU is available, it returns \u0026#39;cuda\u0026#39;, otherwise it returns \u0026#39;cpu\u0026#39;. \u0026#34;\u0026#34;\u0026#34; device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) if not silent: print(f\u0026#34;Using device: {device}\u0026#34;) return device def get_cifar10_loaders(train_ratio=0.9, train_batch_size=128, test_batch_size=128, silent=False): \u0026#34;\u0026#34;\u0026#34; Returns the CIFAR-10 dataset loaders for training, validation and testing. The training set is shuffled, while the test set is not. reference: Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009. \u0026#34;\u0026#34;\u0026#34; # define transform for CIFAR-10 dataset transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean = [0.49139968, 0.48215827, 0.44653124], # CIFAR-10 means std = [0.24703233, 0.24348505, 0.26158768]) # CIFAR-10 stds ]) # load full CIFAR-10 train set full_trainset = datasets.CIFAR10(root=\u0026#39;./data\u0026#39;, train=True, download=True, transform=transform) # calculate split sizes for train and validation sets train_size = int(train_ratio * len(full_trainset)) val_size = len(full_trainset) - train_size # perform split train_subset, val_subset = random_split(full_trainset, [train_size, val_size]) # create DataLoaders train_loader = DataLoader(train_subset, batch_size=train_batch_size, shuffle=True) val_loader = DataLoader(val_subset, batch_size=train_batch_size, shuffle=False) # CIFAR-10 test set and loader for accuracy evaluation test_set = datasets.CIFAR10(root=\u0026#39;./data\u0026#39;, train=False, download=True, transform=transform) test_loader = DataLoader(test_set, batch_size=test_batch_size, shuffle=False) if not silent: print(f\u0026#34;Full train set size: {len(full_trainset)}\u0026#34;) print(f\u0026#34;Train ratio: {train_ratio}\u0026#34;) print(f\u0026#34;Train samples: {len(train_subset)}\u0026#34;) print(f\u0026#34;Validation samples: {len(val_subset)}\u0026#34;) print(f\u0026#34;Test samples: {len(test_set)}\u0026#34;) print(f\u0026#34;Number of training batches: {len(train_loader)}\u0026#34;) print(f\u0026#34;Number of validation batches: {len(val_loader)}\u0026#34;) print(f\u0026#34;Number of test batches: {len(test_loader)}\u0026#34;) return train_loader, val_loader, test_loader def get_resnet50_for_cifar10(device=None): \u0026#34;\u0026#34;\u0026#34; Returns a modified ResNet-50 model for CIFAR-10 classification. \u0026#34;\u0026#34;\u0026#34; if device is None: device = get_device(silent=True) model = models.resnet50(weights=None, num_classes=10) model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False) model.maxpool = nn.Identity() return model.to(device) def train( model, train_loader, val_loader, optimizer, criterion, device, epochs, scheduler=None, grad_clip=None, save_path=\u0026#34;best_model.pt\u0026#34;, early_stopping_patience=5, resume=True ): \u0026#34;\u0026#34;\u0026#34; Trains the model using the provided data loaders, optimizer, and loss function. Supports early stopping and model checkpointing. \u0026#34;\u0026#34;\u0026#34; model.to(device) start_epoch = 0 best_val_loss = float(\u0026#34;inf\u0026#34;) epochs_without_improvement = 0 # Optional resume if resume and os.path.exists(save_path): checkpoint = torch.load(save_path, map_location=device) model.load_state_dict(checkpoint[\u0026#34;model_state\u0026#34;]) optimizer.load_state_dict(checkpoint[\u0026#34;optimizer_state\u0026#34;]) if \u0026#34;scheduler_state\u0026#34; in checkpoint and scheduler: scheduler.load_state_dict(checkpoint[\u0026#34;scheduler_state\u0026#34;]) best_val_loss = checkpoint.get(\u0026#34;best_val_loss\u0026#34;, best_val_loss) start_epoch = checkpoint.get(\u0026#34;epoch\u0026#34;, 0) + 1 print(f\u0026#34;🔁 Resumed training from epoch {start_epoch}\u0026#34;) for epoch in range(start_epoch, epochs): model.train() total_loss = 0.0 total_correct = 0 total_samples = 0 train_loop = tqdm(train_loader, desc=f\u0026#34;[Epoch {epoch+1}/{epochs}]\u0026#34;, leave=False) for inputs, targets in train_loop: inputs, targets = inputs.to(device), targets.to(device) optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() if grad_clip is not None: torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip) optimizer.step() total_loss += loss.detach() preds = outputs.argmax(dim=1) total_correct += (preds == targets).sum().item() total_samples += targets.size(0) avg_train_loss = total_loss / len(train_loader) train_accuracy = total_correct / total_samples tqdm.write(f\u0026#34;Epoch {(epoch+1):\u0026gt;3} | Train Loss: {avg_train_loss.item():.4f} | Acc: {train_accuracy:.4f}\u0026#34;) # Validation model.eval() val_loss = 0.0 val_correct = 0 val_samples = 0 with torch.no_grad(): for inputs, targets in val_loader: inputs, targets = inputs.to(device), targets.to(device) outputs = model(inputs) loss = criterion(outputs, targets) val_loss += loss.detach() preds = outputs.argmax(dim=1) val_correct += (preds == targets).sum().item() val_samples += targets.size(0) avg_val_loss = val_loss / len(val_loader) val_accuracy = val_correct / val_samples tqdm.write(f\u0026#34; | Val Loss: {avg_val_loss.item():.4f} | Acc: {val_accuracy:.4f}\u0026#34;) # Scheduler step if scheduler is not None: try: scheduler.step(avg_val_loss) # for ReduceLROnPlateau except TypeError: scheduler.step() # Early stopping + checkpoint if avg_val_loss.item() \u0026lt; best_val_loss: best_val_loss = avg_val_loss.item() epochs_without_improvement = 0 torch.save({ \u0026#34;model_state\u0026#34;: model.state_dict(), \u0026#34;optimizer_state\u0026#34;: optimizer.state_dict(), \u0026#34;scheduler_state\u0026#34;: scheduler.state_dict() if scheduler else None, \u0026#34;best_val_loss\u0026#34;: best_val_loss, \u0026#34;epoch\u0026#34;: epoch, }, save_path) tqdm.write(f\u0026#34; | ✅ New best model saved to \u0026#39;{save_path}\u0026#39;\u0026#34;) else: epochs_without_improvement += 1 tqdm.write(f\u0026#34; | No improvement for {epochs_without_improvement} epoch(s)\u0026#34;) if epochs_without_improvement \u0026gt;= early_stopping_patience: tqdm.write(f\u0026#34;🛑 Early stopping triggered after {early_stopping_patience} epochs without improvement.\u0026#34;) break print(\u0026#34;Training complete.\u0026#34;) Low-Rank Factorization Once the baseline model is established, we apply low-rank factorization to compress it. As described earlier, we iterate through all linear and convolutional layers, replacing their weight matrices wherever it results in a meaningful reduction in model size. For linear layers, the weight matrix is factorized into two smaller matrices, which are implemented as two sequential linear layers. For convolutional layers, the weight tensor is first reshaped into a 2D matrix, factorized, and then replaced with two consecutive convolutional layers that approximate the original operation.\nIt is worth noting that if the reduction in size is not substantial, the latency might slightly increase. This is due to the additional overhead of performing two matrix multiplications instead of the single operation in the original layer. This trade-off is evident in the first few rows of the results table below.\ndef compress_layer(layer, epsilon=0.10): \u0026#34;\u0026#34;\u0026#34; Compresses a layer using SVD if the compression is beneficial. Args: layer (nn.Module): The layer to compress. epsilon (float): The energy threshold for compression. Returns: nn.Module: The compressed layer or the original layer if compression is not beneficial. \u0026#34;\u0026#34;\u0026#34; # handle Linear layers if isinstance(layer, nn.Linear): # get linear layer weight matrix W = layer.weight.data.cpu() # run SVD on flat weight matrix U, S, Vh = torch.linalg.svd(W, full_matrices=False) # find rank that capture the asked energy (1-epsilon) energy = torch.cumsum(S**2, dim=0) / torch.sum(S**2) rank = torch.searchsorted(energy, 1 - epsilon).item() + 1 # check that factorization actually reduces number of parameters old_size = W.numel() new_size = rank * (W.shape[0] + W.shape[1]) if new_size \u0026lt; old_size: # define low rank factorization from SVD and rank U_r = U[:, :rank] @ torch.diag(S[:rank]) V_r = Vh[:rank, :] # define two linear layers to replace the original linear layer compressed_layer = nn.Sequential( nn.Linear(W.shape[1], rank, bias=False), nn.Linear(rank, W.shape[0], bias=True) ) compressed_layer[0].weight.data = V_r.to(device) compressed_layer[1].weight.data = U_r.to(device) compressed_layer[1].bias.data = layer.bias.data.to(device) return compressed_layer, old_size, new_size # handle Conv2d layers elif isinstance(layer, nn.Conv2d): # get convolution weight 4d matrix, shape: [out_channels, in_channels, kH, kW] W = layer.weight.data.cpu() OC, IC, kH, kW = W.shape # reshape to 2d matrix, with shape: [OC, IC*kH*kW] W_flat = W.view(OC, -1) # run SVD on flat weight matrix U, S, Vh = torch.linalg.svd(W_flat, full_matrices=False) # find rank that capture the asked energy (1-epsilon) energy = torch.cumsum(S**2, dim=0) / torch.sum(S**2) rank = torch.searchsorted(energy, 1 - epsilon).item() + 1 # check that factorization actually reduces number of parameters old_size = W.numel() new_size = rank * (IC * kH * kW + OC) if new_size \u0026lt; old_size: # define low rank factorization from SVD and rank U_r = U[:, :rank] @ torch.diag(S[:rank]) V_r = Vh[:rank, :] # define two convolutional layers to replace the original convolutional layer conv1 = nn.Conv2d( in_channels=IC, out_channels=rank, kernel_size=1, stride=1, padding=0, bias=False ) conv2 = nn.Conv2d( in_channels=rank, out_channels=OC, kernel_size=(kH, kW), stride=layer.stride, padding=layer.padding, bias=(layer.bias is not None) ) conv1.weight.data = V_r.view(rank, IC, kH, kW).to(device) conv2.weight.data = U_r.view(OC, rank, 1, 1).to(device) if layer.bias is not None: conv2.bias.data = layer.bias.data.to(device) return nn.Sequential(conv1, conv2), old_size, new_size return layer, 0, 0 # return the original layer if compression is not beneficial def compress_model(model, epsilon=0.50): \u0026#34;\u0026#34;\u0026#34; Compresses the given model by applying SVD-based compression to Linear and Conv2d layers. Args: model (nn.Module): The model to compress. epsilon (float): The energy threshold for compression. Returns: nn.Module: The compressed model. \u0026#34;\u0026#34;\u0026#34; compressed_model = deepcopy(model) # Create a copy of the input model total_old_size = 0 total_new_size = 0 for name, module in compressed_model.named_modules(): if isinstance(module, (nn.Linear, nn.Conv2d)): if \u0026#39;.\u0026#39; in name: # Check if the module has a parent parent, attr = name.rsplit(\u0026#39;.\u0026#39;, 1) parent_module = compressed_model for part in parent.split(\u0026#39;.\u0026#39;): parent_module = getattr(parent_module, part) else: # Handle top-level modules parent_module = compressed_model attr = name new_layer, old_size, new_size = compress_layer(module, epsilon) total_old_size += old_size total_new_size += new_size setattr(parent_module, attr, new_layer) return compressed_model, total_old_size, total_new_size Exploring the Impact of Epsilon on Compression and Performance The low-rank factorization method introduces a hyperparameter, epsilon, which determines the proportion of singular values to trim during compression. The code below explores a range of epsilon values to evaluate the trade-offs between compression gains and performance costs. Each compressed model is fine-tuned to maximize its performance after applying the factorization.\n# Evaluate and print metrics for the original model acc_orig = evaluate(original_model, test_loader, device) example_input = torch.rand(128, 3, 32, 32).to(device) orig_latency_mu, orig_latency_std = estimate_latency(original_model, example_input) size_orig = get_size(original_model) print(f\u0026#34;Original -\u0026gt; acc: {100*acc_orig:.2f}%, latency: {orig_latency_mu:.2f} ± {orig_latency_std:.2f} ms, size: {size_orig:.2f}MB\u0026#34;) # Iterate over epsilon values for epsilon in [round(x * 0.1, 2) for x in range(1, 10)]: print(f\u0026#34;\\nCompressing model with epsilon = {epsilon}...\u0026#34;) # Compress the model compressed_model, total_old_size, total_new_size = compress_model(original_model, epsilon=epsilon) # Evaluate compressed model before fine-tuning acc_comp = evaluate(compressed_model, test_loader, device) print(f\u0026#34;Old size: {total_old_size}, New size: {total_new_size}, Parameter count reduction: {total_old_size-total_new_size}\u0026#34;) print(f\u0026#34;Compressed -\u0026gt; acc before tuning: {100*acc_comp:.2f}%\u0026#34;) # Fine-tune the compressed model optimizer = torch.optim.Adam(compressed_model.parameters(), lr=1e-3) criterion = torch.nn.CrossEntropyLoss() scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\u0026#39;min\u0026#39;, patience=2, factor=0.5) train( compressed_model, train_loader, val_loader, optimizer, criterion, device, epochs=50, scheduler=scheduler, grad_clip=1.0, save_path=f\u0026#34;compressed_model_epsilon_{epsilon}_best_model.pt\u0026#34;, early_stopping_patience=3, resume=False, ) # Evaluate compressed model after fine-tuning acc_tuned_comp = evaluate(compressed_model, test_loader, device) comp_latency_mu, comp_latency_std = estimate_latency(compressed_model, example_input) size_comp = get_size(compressed_model) # Print metrics for the fine-tuned compressed model print(f\u0026#34;Compressed -\u0026gt; acc after tuning: {100*acc_tuned_comp:.2f}%, latency: {comp_latency_mu:.2f} ± {comp_latency_std:.2f} ms, size: {size_comp:.2f}MB\u0026#34;) Results As demonstrated in the table below, selecting the optimal compression ratio allowed us to achieve a remarkable 93% reduction in network size, shrinking it from 94.38 MB to just 6.9 MB. Additionally, we observed a 10% decrease in latency, all while incurring only a modest 3.5% drop in accuracy, a highly favorable trade-off for many practical applications.\nModel Accuracy Latency (ms) Size (MB) Size Reduction Baseline 84.64 % 71.87 ms 94.38 MB Compressed, eps=0.10 80.78 % 84.57 ms 69.86 MB -26 % Compressed, eps=0.20 80.18 % 78.12 ms 45.37 MB -52 % Compressed, eps=0.30 81.17 % 70.42 ms 28.74 MB -70 % Compressed, eps=0.40 79.75 % 68.78 ms 17.93 MB -81 % Compressed, eps=0.50 78.98 % 65.52 ms 11.14 MB -88 % Compressed, eps=0.60 81.14 % 64.47 ms 6.90 MB -93 % Compressed, eps=0.70 75.46 % 64.04 ms 4.16 MB -96 % Compressed, eps=0.80 68.11 % 63.51 ms 2.46 MB -97 % Compressed, eps=0.90 35.11 % 66.13 ms 1.37 MB -98 % When It Works, and When It Doesn’t Low-rank factorization is especially effective when:\nYou’re working with large layers You can tolerate a small accuracy drop Deployment constraints favor smaller or faster models It’s less useful when:\nThe network is already compact Most layers are small or already optimized You\u0026rsquo;re applying it blindly without analyzing the rank spectrum It’s important to validate the impact layer by layer. Not every matrix benefits from rank reduction.\nCombining with Other Techniques Low-rank factorization plays well with other compression techniques:\nPruning: Prune weights before or after factorization for even more savings. Quantization: Factor the model, then apply INT8 quantization to further shrink. Knowledge Distillation: Use a distilled model as the starting point for factorization, better performance and fewer parameters. In some settings, stacking these methods leads to additive gains with only minimal engineering overhead.\nSummary Low-rank factorization is a matrix decomposition technique that reduces the size of neural networks by approximating weight matrices with fewer parameters. This method is particularly effective for compressing large, redundant layers, achieving significant reductions in model size and latency with minimal accuracy loss when fine-tuned. By leveraging Singular Value Decomposition (SVD), it simplifies implementation and can be combined with other optimization techniques like pruning, quantization, and knowledge distillation for even greater efficiency. This post demonstrated its application on a ResNet50 model, achieving a 93% size reduction and a 10% latency improvement with only a 3.5% accuracy drop, showcasing its practicality for real-world deployments.\n","permalink":"https://arikpoz.github.io/posts/2025-04-29-low-rank-factorization-in-pytorch-compressing-neural-networks-with-linear-algebra/","summary":"\u003cp\u003e\u003cimg alt=\"\u0026ldquo;Diagram of low-rank factorization compressing ResNet50, showing 93% size reduction, 10% lower latency, and 3.5% accuracy drop.\u0026rdquo;\" loading=\"lazy\" src=\"/posts/2025-04-29-low-rank-factorization-in-pytorch-compressing-neural-networks-with-linear-algebra/lead-image.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eCan we shrink neural networks without sacrificing much accuracy?\nLow-rank factorization is a powerful, often overlooked technique that compresses models by decomposing large weight matrices into smaller components.\u003c/p\u003e\n\u003cp\u003eIn this post, we\u0026rsquo;ll explain what low-rank factorization is, show how to apply it to a ResNet50 model in PyTorch, and evaluate the trade-offs.\u003c/p\u003e","title":"Low-Rank Factorization in PyTorch: Compressing Neural Networks with Linear Algebra"},{"content":"\nIntroduction What if your model could run twice as fast and use half the memory, without giving up much accuracy?\nThis is the promise of knowledge distillation: training smaller, faster models to mimic larger, high-performing ones. In this post, we’ll walk through how to distill a powerful ResNet50 model into a lightweight ResNet18 and demonstrate a +5% boost in accuracy compared to training the smaller model from scratch, all while cutting inference latency by over 50%.\nYou\u0026rsquo;ll learn:\nWhat knowledge distillation is and how it works Why it’s useful for deployment on resource-constrained devices How to implement it in PyTorch using both soft target alignment and intermediate feature matching How a distilled ResNet18 performs against a baseline trained from scratch What Is Knowledge Distillation? Knowledge distillation, first proposed by Hinton et al., is a technique designed to transfer the rich, nuanced information, often referred to as \u0026ldquo;dark knowledge\u0026rdquo;, from a large, high-capacity model (the teacher model) to a smaller, more efficient model (the student model). This process enables the student model to mimic the teacher\u0026rsquo;s behavior, capturing its insights and generalization capabilities while maintaining a significantly reduced size.\nThe key idea: rather than training the student only on ground-truth labels, we also train it to mimic the output distribution of the teacher model. These soft targets contain valuable information about class relationships that can help the student generalize better.\nIf the teacher and student architectures are compatible, the student model can be trained to mimic not only the teacher\u0026rsquo;s output distribution but also its intermediate feature representations from inner layers. By aligning the student\u0026rsquo;s internal feature maps with those of the teacher, this additional supervision enables the student to better capture the teacher\u0026rsquo;s reasoning process. The following code example demonstrates how to implement this approach effectively.\nWhy Use It? Reduce model size for mobile \u0026amp; embedded devices Achieve faster inference with smaller models Maintain much of the accuracy of larger models Leverage expensive pretrained models efficiently How Does It Work? The training loss for the student typically combines:\nCross-entropy loss with the ground-truth labels (hard targets) KL divergence between the student and teacher soft logits (soft targets) To understand the second part, let’s first recall how the softmax function works:\n$$ P_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}} $$\nThis turns the raw model logits into a probability distribution. In a well-trained model, this distribution is often very \u0026ldquo;peaked\u0026rdquo;, assigning high confidence to one class and nearly zero to others.\nFor example, regular softmax might output: [0.95, 0.02, 0.01, 0.01, 0.01]\nThese probabilities are not very informative beyond the top prediction.\nTemperature Scaling To soften this distribution and reveal more information about the model’s understanding of class relationships, we introduce a temperature parameter ( T \u0026gt; 1 ):\n$$ P_i^{(T)} = \\frac{e^{z_i / T}}{\\sum_j e^{z_j / T}} $$\nWith a higher temperature:\nThe probability distribution becomes more spread out We get outputs like: [0.4, 0.2, 0.15, 0.15, 0.1] The student learns not just the right answer, but how the teacher differentiates among all classes This is the core of knowledge distillation: using these soft targets alongside the hard labels to teach a student network not just what to predict, but how to think like the teacher.\nPyTorch Example: ResNet50 ➞ ResNet18 on CIFAR-10 In this section, we will take a large teacher model, specifically a ResNet50 pretrained on ImageNet1K and fine-tuned on CIFAR-10, and distill its knowledge into a smaller, more efficient ResNet18 model.\nWe will then evaluate and compare the teacher and student models in terms of accuracy, latency, and size. Additionally, we will assess the performance of the student model trained directly on CIFAR-10 without knowledge distillation, to highlight the benefits of this technique.\nFor a complete, self-contained Jupyter Notebook implementation, visit this link. It is part of my Neural Network Optimization GitHub repository.\nBasic Setup import os import time import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import random_split, DataLoader from torch.optim.lr_scheduler import ReduceLROnPlateau from torchvision import models import torchvision.transforms as transforms from torchvision.datasets import CIFAR10 print(f\u0026#34;PyTorch Version: {torch.__version__}\u0026#34;) device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) print(f\u0026#34;Device used: {device.type}\u0026#34;) Load Dataset Loads the CIFAR-10 data, prepare train / validation / test split and creates data loaders.\n# define transform for CIFAR-10 dataset transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], # CIFAR-10 means std=[0.2023, 0.1994, 0.2010]) ]) # load full CIFAR-10 train set full_trainset = CIFAR10(root=\u0026#39;./data\u0026#39;, train=True, download=True, transform=transform) # calculate split sizes for train and validation sets train_size = int(0.9 * len(full_trainset)) val_size = len(full_trainset) - train_size # perform split train_subset, val_subset = random_split(full_trainset, [train_size, val_size]) print(f\u0026#34;Train samples: {train_size}\u0026#34;) print(f\u0026#34;Validation samples: {val_size}\u0026#34;) # create DataLoaders train_loader = DataLoader(train_subset, batch_size=128, shuffle=True) val_loader = DataLoader(val_subset, batch_size=128, shuffle=False) # CIFAR-10 test set and loader for accuracy evaluation test_set = CIFAR10(root=\u0026#39;./data\u0026#39;, train=False, download=True, transform=transform) test_loader = DataLoader(test_set, batch_size=128, shuffle=False) print(f\u0026#34;Test samples: {len(test_set)}\u0026#34;) Output:\nTrain samples: 45000 Validation samples: 5000 Test samples: 10000 Define Models In this example, we enhance the knowledge distillation process by training the student model to learn not only from the teacher model\u0026rsquo;s output distribution but also from its intermediate feature representations. Since ResNet50 and ResNet18 share a similar architecture, their intermediate features can be aligned for additional supervision. However, the number of channels in their feature maps differs. To address this, we introduce a 1x1 convolutional layer to project the teacher\u0026rsquo;s feature space into the student\u0026rsquo;s feature space, enabling effective feature matching between the two models.\ndef setup_models(device): \u0026#34;\u0026#34;\u0026#34; Setup teacher and student wrapper \u0026#34;\u0026#34;\u0026#34; # teacher: ResNet50 pretrained on ImageNet, re-headed for CIFAR-10 teacher = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2) teacher.fc = nn.Linear(2048, 10) teacher = teacher.to(device) # student: ResNet18 without pretrained weights student = models.resnet18(weights=None) student.fc = nn.Linear(512, 10) student = student.to(device) # define the intermediate feature channels for both teacher and student student_channels = [64, 128, 256, 512] teacher_channels = [256, 512, 1024, 2048] # create projection layers to align teacher\u0026#39;s feature maps with student\u0026#39;s feature maps proj_layers = [ FeatureProjector(in_c, out_c).to(device) for in_c, out_c in zip(student_channels, teacher_channels) ] # wrap the student model with the projection layers student_wrapper = StudentWrapper(student, proj_layers).to(device) return teacher, student_wrapper class FeatureProjector(nn.Module): \u0026#34;\u0026#34;\u0026#34; Feature projector to match student -\u0026gt; teacher feature shapes \u0026#34;\u0026#34;\u0026#34; def __init__(self, in_channels, out_channels): super().__init__() # define a 1x1 convolutional layer to project feature maps self.proj = nn.Conv2d(in_channels, out_channels, kernel_size=1) def forward(self, x, target_shape): # check if the spatial dimensions of the input match the target shape if x.shape[2:] != target_shape[2:]: # adjust spatial dimensions using adaptive average pooling x = F.adaptive_avg_pool2d(x, output_size=target_shape[2:]) # apply the projection layer to transform feature maps return self.proj(x) class StudentWrapper(nn.Module): \u0026#34;\u0026#34;\u0026#34; Wrapper for the student model with projection layers \u0026#34;\u0026#34;\u0026#34; def __init__(self, student_model, proj_layers): super().__init__() # store student model self.model = student_model # store projection layers for feature alignment self.projections = nn.ModuleList(proj_layers) def forward(self, x): # collect intermediate features from ResNet blocks features = [] x = self.model.conv1(x) x = self.model.bn1(x) x = self.model.relu(x) x = self.model.maxpool(x) for i, block in enumerate([self.model.layer1, self.model.layer2, self.model.layer3, self.model.layer4]): # pass through ResNet blocks x = block(x) # append features from each block features.append(x) # pool the final feature map and compute logits pooled = F.adaptive_avg_pool2d(x, (1, 1)) flat = torch.flatten(pooled, 1) logits = self.model.fc(flat) return logits, features def project_features(self, features, target_shapes): \u0026#34;\u0026#34;\u0026#34; Project student features to match the shapes of teacher features. \u0026#34;\u0026#34;\u0026#34; return [ proj(s_feat, t_shape) for s_feat, t_shape, proj in zip(features, target_shapes, self.projections) ] def extract_teacher_features(model, x, layers=[1, 2, 3, 4]): \u0026#34;\u0026#34;\u0026#34; Extract teacher logits and intermediate features \u0026#34;\u0026#34;\u0026#34; # collect intermediate features from ResNet blocks features = [] x = model.conv1(x) x = model.bn1(x) x = model.relu(x) x = model.maxpool(x) for i, block in enumerate([model.layer1, model.layer2, model.layer3, model.layer4]): x = block(x) if (i + 1) in layers: features.append(x) # pool the final feature map and compute logits pooled = F.adaptive_avg_pool2d(x, (1, 1)) # [B, C, 1, 1] flat = torch.flatten(pooled, 1) # [B, C] logits = model.fc(flat) # [B, 10] return logits, features # setup models teacher, student_wrapper = setup_models(device) Evaluation Functions for Size, Latency and Accuracy def count_params(model): \u0026#34;\u0026#34;\u0026#34; Function to count trainable parameters \u0026#34;\u0026#34;\u0026#34; return sum(p.numel() for p in model.parameters() if p.requires_grad) def measure_latency(model, input_size=(1, 3, 32, 32), device=\u0026#39;cuda\u0026#39;, repetitions=50): \u0026#34;\u0026#34;\u0026#34; Function to measure average inference latency over multiple runs \u0026#34;\u0026#34;\u0026#34; model.eval() inputs = torch.randn(input_size).to(device) with torch.no_grad(): # Warm-up for _ in range(10): _ = model(inputs) # Measure times = [] for _ in range(repetitions): start = time.time() _ = model(inputs) end = time.time() times.append(end - start) return (sum(times) / repetitions) * 1000 # ms def evaluate_accuracy(model, dataloader): \u0026#34;\u0026#34;\u0026#34; Evaluate accuracy given model and loader \u0026#34;\u0026#34;\u0026#34; model.eval() model.to(device) correct, total = 0, 0 with torch.no_grad(): for inputs, labels in dataloader: inputs, labels = inputs.to(device), labels.to(device) outputs = model(inputs) preds = outputs.argmax(dim=1) correct += (preds == labels).sum().item() total += labels.size(0) accuracy = correct / total return accuracy Fine-tuning the Teacher Note: This fine-tuning is required only in this example since the ResNet50 network was pretrained on ImageNet1K and we need to replace its output layer to match to CIFAR-10.\ndef train_teacher(teacher, loader, epochs, tag, lr=1e-3, save_path=\u0026#34;model.pth\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Trains a model with Adam and cross-entropy loss. Loads from save_path if it exists. \u0026#34;\u0026#34;\u0026#34; if os.path.exists(save_path): print(f\u0026#34;Model already trained. Loading from {save_path}\u0026#34;) teacher.load_state_dict(torch.load(save_path)) return teacher # no saved model found. training from given model state optimizer = torch.optim.Adam(teacher.parameters(), lr=1e-3) teacher.train() for epoch in range(epochs): for inputs, labels in loader: inputs, labels = inputs.to(device), labels.to(device) logits, _ = extract_teacher_features(teacher, inputs) loss = F.cross_entropy(logits, labels) optimizer.zero_grad() loss.backward() optimizer.step() accuracy = evaluate_accuracy(teacher, val_loader) print(f\u0026#34;({tag})\\tEpoch {epoch+1}: loss={loss.item():.4f}, Accuracy (validation): {accuracy*100:.2f}%\u0026#34;) teacher.train() if save_path: torch.save(teacher.state_dict(), save_path) print(f\u0026#34;Training complete. Model saved to {save_path}\u0026#34;) return teacher # train the teacher on CIFAR-10 teacher = train_teacher(teacher, train_loader, epochs=25, tag=\u0026#34;Fine-tuning teacher\u0026#34;, save_path=\u0026#34;tuned_pretrained_resnet50_on_CIFAR10.pth\u0026#34;) Output:\n(Fine-tuning teacher)\tEpoch 1: loss=0.4818, Accuracy (validation): 81.60% (Fine-tuning teacher)\tEpoch 2: loss=0.6269, Accuracy (validation): 81.70% (Fine-tuning teacher)\tEpoch 3: loss=0.2588, Accuracy (validation): 82.72% (Fine-tuning teacher)\tEpoch 4: loss=0.2500, Accuracy (validation): 82.88% (Fine-tuning teacher)\tEpoch 5: loss=0.1956, Accuracy (validation): 83.52% ... (Fine-tuning teacher)\tEpoch 25: loss=0.0483, Accuracy (validation): 84.14% Training complete. Model saved to tuned_pretrained_resnet50_on_CIFAR10.pth Training the Student via Distillation The distillation loss is calculated with KL divergence on the student logits and teacher logits\ndef distillation_loss(student_logits, teacher_logits, targets, T=5.0, alpha=0.7): \u0026#34;\u0026#34;\u0026#34; Combine soft and hard targets using KL divergence and cross-entropy T = temperature, alpha = weighting between soft and hard losses \u0026#34;\u0026#34;\u0026#34; # soft target loss (teacher softmax vs student softmax) soft_targets = F.kl_div( F.log_softmax(student_logits / T, dim=1), F.softmax(teacher_logits / T, dim=1), reduction=\u0026#39;batchmean\u0026#39; ) * (T * T) # hard label loss hard_loss = F.cross_entropy(student_logits, targets) return alpha * soft_targets + (1 - alpha) * hard_loss def student_training_step(inputs, labels, teacher, student_wrapper, optimizer, device): \u0026#34;\u0026#34;\u0026#34; Perform a single training step for the student model using knowledge distillation. \u0026#34;\u0026#34;\u0026#34; inputs, labels = inputs.to(device), labels.to(device) # extract teacher logits and intermediate features with torch.no_grad(): teacher_logits, teacher_feats = extract_teacher_features(teacher, inputs) # extract student logits and intermediate features student_logits, student_feats = student_wrapper(inputs) projected_feats = student_wrapper.project_features(student_feats, [t.shape for t in teacher_feats]) # calculate loss from features difference feat_loss = sum(F.mse_loss(p, t.detach()) for p, t in zip(projected_feats, teacher_feats)) # calculate loss from output distribution, and include feature loss loss = distillation_loss(student_logits, teacher_logits, labels) + 0.1 * feat_loss # optimize with loss optimizer.zero_grad() loss.backward() optimizer.step() return loss.item() def train_student(teacher, student_wrapper, dataloader, epochs, save_path=\u0026#34;student_distilled.pth\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Trains a student model using knowledge distillation from a teacher model. \u0026#34;\u0026#34;\u0026#34; # setup optimizer optimizer = torch.optim.Adam(student_wrapper.parameters(), lr=1e-3) # train the student using the teacher\u0026#39;s output as soft targets teacher.eval() best_val_acc = 0.0 # reduce LR if validation loss doesn\u0026#39;t improve for 3 epochs scheduler = ReduceLROnPlateau(optimizer, mode=\u0026#39;min\u0026#39;, factor=0.5, patience=3) for epoch in range(epochs): student_wrapper.train() running_loss = 0 for inputs, labels in dataloader: loss = student_training_step(inputs, labels, teacher, student_wrapper, optimizer, device) running_loss += loss val_acc = evaluate_accuracy(student_wrapper.model, val_loader) print(f\u0026#34;[(Training student)\\tEpoch {epoch+1}] Loss = {running_loss/len(dataloader):.4f} | Val Acc = {val_acc*100:.2f}%\u0026#34;) scheduler.step(loss) # save best checkpoint if val_acc \u0026gt; best_val_acc: best_val_acc = val_acc torch.save(student_wrapper.state_dict(), save_path) print(\u0026#34;New best model saved.\u0026#34;) # load best checkpoint student_wrapper.load_state_dict(torch.load(save_path)) student = student_wrapper.model return student # trigger student training student = train_student(teacher, student_wrapper, train_loader, epochs = 25) Output:\n[(Training student)\tEpoch 1] Loss = 8.7847 | Val Acc = 60.02% New best model saved. [(Training student)\tEpoch 2] Loss = 5.9411 | Val Acc = 65.92% New best model saved. [(Training student)\tEpoch 3] Loss = 4.8069 | Val Acc = 69.38% New best model saved. [(Training student)\tEpoch 4] Loss = 4.0791 | Val Acc = 71.84% New best model saved. [(Training student)\tEpoch 5] Loss = 3.4702 | Val Acc = 74.46% New best model saved. ... [(Training student)\tEpoch 20] Loss = 0.3931 | Val Acc = 78.62% Model Comparison Code Finally we check the size of the teacher and student models, their latency and accuracy on test set.\n# compare size, latency, and accuracy teacher_params = count_params(teacher) student_params = count_params(student) teacher_latency = measure_latency(teacher, device=device) student_latency = measure_latency(student, device=device) teacher_acc = evaluate_accuracy(teacher, test_loader) student_acc = evaluate_accuracy(student, test_loader) print(f\u0026#34;Teacher Params: {teacher_params / 1e6:.2f}M\u0026#34;) print(f\u0026#34;Student Params: {student_params / 1e6:.2f}M\u0026#34;) print(f\u0026#34;Teacher Latency: {teacher_latency:.2f} ms\u0026#34;) print(f\u0026#34;Student Latency: {student_latency:.2f} ms\u0026#34;) print(f\u0026#34;Teacher Test Accuracy: {teacher_acc * 100:.2f}%\u0026#34;) print(f\u0026#34;Student Test Accuracy: {student_acc * 100:.2f}%\u0026#34;) Output:\nTeacher Params: 23.53M Student Params: 11.18M Teacher Latency: 3.82 ms Student Latency: 1.54 ms Teacher Test Accuracy: 85.10% Student Test Accuracy: 79.24% Training a baseline student (ResNet18 from scratch) Although the student model is half the size and 40% the latency of the teacher model, its accuracy dropped from 85.10% to 79.24%. To determine whether this is still a better approach compared to training the student model directly on the data, we trained another student model for the same number of epochs without using knowledge distillation. We refer to this model as the \u0026ldquo;baseline student\u0026rdquo;.\n# define baseline student: ResNet18 training from scratch on its own, re-headed for CIFAR-10 baseline_student = models.resnet18(weights=None) baseline_student.fc = nn.Linear(512, 10).to(device) baseline_student = baseline_student.to(device) # Train the baseline student on CIFAR-10 baseline_student = train_teacher(baseline_student, train_loader, epochs=25, tag=\u0026#34;baseline-student\u0026#34;, save_path=\u0026#34;baseline_student.pth\u0026#34;) # Evaluate baseline student baseline_student_acc = evaluate_accuracy(baseline_student, test_loader) print(f\u0026#34;\\nBaseline Student Test Accuracy: {baseline_student_acc * 100:.2f}%\u0026#34;) Output:\n(baseline-student)\tEpoch 1: loss=1.0124, Accuracy (Epoch 1): 58.06% (baseline-student)\tEpoch 2: loss=0.8091, Accuracy (Epoch 2): 63.10% (baseline-student)\tEpoch 3: loss=0.8210, Accuracy (Epoch 3): 68.06% (baseline-student)\tEpoch 4: loss=0.7438, Accuracy (Epoch 4): 68.74% (baseline-student)\tEpoch 5: loss=0.8731, Accuracy (Epoch 5): 71.18% ... (baseline-student)\tEpoch 25: loss=0.0202, Accuracy (Epoch 25): 74.10% Training complete. Model saved to baseline_student.pth Saved fine-tuned teacher. Baseline Student Test Accuracy: 74.10% Does Knowledge Distillation Help? Training a baseline student model for the same number of epochs, without leveraging knowledge distillation, results in a test accuracy of 74.10%. In contrast, the distilled student achieves a significantly higher test accuracy of 79.24%. This represents a notable improvement of 5.14 percentage points, achieved without any increase in model size or latency.\nThis shows that while the student has lower capacity than the teacher, distillation helps it generalize better by learning not just from ground-truth labels, but also from the richer output distribution of the teacher. The teacher’s \u0026ldquo;dark knowledge\u0026rdquo; encodes class similarities and decision boundaries that the student wouldn’t otherwise see.\nEven when both models are trained on the same data, distillation acts as a form of regularization, guiding the student with softer, more informative targets. This helps the student generalize better than it would by learning from hard labels alone, effectively helping a smaller model punch above its weight.\nModel Comparison Table Model Parameters Latency (ms) Accuracy (approx) How it was trained? ResNet50 (teacher) 23.53M 3.82 ms 85.10% Fine-tuned on CIFAR ResNet18 (distilled student) 11.18M 1.54 ms 79.24% Knowledge distillation ResNet18 (baseline student) 11.18M 1.54 ms 74.10% Trained directly Summary Knowledge distillation is an elegant technique to train smaller models with guidance from larger ones. The use of softened outputs via temperature scaling helps the student capture richer information. Internal representations can be leveraged to establish a stronger connection between the teacher and student models, potentially enhancing the student\u0026rsquo;s performance. This method works well in practice to compress models for deployment without drastically sacrificing performance. References Distilling the Knowledge in a Neural Network - Hinton et al. TorchVision ResNet \u0026amp; MobileNet ","permalink":"https://arikpoz.github.io/posts/2025-04-24-knowledge-distillation-in-pytorch-shrinking-neural-networks-the-smart-way/","summary":"\u003cp\u003e\u003cimg alt=\"\u0026ldquo;A glowing teacher neural network transferring knowledge to a smaller student model, with the title \u0026lsquo;Knowledge Distillation\u0026rsquo; overlaid.\u0026rdquo;\" loading=\"lazy\" src=\"/posts/2025-04-24-knowledge-distillation-in-pytorch-shrinking-neural-networks-the-smart-way/lead-image.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eWhat if your model could run twice as fast and use half the memory, without giving up much accuracy?\u003c/strong\u003e\u003cbr\u003e\nThis is the promise of \u003cstrong\u003eknowledge distillation\u003c/strong\u003e: training smaller, faster models to mimic larger, high-performing ones. In this post, we’ll walk through how to distill a powerful ResNet50 model into a lightweight ResNet18 and demonstrate a \u003cstrong\u003e+5% boost in accuracy\u003c/strong\u003e compared to training the smaller model from scratch, all while cutting inference latency by over \u003cstrong\u003e50%\u003c/strong\u003e.\u003c/p\u003e","title":"Knowledge Distillation in PyTorch: Shrinking Neural Networks the Smart Way"},{"content":"\nIntroduction This tutorial provides an introduction to quantization in PyTorch, covering both theory and practice. We’ll explore the different types of quantization, and apply both post training quantization (PTQ) and quantization aware training (QAT) on a simple example using CIFAR-10 and ResNet18. In the presented example we achieve a 75% reduction in space and 16% reduction in GPU latency with only 1% drop in accuracy.\nWhat is Quantization? Quantization is a model optimization technique that reduces the numerical precision used to represent weights and activations in deep learning models. Its primary benefits include:\nModel Compression - lowers memory usage and storage. Inference Acceleration - speeds up inference and reduces energy consumption. While quantization is most often used for deployment on edge devices (e.g., phones, embedded hardware), it can also reduce infrastructure costs for large-scale inference in the cloud.\nWhy Quantize Weights and Activations? Quantization typically targets both weights and activations, and each serves a different purpose in optimizing model deployment:\nWhy Quantize Weights? Storage savings: Weights are the learned parameters of a model and are saved to disk. Reducing their precision (e.g., from float32 to int8) significantly shrinks the size of the model file. Faster model loading: Smaller weights reduce model loading time, which is especially useful for mobile and edge deployments. Reduced memory footprint: On-device memory use is lower, which allows running larger models or multiple models concurrently. Why Quantize Activations? Runtime efficiency: Activations are the intermediate outputs of each layer computed during the forward pass. Lower-precision activations (e.g., int8 instead of float32) require less memory bandwidth and compute. End-to-end low-precision execution: Quantizing both weights and activations enables optimized hardware kernels (e.g., int8 × int8 → int32) to be used throughout the network, maximizing speed and energy efficiency. Better cache locality: Smaller activation tensors are more cache-friendly, leading to faster inference. Quantizing only the weights can reduce model size but won’t deliver full runtime acceleration. Quantizing both weights and activations is essential to fully unlock the benefits of quantized inference on CPUs, mobile chips, and specialized accelerators.\nTypes of Quantization The two most common approaches to quantization fall into these categories:\nFloating-Point Quantization Floating-point quantization reduces the bit-width of real-valued tensors, typically from 32-bit (float32) to 16-bit (float16 or bfloat16). These formats use fewer bits for the exponent and mantissa, resulting in lower precision but maintaining the continuous range and general expressiveness of real numbers.\nUses 16 bits instead of 32 (e.g., float16, bfloat16). Preserves dynamic range and real-number structure. Maintains relatively high accuracy. Supported efficiently on modern hardware (e.g., GPUs, TPUs). The diagram below compares the internal bit layout of float32 , float16 , and bfloat16 using color-coded segments for the sign, exponent, and mantissa bits:\nbfloat16 , developed by Google Brain, is especially notable because it retains the full 8-bit exponent of float32 , offering a wide dynamic range. While its 7-bit mantissa provides less precision, this makes it more numerically stable than float16 , particularly for training deep networks\nInteger Quantization Integer quantization maps real-valued numbers to a discrete integer range using an affine transformation. This process enables efficient inference using low-precision arithmetic.\nQuantization: $q = \\text{round}\\left(\\frac{x}{s}\\right) + z$\nDequantization: $x \\approx s \\cdot (q - z)$\nWhere:\nx is the original float q is the quantized integer s is the scale (a float) z is the zero-point (an int) These mappings let the model operate primarily with integers during inference, reducing memory usage and enabling faster execution on integer-optimized hardware.\nHow Are Scale and Zero-Point Determined? (Calibration) The scale and zero-point are calculated based on the distribution of float values in a tensor. Typically:\nScale (s ) is derived from the min and max float values of the tensor, and the min and max values of the quantized range (0-255 for uint8 or -128 to 127 for int8)\ns = (x_max - x_min) / (q_max - q_min) Zero-point (z ) ensures that 0 is exactly representable:\nz = round(q_min - x_min / s) This process of determining the appropriate scale and zero-point by observing real-valued data flowing through the model is known as calibration. It is especially important for static quantization, where activation ranges are fixed based on representative input data.\nThese parameters are then stored along with each quantized tensor. There are two main approaches:\nPer-tensor quantization: One scale and zero-point for the entire tensor. Per-channel quantization: Separate scale and zero-point per output channel (commonly used for weights in convolutional layers). During inference, these values are used to convert between quantized and real representations efficiently. Some characteristics: Aggressive memory/computation savings. May introduce more quantization error. Commonly used in edge-optimized frameworks like TensorFlow Lite and PyTorch Mobile. Tradeoffs Quantization enables efficient inference but can degrade accuracy, especially if done post training without calibration. To minimize this, modern techniques like Quantization Aware Training (QAT) are used, see below.\nWhen Is Quantization Applied? Quantization can be applied at different stages in the model lifecycle. The two primary approaches are Post Training Quantization (PTQ) and Quantization Aware Training (QAT), each with its own benefits and tradeoffs.\nPost Training Quantization (PTQ) PTQ is applied to a fully trained model without requiring any retraining. It’s simple and quick to implement, but may cause some degradation in model accuracy, especially when using aggressive quantization like int8.\nAdvantages:\nEasy to integrate into existing workflows No need to modify training code Can dramatically reduce model size and inference cost Limitations:\nAccuracy may drop, especially for sensitive models or tasks Works best on models that are already robust to small numeric changes Variants:\nDynamic Quantization:\nWhen? After training. What? Only weights are quantized and stored in int8. Activations remain in float and are quantized dynamically during inference. How? No calibration needed. Activation ranges are computed on-the-fly at runtime. Pros: Easy to apply; works well for models with large nn.Linear layers (e.g., NLP). Cons: Some operations still use float intermediates; less efficient than static quantization. Static Quantization:\nWhen? After training. What? Both weights and activations are quantized to int8. How? Requires calibration, passing representative data through the model to collect activation stats. Pros: Enables full integer inference; maximizes performance. Cons: Slightly more setup due to calibration requirement. Weight-only Quantization:\nWhen? After training. What? Only weights are quantized; activations remain float32. How? No activation quantization, so no calibration needed. Pros: Reduces model size. Cons: Limited inference speedup since activations are still float. Only weights are quantized; activations remain in float. Saves memory, but yields limited inference speedup. Quantization Aware Training (QAT) QAT introduces quantization effects during the training process by simulating them using fake quantization operations. These operations emulate the behavior of quantization during the forward pass (e.g., rounding, clamping) while still allowing gradients to flow through in full precision during the backward pass. This enables the model to learn to be robust to quantization effects while maintaining effective training dynamics.\nAdvantages:\nHighest accuracy among quantized models Especially useful for smaller or sensitive models that suffer from PTQ degradation Limitations:\nRequires retraining or fine-tuning Slightly slower training due to added quantization simulation steps QAT is particularly effective for compact architectures like MobileNet or for models deployed on edge devices where low-precision inference is essential and even small drops in accuracy can be problematic. (like MobileNet) or models deployed in latency-sensitive, low-precision environments (e.g., mobile or embedded devices).\nCode Walkthrough In this section I will provide a complete example of applying both Post Training Quantization (PTQ) and Quantization Aware Training (QAT) to a ResNet18 model adjusted for CIFAR-10 dataset. The code was tested to work on PyTorch 2.4 through 2.8 (nightly build) using both X86 Quantizer for CPU deployments and XNNPACK Quantizer used for mobile and edge devices. You can find the full self-contained jupyter notebooks below, or in the Neural Network Optimization GitHub repository.\nQuantization - PTQ using PyTorch 2 Export Quantization and X86 Backend Quantization - QAT using PyTorch 2 Export Quantization and X86 Backend Quantization - PTQ using PyTorch 2 Export Quantization and XNNPACK Quantizer Quantization - QAT using PyTorch 2 Export Quantization and XNNPACK Quantizer Below I will go over the code for PTQ and QAT for the X86 scenario, as the edge device case is very similar.\nShared code We start with defining some code to get the CIFAR-10 dataset, adjust the ResNet18 model, and define training and evaluation functions to measure the model\u0026rsquo;s size, accuracy, and latency. We end this section with the training and evaluation of the baseline model, before quantization.\nBasic Setup import os import time import warnings from packaging import version import numpy as np import torch import torch.nn as nn from torchvision import datasets, transforms, models from torch.quantization import quantize_dynamic from torch.ao.quantization import get_default_qconfig, QConfigMapping from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx from torch.utils.data import DataLoader, Subset # ignores irrelevant warning, see: https://github.com/pytorch/pytorch/issues/149829 warnings.filterwarnings(\u0026#34;ignore\u0026#34;, message=\u0026#34;.*TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support.*\u0026#34;) # ignores irrelevant warning, see: https://github.com/tensorflow/tensorflow/issues/77293 warnings.filterwarnings(\u0026#34;ignore\u0026#34;, message=\u0026#34;.*erase_node(.*) on an already erased node.*\u0026#34;) print(f\u0026#34;PyTorch Version: {torch.__version__}\u0026#34;) device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) print(f\u0026#34;Device used: {device.type}\u0026#34;) skip_cpu = False # change to True to skip the slow checks on CPU print(f\u0026#34;Should skip CPU evaluations: {skip_cpu}\u0026#34;) Get CIFAR-10 train and test sets transform = transforms.Compose([ transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ]) train_dataset = datasets.CIFAR10(root=\u0026#34;./data\u0026#34;, train=True, download=True, transform=transform) train_loader = DataLoader( datasets.CIFAR10(root=\u0026#34;./data\u0026#34;, train=True, download=True, transform=transform), batch_size=128, shuffle=True ) test_dataset = datasets.CIFAR10(root=\u0026#34;./data\u0026#34;, train=False, download=True, transform=transform) test_loader = DataLoader( datasets.CIFAR10(root=\u0026#34;./data\u0026#34;, train=False, download=True, transform=transform), batch_size=128, shuffle=False, num_workers=2, drop_last=True, ) calibration_dataset = Subset(train_dataset, range(256)) calibration_loader = DataLoader(calibration_dataset, batch_size=128, shuffle=False) Adjust ResNet18 network for CIFAR-10 dataset def get_resnet18_for_cifar10(): \u0026#34;\u0026#34;\u0026#34; Returns a ResNet-18 model adjusted for CIFAR-10: - 3x3 conv with stride 1 - No max pooling - 10 output classes \u0026#34;\u0026#34;\u0026#34; model = models.resnet18(weights=None, num_classes=10) model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False) model.maxpool = nn.Identity() return model.to(device) model_to_quantize = get_resnet18_for_cifar10() Define Train and Evaluate functions def train(model, loader, epochs, lr=0.01, save_path=\u0026#34;model.pth\u0026#34;, silent=False): \u0026#34;\u0026#34;\u0026#34; Trains a model with SGD and cross-entropy loss. Loads from save_path if it exists. \u0026#34;\u0026#34;\u0026#34; try: model.train() except NotImplementedError: torch.ao.quantization.move_exported_model_to_train(model) if os.path.exists(save_path): if not silent: print(f\u0026#34;Model already trained. Loading from {save_path}\u0026#34;) model.load_state_dict(torch.load(save_path)) return # no saved model found. training from given model state criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9) for epoch in range(epochs): for x, y in loader: x, y = x.to(device), y.to(device) optimizer.zero_grad() loss = criterion(model(x), y) loss.backward() optimizer.step() if not silent: print(f\u0026#34;Epoch {epoch+1}: loss={loss.item():.4f}\u0026#34;) evaluate(model, f\u0026#34;Epoch {epoch+1}\u0026#34;) try: model.train() except NotImplementedError: torch.ao.quantization.move_exported_model_to_train(model) if save_path: torch.save(model.state_dict(), save_path) if not silent: print(f\u0026#34;Training complete. Model saved to {save_path}\u0026#34;) def evaluate(model, tag): \u0026#34;\u0026#34;\u0026#34; Evaluates the model on test_loader and prints accuracy. \u0026#34;\u0026#34;\u0026#34; try: model.eval() except NotImplementedError: model = torch.ao.quantization.move_exported_model_to_eval(model) model.to(device) correct = total = 0 with torch.no_grad(): for x, y in test_loader: x, y = x.to(device), y.to(device) preds = model(x).argmax(1) correct += (preds == y).sum().item() total += y.size(0) accuracy = correct / total print(f\u0026#34;Accuracy ({tag}): {accuracy*100:.2f}%\u0026#34;) Define helper functions to measure latency class Timer: \u0026#34;\u0026#34;\u0026#34; A simple timer utility for measuring elapsed time in milliseconds. Supports both GPU and CPU timing: - If CUDA is available, uses torch.cuda.Event for accurate GPU timing. - Otherwise, falls back to wall-clock CPU timing via time.time(). Methods: start(): Start the timer. stop(): Stop the timer and return the elapsed time in milliseconds. \u0026#34;\u0026#34;\u0026#34; def __init__(self): self.use_cuda = torch.cuda.is_available() if self.use_cuda: self.starter = torch.cuda.Event(enable_timing=True) self.ender = torch.cuda.Event(enable_timing=True) def start(self): if self.use_cuda: self.starter.record() else: self.start_time = time.time() def stop(self): if self.use_cuda: self.ender.record() torch.cuda.synchronize() return self.starter.elapsed_time(self.ender) # ms else: return (time.time() - self.start_time) * 1000 # ms def estimate_latency(model, example_inputs, repetitions=50): \u0026#34;\u0026#34;\u0026#34; Returns avg and std inference latency (ms) over given runs. \u0026#34;\u0026#34;\u0026#34; timer = Timer() timings = np.zeros((repetitions, 1)) # warm-up for _ in range(5): _ = model(example_inputs) with torch.no_grad(): for rep in range(repetitions): timer.start() _ = model(example_inputs) elapsed = timer.stop() timings[rep] = elapsed return np.mean(timings), np.std(timings) def estimate_latency_full(model, tag, skip_cpu): \u0026#34;\u0026#34;\u0026#34; Prints model latency on GPU and (optionally) CPU. \u0026#34;\u0026#34;\u0026#34; # estimate latency on CPU if not skip_cpu: example_input = torch.rand(128, 3, 32, 32).cpu() model.cpu() latency_mu, latency_std = estimate_latency(model, example_input) print(f\u0026#34;Latency ({tag}, on CPU): {latency_mu:.2f} ± {latency_std:.2f} ms\u0026#34;) # estimate latency on GPU example_input = torch.rand(128, 3, 32, 32).cuda() model.cuda() latency_mu, latency_std = estimate_latency(model, example_input) print(f\u0026#34;Latency ({tag}, on GPU): {latency_mu:.2f} ± {latency_std:.2f} ms\u0026#34;) def print_size_of_model(model, tag=\u0026#34;\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Prints model size (MB). \u0026#34;\u0026#34;\u0026#34; torch.save(model.state_dict(), \u0026#34;temp.p\u0026#34;) size_mb_full = os.path.getsize(\u0026#34;temp.p\u0026#34;) / 1e6 print(f\u0026#34;Size ({tag}): {size_mb_full:.2f} MB\u0026#34;) os.remove(\u0026#34;temp.p\u0026#34;) Train full model train(model_to_quantize, train_loader, epochs=15, save_path=\u0026#34;full_model.pth\u0026#34;) Evaluate full model # get full model size print_size_of_model(model_to_quantize, \u0026#34;full\u0026#34;) # evaluate full accuracy accuracy_full = evaluate(model_to_quantize, \u0026#39;full\u0026#39;) # estimate full model latency estimate_latency_full(model_to_quantize, \u0026#39;full\u0026#39;, skip_cpu) Results:\nSize (full): 44.77 MB Accuracy (full): 80.53% Latency (full, on CPU): 804.16 ± 57.55 ms Latency (full, on GPU): 16.39 ± 0.30 ms Post Training Quantization (PTQ) The basic flow is as follow:\nExport the model to to a stable, backend-agnostic format that\u0026rsquo;s suitable for transformations, optimizations, and deployment. Define the quantizer that will prepare the model for quantization. Here I used the X86 for CPU deployments, but there is a simple variant that works better for mobile and edge devices working on ARM CPUs. Preparing the model for quantization. For example, folding batch-norm into preceding conv2d operators, and inserting observers in appropriate places to collect activation statistics needed for calibration. Running inference on calibration data to collect activation statistics Converts calibrated model to a quantized model. While the quantized model already takes less space, it is not yet optimized for the final deployment. from torch.ao.quantization.quantize_pt2e import ( prepare_pt2e, convert_pt2e, ) import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq from torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer # batch of 128 images, each with 3 color channels and 32x32 resolution (CIFAR-10) example_inputs = (torch.rand(128, 3, 32, 32).to(device),) # export the model to a standardized format before quantization if version.parse(torch.__version__) \u0026gt;= version.parse(\u0026#34;2.5\u0026#34;): # for pytorch 2.5+ exported_model = torch.export.export_for_training(model_to_quantize, example_inputs).module() else: # for pytorch 2.4 from torch._export import capture_pre_autograd_graph exported_model = capture_pre_autograd_graph(model_to_quantize, example_inputs) # quantization setup for X86 Inductor Quantizer quantizer = X86InductorQuantizer() quantizer.set_global(xiq.get_default_x86_inductor_quantization_config()) # preparing for PTQ by folding batch-norm into preceding conv2d operators, and inserting observers in appropriate places prepared_model = prepare_pt2e(exported_model, quantizer) # run inference on calibration data to collect activation stats needed for activation quantization def calibrate(model, data_loader): torch.ao.quantization.move_exported_model_to_eval(model) with torch.no_grad(): for image, target in data_loader: model(image.to(device)) calibrate(prepared_model, calibration_loader) # converts calibrated model to a quantized model quantized_model = convert_pt2e(prepared_model) # export again to remove unused weights after quantization if version.parse(torch.__version__) \u0026gt;= version.parse(\u0026#34;2.5\u0026#34;): # for pytorch 2.5+ quantized_model = torch.export.export_for_training(quantized_model, example_inputs).module() else: # for pytorch 2.4 quantized_model = capture_pre_autograd_graph(quantized_model, example_inputs) Evaluate quantized model # get quantized model size print_size_of_model(quantized_model, \u0026#34;quantized\u0026#34;) # evaluate quantized accuracy accuracy_full = evaluate(quantized_model, \u0026#39;quantized\u0026#39;) # estimate quantized model latency estimate_latency_full(quantized_model, \u0026#39;quantized\u0026#39;, skip_cpu) Results:\nSize (quantized): 11.26 MB Accuracy (quantized): 80.45% Latency (quantized, on CPU): 1982.11 ± 229.35 ms Latency (quantized, on GPU): 37.15 ± 0.08 ms Notice the space dropped by 75%, but CPU and GPU latency more than doubled. This is because the model while quantized is not optimized yet to run on the specific device. This will happen in the next section.\nOptimize quantized model for inference Here we do the final optimization to squeeze the performance. This uses C++ wrapper which reduces the Python overhead\n# enable the use of the C++ wrapper for TorchInductor which reduces Python overhead import torch._inductor.config as config config.cpp_wrapper = True # compiles quantized model to generate optimized model with torch.no_grad(): optimized_model = torch.compile(quantized_model) Evaluate optimized model # get optimized model size print_size_of_model(optimized_model, \u0026#34;optimized\u0026#34;) # evaluate optimized accuracy accuracy_full = evaluate(optimized_model, \u0026#39;optimized\u0026#39;) # estimate optimized model latency estimate_latency_full(optimized_model, \u0026#39;optimized\u0026#39;, skip_cpu) Results:\nSize (optimized): 11.26 MB Accuracy (optimized): 79.53% Latency (optimized, on CPU): 782.53 ± 51.36 ms Latency (optimized, on GPU): 13.80 ± 0.28 ms Notably, it achieves a 75% reduction in space, reduces GPU latency by 16% and 3% on CPU, with only a 1% drop in accuracy.\nQuantization Aware Training (QAT) In QAT the basic flow is very similiar to PTQ, the main difference is the replacement of the calibration step that collects activation statistics with a much longer fine-tuning step which fine-tunes the model considering the quantization constraints. The collection of activation statistics also happens, as part of the fine-tuning process.\nfrom torch.ao.quantization.quantize_pt2e import ( prepare_qat_pt2e, convert_pt2e, ) import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq from torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer # batch of 128 images, each with 3 color channels and 32x32 resolution (CIFAR-10) example_inputs = (torch.rand(128, 3, 32, 32).to(device),) # export the model to a standardized format before quantization if version.parse(torch.__version__) \u0026gt;= version.parse(\u0026#34;2.5\u0026#34;): # for pytorch 2.5+ exported_model = torch.export.export_for_training(model_to_quantize, example_inputs).module() else: # for pytorch 2.4 from torch._export import capture_pre_autograd_graph exported_model = capture_pre_autograd_graph(model_to_quantize, example_inputs) # quantization setup for X86 Inductor Quantizer quantizer = X86InductorQuantizer() quantizer.set_global(xiq.get_default_x86_inductor_quantization_config()) # inserts fake quantizes in appropriate places in the model and performs the fusions, like conv2d + batch-norm prepared_model = prepare_qat_pt2e(exported_model, quantizer) # fine-tune with quantization constraints train(prepared_model, train_loader, epochs=3, save_path=\u0026#34;qat_model_x86.pth\u0026#34;) # converts calibrated model to a quantized model quantized_model = convert_pt2e(prepared_model) # export again to remove unused weights after quantization if version.parse(torch.__version__) \u0026gt;= version.parse(\u0026#34;2.5\u0026#34;): # for pytorch 2.5+ quantized_model = torch.export.export_for_training(quantized_model, example_inputs).module() else: # for pytorch 2.4 quantized_model = capture_pre_autograd_graph(quantized_model, example_inputs) Evaluate quantized model # get quantized model size print_size_of_model(quantized_model, \u0026#34;quantized\u0026#34;) # evaluate quantized accuracy accuracy_full = evaluate(quantized_model, \u0026#39;quantized\u0026#39;) # estimate quantized model latency estimate_latency_full(quantized_model, \u0026#39;quantized\u0026#39;, skip_cpu) Results:\nSize (quantized): 11.26 MB Accuracy (quantized): 80.57% Latency (quantized, on CPU): 1617.82 ± 158.67 ms Latency (quantized, on GPU): 33.62 ± 0.16 ms Optimize quantized model for inference # enable the use of the C++ wrapper for TorchInductor which reduces Python overhead import torch._inductor.config as config config.cpp_wrapper = True # compiles quantized model to generate optimized model with torch.no_grad(): optimized_model = torch.compile(quantized_model) Evaluate optimized model # get optimized model size print_size_of_model(optimized_model, \u0026#34;optimized\u0026#34;) # evaluate optimized accuracy accuracy_full = evaluate(optimized_model, \u0026#39;optimized\u0026#39;) # estimate optimized model latency estimate_latency_full(optimized_model, \u0026#39;optimized\u0026#39;, skip_cpu) Results:\nSize (optimized): 11.26 MB Accuracy (optimized): 79.54% Latency (optimized, on CPU): 831.76 ± 39.63 ms Latency (optimized, on GPU): 13.71 ± 0.24 ms While in this small-scale model the results of QAT are very similar to PTQ, it is suggested that for larger models QAT has an opportunity to provide higher accuracy than the PTQ variant.\nComparison of PTQ and QAT Results Below is a summary table comparing the baseline model with the Post Training Quantization (PTQ) and Quantization Aware Training (QAT) results based on our CIFAR-10 ResNet18 experiments:\nMethod Model Size Accuracy GPU Latency (ms) CPU Latency (ms) Baseline (no quantization) 44.77 MB 80.53% 16.39 ± 0.30 ms 804.16 ± 57.55 ms Post Training Quantization 11.26 MB 79.53% 13.80 ± 0.28 ms 782.53 ± 51.36 ms Quantization Aware Training 11.26 MB 79.54% 13.71 ± 0.24 ms 831.76 ± 39.63 ms Summary Quantization is a powerful technique for compressing and accelerating deep learning models by lowering numerical precision. PyTorch provides flexible APIs for applying both Post Training Quantization (PTQ) and Quantization Aware Training (QAT).\nUse PTQ when simplicity and speed are key, and you can tolerate some loss in accuracy. Use QAT when you need the best possible performance from quantized models, especially for smaller or sensitive models. With good calibration and training strategies, quantization can reduce model size and inference time significantly with minimal impact on performance.\nReferences For additional details, the following sources were helpful when preparing this post.\nPyTorch Documentation: Quantization PyTorch Documentation: PyTorch 2 Export Post Training Quantization PyTorch Documentation: PyTorch 2 Export Quantization-Aware Training (QAT) PyTorch Documentation: PyTorch 2 Export Quantization with X86 Backend through Inductor PyTorch Dev Discussions: TorchInductor Update 6: CPU backend performance update and new features in PyTorch 2.1 ","permalink":"https://arikpoz.github.io/posts/2025-04-16-neural-network-quantization-in-pytorch/","summary":"\u003cp\u003e\u003cimg alt=\"image here\u0026quot;\" loading=\"lazy\" src=\"/posts/2025-04-16-neural-network-quantization-in-pytorch/lead-image.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis tutorial provides an introduction to quantization in PyTorch, covering both theory and practice. We’ll explore the different types of quantization, and apply both post training quantization (PTQ) and quantization aware training (QAT) on a simple example using CIFAR-10 and ResNet18. In the presented example we achieve a \u003cstrong\u003e75% reduction in space\u003c/strong\u003e and \u003cstrong\u003e16% reduction in GPU latency\u003c/strong\u003e with only 1% drop in accuracy.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"what-is-quantization\"\u003eWhat is Quantization?\u003c/h2\u003e\n\u003cp\u003eQuantization is a model optimization technique that reduces the numerical precision used to represent weights and activations in deep learning models. Its primary benefits include:\u003c/p\u003e","title":"Neural Network Quantization in PyTorch"},{"content":"\nIntroduction In this post, I will demonstrate how to use pruning to significantly reduce a model\u0026rsquo;s size and latency while maintaining minimal accuracy loss. In the example, we achieve a 90% reduction in model size and 5.5x faster inference time, all while preserving the same level of accuracy.\nWe will begin with a brief explanation of what pruning is and why it is important. Then, I’ll provide a hands-on demonstration of applying pruning to a PyTorch model.\nOverview of Pruning Neural network pruning involves removing less important weights, channels, or neurons from a neural network to make it smaller and faster. The goal is to reduce computational costs (such as latency and memory usage) without significantly affecting model accuracy.\nDeep neural networks often contain a lot of redundancy. This redundancy arises because models are typically overparameterized to ensure high accuracy and generalization. During training, many parameters become co-dependent or have little impact on the final output. For example, multiple neurons may learn similar features, or certain filters may remain underutilized. This redundancy makes models robust but also bloated. Pruning helps streamline these models by eliminating parts that contribute the least to the output, resulting in a more efficient network that is easier to deploy on edge devices or in latency-sensitive applications.\nThere are two main types of pruning:\nUnstructured Pruning: Removes individual weights regardless of their position. While it can achieve high sparsity, it often requires specialized hardware or libraries to fully utilize the sparsity. Zeroing out individual weights typically does not improve latency because standard deep learning libraries use dense matrix multiplication regardless of how many weights are zeroed out. To benefit from sparsity, the model must be converted into a sparse format, which is often not well-supported by commodity hardware. In fact, these sparse representations can sometimes be slower than dense operations due to less optimized memory access patterns and lack of hardware acceleration. As a result, unstructured pruning offers theoretical compression but not always practical speedups unless carefully integrated into the deployment pipeline.\nStructured Pruning: Removes entire filters, channels, or layers, leading to real improvements in inference speed on standard hardware. Unlike unstructured pruning, which retains the original dense structure and thus doesn’t alter compute patterns, structured pruning directly reduces the dimensionality of tensors and layers. This means fewer floating-point operations (FLOPs) and less memory access, as the actual matrices involved in convolutions and linear operations are physically smaller. As a result, inference is faster and more efficient on standard hardware using optimized dense kernels, with no need for specialized sparse computation support.\nPyTorch Pruning API PyTorch provides a built-in pruning utility under torch.nn.utils.prune. This API supports both unstructured pruning (zeroing individual weights by magnitude or custom metrics) and structured pruning (removing entire channels or neurons). The PyTorch pruning tutorial offers a solid introduction using iterative magnitude pruning. However, it is important to note that the PyTorch pruning API does not result in real inference speedups out-of-the-box. This is because it primarily focuses on zeroing out weights rather than removing them. For unstructured pruning, it does not convert the model to a sparse representation, which is necessary to leverage computational gains. For structured pruning, it does not automatically modify the architecture to remove entire channels or filters, which means the computational graph remains unchanged.\nThat said, the PyTorch pruning API is a flexible and useful tool for experimenting with pruning strategies. It provides a simple interface to apply custom pruning criteria, evaluate sparsity effects, and implement iterative pruning and retraining loops. It is especially helpful for research and prototyping where exact hardware efficiency is less critical than functional model behavior.\nWhy Use Torch-Pruning? Structured pruning isn’t trivial. Removing a channel in one layer often requires modifying downstream layers. Structured pruning often involves complex inter-layer dependencies. For example, if you prune an output channel from a convolutional layer, any layer that consumes its output, such as a batch normalization layer or subsequent convolution, must also be updated to match the new shape. Managing these changes across many layers can be error-prone and tedious when done manually. Torch-Pruning solves this by introducing a graph-based algorithm called DepGraph, which automatically analyzes the model\u0026rsquo;s computation graph, identifies dependencies, and organizes pruning into safe and consistent execution plans.\nPractical Usage Example: Pruning ResNet-18 in PyTorch Let’s walk through pruning a ResNet-18 model step-by-step using torch-pruning. We\u0026rsquo;ll do this in Google Colab, so you can follow along easily. This example is adapted from the official README of Torch-Pruning.\nRun this code in Google Colab to try it yourself.\nSetup First, install the required library:\n!pip install torch-pruning Then, define the required imports:\nimport os import time import copy import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from torchvision import datasets, transforms, models from torch.utils.data import DataLoader import torch_pruning as tp device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) print(f\u0026#34;{device=}\u0026#34;) Get CIFAR-10 Train and Test Sets transform = transforms.Compose([ transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ]) train_loader = DataLoader( datasets.CIFAR10(root=\u0026#34;./data\u0026#34;, train=True, download=True, transform=transform), batch_size=128, shuffle=True ) test_loader = DataLoader( datasets.CIFAR10(root=\u0026#34;./data\u0026#34;, train=False, download=True, transform=transform), batch_size=256 ) Adjust ResNet-18 Network for CIFAR-10 Dataset def get_resnet18_for_cifar10(): model = models.resnet18(weights=None, num_classes=10) model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False) model.maxpool = nn.Identity() return model.to(device) full_model = get_resnet18_for_cifar10() Define Train and Evaluate Functions def train(model, loader, epochs, lr=0.01, save_path=\u0026#34;model.pth\u0026#34;, silent=False): if os.path.exists(save_path): if not silent: print(f\u0026#34;Model already trained. Loading from {save_path}\u0026#34;) model.load_state_dict(torch.load(save_path)) return criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9) model.train() for epoch in range(epochs): for x, y in loader: x, y = x.to(device), y.to(device) optimizer.zero_grad() loss = criterion(model(x), y) loss.backward() optimizer.step() if not silent: print(f\u0026#34;Epoch {epoch+1}: loss={loss.item():.4f}\u0026#34;) torch.save(model.state_dict(), save_path) if not silent: print(f\u0026#34;Training complete. Model saved to {save_path}\u0026#34;) def evaluate(model): model.eval() correct = total = 0 with torch.no_grad(): for x, y in test_loader: x, y = x.to(device), y.to(device) preds = model(x).argmax(1) correct += (preds == y).sum().item() total += y.size(0) return correct / total Define Helper Functions to Measure Latency class Timer: def __init__(self): self.use_cuda = torch.cuda.is_available() if self.use_cuda: self.starter = torch.cuda.Event(enable_timing=True) self.ender = torch.cuda.Event(enable_timing=True) def start(self): if self.use_cuda: self.starter.record() else: self.start_time = time.time() def stop(self): if self.use_cuda: self.ender.record() torch.cuda.synchronize() return self.starter.elapsed_time(self.ender) # ms else: return (time.time() - self.start_time) * 1000 # ms def estimate_latency(model, example_inputs, repetitions=50): timer = Timer() timings = np.zeros((repetitions, 1)) # Warm-up for _ in range(5): _ = model(example_inputs) with torch.no_grad(): for rep in range(repetitions): timer.start() _ = model(example_inputs) elapsed = timer.stop() timings[rep] = elapsed return np.mean(timings), np.std(timings) Train and Evaluate the Full Model train(full_model, train_loader, epochs=10, save_path=\u0026#34;full_model.pth\u0026#34;) accuracy_full = evaluate(full_model) example_input = torch.rand(128, 3, 32, 32).to(device) macs, parameters = tp.utils.count_ops_and_params(full_model, example_input) latency_mu, latency_std = estimate_latency(full_model, example_input) print(f\u0026#34;[full model] \\t\\tMACs: {macs/1e9:.2f} G, \\tParameters: {parameters/1e6:.2f} M, \\tLatency: {latency_mu:.2f} ± {latency_std:.2f} ms \\tAccuracy: {accuracy_full*100:.2f}%\u0026#34;) To save you some time, here are the results for the fully trained model:\n[full model] MACs: 0.56 G, Parameters: 11.17 M, Latency: 16.52 ± 0.03 ms Accuracy: 76.85% Prune by L2 Magnitude # Clone full model before pruning pruned_model = copy.deepcopy(full_model) pruned_model = pruned_model.to(device) # Set which layers to skip pruning. Important to keep the final classifier layer ignored_layers = [] for m in pruned_model.modules(): if isinstance(m, torch.nn.Linear) and m.out_features == 10: ignored_layers.append(m) # Iterative pruning iterative_steps = 20 pruner = tp.pruner.MagnitudePruner( model=pruned_model, example_inputs=example_input, importance=tp.importance.MagnitudeImportance(p=2), pruning_ratio=1, iterative_steps=iterative_steps, ignored_layers=ignored_layers, round_to=2, ) for iter in range(iterative_steps): # Prune pruner.step() # Evaluate after pruning acc_before = evaluate(pruned_model) # Fine-tune pruned model train(pruned_model, train_loader, epochs=1, save_path=f\u0026#34;pruned_model_{iter}.pth\u0026#34;, silent=True) # Evaluate after fine-tuning acc_after = evaluate(pruned_model) # Count MACs and parameters macs, parameters = tp.utils.count_ops_and_params(pruned_model, example_input) latency_mu, latency_std = estimate_latency(pruned_model, example_input) current_pruning_ratio = 1 / iterative_steps * (iter + 1) print(f\u0026#34;[pruned model] \\tPruning ratio: {current_pruning_ratio:.2f}, \\tMACs: {macs/1e9:.2f} G, \\tParameters: {parameters/1e6:.2f} M, \\tLatency: {latency_mu:.2f} ± {latency_std:.2f} ms \\tAccuracy pruned: {acc_before*100:.2f}%\\tFinetuned: {acc_after*100:.2f}%\u0026#34;) The pruning results show the model\u0026rsquo;s accuracy immediately after pruning and again after fine-tuning the smaller, pruned model. While accuracy initially drops following pruning, it recovers significantly after just one epoch of fine-tuning.\n[pruned model] Pruning ratio: 0.05, MACs: 0.49 G, Parameters: 10.03 M, Latency: 17.64 ± 0.04 ms Accuracy pruned: 63.60%\tFinetuned: 72.17% [pruned model] Pruning ratio: 0.10, MACs: 0.44 G, Parameters: 9.00 M, Latency: 16.12 ± 0.04 ms Accuracy pruned: 44.51%\tFinetuned: 76.51% [pruned model] Pruning ratio: 0.15, MACs: 0.40 G, Parameters: 8.01 M, Latency: 16.40 ± 0.04 ms Accuracy pruned: 66.98%\tFinetuned: 75.18% [pruned model] Pruning ratio: 0.20, MACs: 0.35 G, Parameters: 7.09 M, Latency: 16.33 ± 0.04 ms Accuracy pruned: 51.83%\tFinetuned: 74.64% [pruned model] Pruning ratio: 0.25, MACs: 0.31 G, Parameters: 6.29 M, Latency: 14.40 ± 0.05 ms Accuracy pruned: 63.51%\tFinetuned: 76.73% [pruned model] Pruning ratio: 0.30, MACs: 0.27 G, Parameters: 5.44 M, Latency: 14.07 ± 0.03 ms Accuracy pruned: 49.36%\tFinetuned: 74.64% [pruned model] Pruning ratio: 0.35, MACs: 0.23 G, Parameters: 4.69 M, Latency: 12.27 ± 0.03 ms Accuracy pruned: 58.74%\tFinetuned: 77.56% [pruned model] Pruning ratio: 0.40, MACs: 0.20 G, Parameters: 3.98 M, Latency: 12.28 ± 0.03 ms Accuracy pruned: 63.98%\tFinetuned: 78.29% [pruned model] Pruning ratio: 0.45, MACs: 0.16 G, Parameters: 3.34 M, Latency: 11.41 ± 0.02 ms Accuracy pruned: 45.66%\tFinetuned: 78.58% [pruned model] Pruning ratio: 0.50, MACs: 0.14 G, Parameters: 2.80 M, Latency: 7.06 ± 0.03 ms Accuracy pruned: 49.91%\tFinetuned: 72.77% [pruned model] Pruning ratio: 0.55, MACs: 0.11 G, Parameters: 2.24 M, Latency: 6.82 ± 0.05 ms Accuracy pruned: 38.72%\tFinetuned: 76.13% [pruned model] Pruning ratio: 0.60, MACs: 0.09 G, Parameters: 1.77 M, Latency: 5.96 ± 0.05 ms Accuracy pruned: 42.84%\tFinetuned: 79.09% [pruned model] Pruning ratio: 0.65, MACs: 0.07 G, Parameters: 1.34 M, Latency: 4.88 ± 0.09 ms Accuracy pruned: 33.88%\tFinetuned: 75.54% [pruned model] Pruning ratio: 0.70, MACs: 0.05 G, Parameters: 0.99 M, Latency: 4.17 ± 0.01 ms Accuracy pruned: 22.50%\tFinetuned: 75.60% [pruned model] Pruning ratio: 0.75, MACs: 0.04 G, Parameters: 0.70 M, Latency: 2.96 ± 0.08 ms Accuracy pruned: 34.23%\tFinetuned: 78.91% [pruned model] Pruning ratio: 0.80, MACs: 0.02 G, Parameters: 0.44 M, Latency: 2.70 ± 0.02 ms Accuracy pruned: 15.91%\tFinetuned: 75.55% [pruned model] Pruning ratio: 0.85, MACs: 0.01 G, Parameters: 0.25 M, Latency: 2.69 ± 0.04 ms Accuracy pruned: 14.16%\tFinetuned: 75.01% [pruned model] Pruning ratio: 0.90, MACs: 0.01 G, Parameters: 0.11 M, Latency: 2.63 ± 0.01 ms Accuracy pruned: 10.00%\tFinetuned: 68.87% [pruned model] Pruning ratio: 0.95, MACs: 0.00 G, Parameters: 0.03 M, Latency: 2.59 ± 0.02 ms Accuracy pruned: 10.00%\tFinetuned: 53.36% [pruned model] Pruning ratio: 1.00, MACs: 0.00 G, Parameters: 0.03 M, Latency: 2.57 ± 0.01 ms Accuracy pruned: 53.36%\tFinetuned: 54.91% Note that one of the final models achives same accuracy (even higher, 78.91%) while having 15x less parameters (0.7M vs. 11.17M), and is 5.5x faster than original (2.96 ms vs. 16.52 ms).\nSummary Pruning is a powerful technique to make deep networks lighter and faster. In this blog post, we:\nExplored what pruning is and why it matters Compared the native PyTorch pruning API with Torch-Pruning Used torch-pruning to prune a ResNet-18 model in PyTorch Evaluated model size, inference latency, and top-1 prediction accuracy using CIFAR-10 data By applying structured pruning, you can make your models more efficient with minimal impact on performance, a valuable step in any model optimization workflow.\n","permalink":"https://arikpoz.github.io/posts/2025-04-10-neural-network-pruning-how-to-accelerate-inference-with-minimal-accuracy-loss/","summary":"\u003cp\u003e\u003cimg alt=\"Cartoon of a person in an \u0026ldquo;NN Pruning\u0026rdquo; shirt trimming a large robot labeled \u0026ldquo;Neural Network\u0026rdquo; into a slim, fast robot labeled \u0026ldquo;Pruned Network.\u0026rdquo;\" loading=\"lazy\" src=\"/posts/2025-04-10-neural-network-pruning-how-to-accelerate-inference-with-minimal-accuracy-loss/lead-image.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this post, I will demonstrate how to use pruning to \u003cstrong\u003esignificantly reduce a model\u0026rsquo;s size and latency\u003c/strong\u003e while maintaining minimal accuracy loss. In the example, we achieve a \u003cstrong\u003e90% reduction in model size\u003c/strong\u003e and \u003cstrong\u003e5.5x faster inference time\u003c/strong\u003e, all while preserving the same level of accuracy.\u003c/p\u003e","title":"Neural Network Pruning: How to Accelerate Inference with Minimal Accuracy Loss"},{"content":"\nIntroduction In deep learning pipelines, especially those involving image data, data loading and preprocessing often become major bottlenecks. Traditionally, image decoding is performed using libraries like OpenCV or Pillow, which rely on CPU-based processing. After decoding, the data must be transferred to GPU memory for further operations. But what if the decoding process itself could be performed directly on the GPU? Could this lead to faster performance?\nIn this post, I demonstrate how to use nvImageCodec to achieve a 2.18x speedup in JPEG loading by decoding directly on the GPU. Learn more about nvImageCodec in its documentation or on GitHub.\n🔍 What is nvImageCodec? nvImageCodec is a high-performance image codec optimized for GPU acceleration. It is designed for scenarios like model training and batch inference, where decoding thousands of images quickly is critical. The library supports decoding (bytes to pixels) and encoding (pixels to bytes) for various common image formats. However, not all formats are fully supported on the GPU. Some, like PNG and WebP, fall back to CPU-based decoding. Below is a summary of supported formats:\n✅ Format Support: Format GPU Decode GPU Encode Notes JPEG ✅ Yes ✅ Yes Fastest, hardware-accelerated JPEG 2000 ✅ Yes ✅ Yes TIFF ✅ Yes ❌ No (planned) CUDA decoder PNG ❌ No (planned) ❌ No (planned) CPU only WebP ❌ No ❌ No CPU only 🌟 What was Benchmarked? We compared the performance of:\nOpenCV: CPU-based decoding followed by PIL transformations. nvImageCodec: GPU-based decoding with tensor transformations. Benchmark Details: Dataset: 1000 JPEG images from the ImageNet Sample Images dataset (credit: Eli Schwartz). Model: ResNet18 for inference. Transform Pipeline: Resize and crop applied to all images. Each benchmark was run 10 times (plus 1 warmup iteration), and the average times were recorded for:\n🧪 Loading: Decoding, resizing, and tensor conversion. ⚡ Inference: Model forward pass. ⏱️ Total: Combined loading and inference time. All benchmarks were conducted in Google Colab using a T4 GPU instance.\nRun this code in Google Colab to try it yourself.\n🛠️ Setup in Colab Install Dependencies and Load Dataset !pip install nvidia-nvimgcodec-cu11 opencv-python-headless !git clone https://github.com/EliSchwartz/imagenet-sample-images.git Prepare the Images import os, shutil from pathlib import Path source_dir = Path(\u0026#34;imagenet-sample-images\u0026#34;) dest_dir = Path(\u0026#34;benchmark_images\u0026#34;) dest_dir.mkdir(exist_ok=True) all_images = list(source_dir.glob(\u0026#34;*.JPEG\u0026#34;)) for img in all_images: shutil.copy(img, dest_dir / img.name) image_paths = sorted(list(dest_dir.glob(\u0026#34;*.JPEG\u0026#34;))) print(f\u0026#34;Prepared {len(image_paths)} images.\u0026#34;) Define Model and Preprocessing import torch import torchvision.transforms as transforms import torchvision.models as models from torchvision.transforms import Resize, CenterCrop device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = models.resnet18(pretrained=True).to(device).eval() transform = transforms.Compose([ Resize(256), CenterCrop(224), ]) 🧲 Benchmark Functions (10x Repeated Runs) OpenCV Benchmark def run_opencv_inference(image_paths, runs=10): import time, numpy as np from PIL import Image load_times, infer_times = [], [] for run_idx in range(runs + 1): imgs = [] t0 = time.time() for path in image_paths: img = cv2.imread(str(path)) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img = Image.fromarray(img) img = transform(img) img = transforms.ToTensor()(img) imgs.append(img) batch = torch.stack(imgs).to(device) load_time = time.time() - t0 t1 = time.time() with torch.no_grad(): model(batch) infer_time = time.time() - t1 if run_idx == 0: print(f\u0026#34;Run {run_idx + 1}: Warmup iteration (not included in mean). Loading Time = {load_time:.4f}s, Inference Time = {infer_time:.4f}s\u0026#34;) else: load_times.append(load_time) infer_times.append(infer_time) print(f\u0026#34;Run {run_idx + 1}: Loading Time = {load_time:.4f}s, Inference Time = {infer_time:.4f}s\u0026#34;) return np.mean(load_times), np.mean(infer_times) opencv_load, opencv_infer = run_opencv_inference(image_paths) nvImageCodec Benchmark def run_nvimagecodec_inference(image_paths, runs=10): import time, numpy as np decoder = nvimgcodec.Decoder(device_id=0) load_times, infer_times = [], [] for run_idx in range(runs + 1): imgs = [] t0 = time.time() for path in image_paths: with open(path, \u0026#39;rb\u0026#39;) as f: data = f.read() nv_img = decoder.decode(data) img = torch.as_tensor(nv_img.cuda()).permute(2, 0, 1).float().div(255) img = transform(img) imgs.append(img) batch = torch.stack(imgs).to(device) load_time = time.time() - t0 t1 = time.time() with torch.no_grad(): model(batch) infer_time = time.time() - t1 if run_idx == 0: print(f\u0026#34;Run {run_idx + 1}: Warmup iteration (not included in mean). Loading Time = {load_time:.4f}s, Inference Time = {infer_time:.4f}s\u0026#34;) else: load_times.append(load_time) infer_times.append(infer_time) print(f\u0026#34;Run {run_idx + 1}: Loading Time = {load_time:.4f}s, Inference Time = {infer_time:.4f}s\u0026#34;) return np.mean(load_times), np.mean(infer_times) nv_load, nv_infer = run_nvimagecodec_inference(image_paths) 📊 Results \u0026amp; Visualization import pandas as pd import matplotlib.pyplot as plt results = pd.DataFrame({ \u0026#34;Method\u0026#34;: [\u0026#34;OpenCV\u0026#34;, \u0026#34;nvImageCodec\u0026#34;], \u0026#34;Loading Time (s)\u0026#34;: [opencv_load, nv_load], \u0026#34;Inference Time (s)\u0026#34;: [opencv_infer, nv_infer], \u0026#34;Total Time (s)\u0026#34;: [ opencv_load + opencv_infer, nv_load + nv_infer ], }) print(results) results.plot(x=\u0026#34;Method\u0026#34;, y=[\u0026#34;Loading Time (s)\u0026#34;, \u0026#34;Inference Time (s)\u0026#34;, \u0026#34;Total Time (s)\u0026#34;], kind=\u0026#34;bar\u0026#34;, figsize=(10, 6)) plt.title(\u0026#34;OpenCV vs. nvImageCodec on 1000 ImageNet JPEGs (10-run average)\u0026#34;) plt.ylabel(\u0026#34;Seconds\u0026#34;) plt.grid(True) plt.show() ✅ Summary Method Loading Time (s) Inference Time (s) Total Time (s) OpenCV 6.08343 0.00349 6.08693 nvImageCodec 2.78262 0.00323 2.78585 By leveraging the T4 GPU, nvImageCodec achieves a 2.18x speedup in JPEG loading times by performing decoding directly on the GPU. This eliminates CPU bottlenecks and enables a more efficient data pipeline.\nFor workflows heavily reliant on JPEGs, integrating nvImageCodec into your training or inference pipeline can deliver substantial performance improvements with minimal effort.\nTip: Before integrating, ensure that loading time is indeed a bottleneck in your pipeline. For example, test by preloading a single image or skipping loading altogether to simulate random data. In training pipelines, prefetching images in parallel with GPU processing is also a common optimization strategy.\n","permalink":"https://arikpoz.github.io/posts/2025-04-07-fast-image-loading-with-nvidia-nvimagecodec/","summary":"\u003cp\u003e\u003cimg alt=\"OpenCV turtle loses the race with nvImageCodec rabbit\" loading=\"lazy\" src=\"/posts/2025-04-07-fast-image-loading-with-nvidia-nvimagecodec/lead-image.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eIn deep learning pipelines, especially those involving image data, \u003cstrong\u003edata loading and preprocessing\u003c/strong\u003e often become major bottlenecks. Traditionally, image decoding is performed using libraries like \u003ca href=\"https://docs.opencv.org/4.x/index.html\"\u003eOpenCV\u003c/a\u003e or \u003ca href=\"https://pillow.readthedocs.io/en/stable/\"\u003ePillow\u003c/a\u003e, which rely on CPU-based processing. After decoding, the data must be transferred to GPU memory for further operations. But what if the decoding process itself could be performed directly on the GPU? Could this lead to faster performance?\u003c/p\u003e","title":"Fast Image Loading with NVIDIA nvImageCodec"},{"content":"\nIntroduction Deep learning models have grown increasingly large and complex, enabling state-of-the-art performance in tasks such as image recognition, natural language processing, and generative AI. However, these large models often come with high computational costs, making them slow to run on edge devices, embedded systems, or even in cloud environments with strict latency requirements.\nModel compression techniques aim to reduce the size and computational requirements of neural networks while maintaining their accuracy. This enables faster inference, lower power consumption, and better deployment flexibility. In this post, we’ll explore why model compression is essential and provide an overview of four key techniques: pruning, quantization, knowledge distillation, and low-rank factorization.\nWhy Compress Neural Networks? Compression is not just about saving memory - it significantly improves inference speed and enables deployment on a wider range of hardware. Here are some key benefits:\nFaster Inference: Smaller models require fewer computations, reducing latency in real-time applications. Lower Memory Footprint: Compressed models take up less storage, making them ideal for mobile and edge devices. Reduced Power Consumption: Lower computation means lower energy usage, which is critical for battery-powered devices. Easier Deployment: Efficient models can be deployed on a broader range of hardware, including microcontrollers and IoT devices. Cost Savings: Running optimized models on lower-end hardware reduces infrastructure costs in cloud-based AI applications. Now, let\u0026rsquo;s explore the four primary methods of model compression.\n1. Pruning: Cutting Down Redundant Weights What is Pruning? Pruning removes unnecessary weights or neurons from a neural network, reducing its size without significantly impacting performance. The idea is that many parameters contribute little to the final output, and eliminating them can speed up computation.\nTypes of Pruning: Unstructured Pruning: Removes individual weights that have minimal impact on the network. Structured Pruning: Removes entire neurons, channels, or layers for a more hardware-friendly compression. Global vs. Layer-wise Pruning: Some methods prune across the entire model, while others prune within each layer independently. Use Cases and Benefits: Works well for over-parameterized models. Can be applied iteratively during training or post-training. Reduces memory usage and speeds up inference. 2. Quantization: Reducing Precision for Faster Computation What is Quantization? Quantization lowers the precision of a model’s weights and activations, reducing memory usage and enabling faster execution, particularly on specialized hardware like GPUs, TPUs, and mobile processors.\nTypes of Quantization: Post-Training Quantization: Converts a trained FP32 model to a lower precision (e.g., INT8) after training. Quantization-Aware Training: Trains the model with quantization effects simulated to minimize accuracy loss. Dynamic vs. Static Quantization: Determines whether quantization is applied per batch dynamically or precomputed for inference. Use Cases and Benefits: Significant speedup on hardware optimized for lower precision (TensorRT, OpenVINO, TFLite). Works well for inference-time optimization. Reduces model size while maintaining accuracy in many cases. 3. Knowledge Distillation: Training Small Models Using Large Models What is Knowledge Distillation? Knowledge distillation trains a smaller “student” model to mimic the behavior of a larger “teacher” model. Instead of learning directly from labeled data, the student learns from the teacher’s output distribution, capturing nuanced knowledge that direct training may miss.\nTypes of Knowledge Distillation: Logit-based Distillation: The student learns from the softened output probabilities of the teacher. Feature-based Distillation: The student mimics intermediate feature representations from the teacher. Self-Distillation: A single model is trained in stages, where later iterations learn from earlier iterations. Use Cases and Benefits: Enables smaller models to achieve near teacher-level accuracy. Useful for transferring knowledge from large pretrained models (e.g., BERT → DistilBERT). Can be used in conjunction with other compression techniques. 4. Low-Rank Factorization: Decomposing Weights for Efficiency What is Low-Rank Factorization? Low-rank factorization techniques decompose large weight matrices into smaller ones that approximate the original matrix, reducing computational cost without major accuracy loss.\nMethods of Factorization: Singular Value Decomposition (SVD): Breaks down weight matrices into simpler components. Tensor Decomposition: Extends matrix factorization to multi-dimensional tensors for convolutional layers. Factorized Convolutions: Reduces convolutional kernel complexity (e.g., depthwise separable convolutions in MobileNet). Use Cases and Benefits: Particularly useful for CNNs and Transformer models. Reduces FLOPs (floating point operations) in matrix multiplications. Can be combined with pruning and quantization for additional gains. Choosing the Right Compression Method Each compression technique has trade-offs, and the best choice depends on your target hardware and application:\nCompression Method Best For Key Benefit Potential Drawback Pruning Over-parameterized models Reduces model size May require fine-tuning Quantization Hardware acceleration Significant speedup Some accuracy loss possible Knowledge Distillation Efficient small models Retains knowledge from large models Requires a good teacher model Low-Rank Factorization CNNs, Transformers Reduces computation Approximation can impact accuracy Conclusion Model compression is a critical step in optimizing deep learning models for speed and efficiency. Each method: pruning, quantization, knowledge distillation, and low-rank factorization, offers unique advantages depending on the application.\n","permalink":"https://arikpoz.github.io/posts/2025-04-02-introduction-to-model-compression-why-and-how-to-shrink-neural-networks-for-speed/","summary":"\u003cp\u003e\u003cimg alt=\"Glowing digital brain shrinking in size, symbolizing faster, smaller neural networks\" loading=\"lazy\" src=\"/posts/2025-04-02-introduction-to-model-compression-why-and-how-to-shrink-neural-networks-for-speed/lead-image.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eDeep learning models have grown increasingly large and complex, enabling state-of-the-art performance in tasks such as image recognition, natural language processing, and generative AI. However, these large models often come with high computational costs, making them slow to run on edge devices, embedded systems, or even in cloud environments with strict latency requirements.\u003c/p\u003e\n\u003cp\u003eModel compression techniques aim to reduce the size and computational requirements of neural networks while maintaining their accuracy. This enables faster inference, lower power consumption, and better deployment flexibility. In this post, we’ll explore why model compression is essential and provide an overview of four key techniques: \u003cstrong\u003epruning, quantization, knowledge distillation, and low-rank factorization\u003c/strong\u003e.\u003c/p\u003e","title":"Introduction to Model Compression: Why and How to Shrink Neural Networks for Speed"},{"content":"\nIntroduction Neural networks are becoming increasingly powerful, but speed remains a crucial factor in real-world applications. Whether you\u0026rsquo;re running models on the cloud, edge devices, or personal hardware, optimizing them for speed can lead to faster inference, lower latency, and reduced resource consumption.\nIn this post, we\u0026rsquo;ll explore various techniques to accelerate neural networks, from model compression to hardware optimizations. This will serve as a foundation for future deep dives into each method.\n1. Model Compression: Shrinking the Network Without Losing Power One of the most effective ways to speed up a neural network is by reducing its size while maintaining performance. This can be achieved through:\nPruning – Removing unnecessary weights and neurons that contribute little to the model’s output. This reduces the number of computations needed during inference, improving speed without significantly affecting accuracy. Techniques include structured and unstructured pruning, where entire neurons or just individual weights are removed.\nQuantization – Lowering the precision of weights and activations, typically from 32-bit floating point (FP32) to 16-bit (FP16) or even 8-bit integers (INT8). Since lower precision numbers require fewer bits to store and process, inference can be significantly accelerated, especially on hardware optimized for integer operations like NVIDIA TensorRT or TensorFlow Lite.\nKnowledge Distillation – Training a smaller \u0026ldquo;student\u0026rdquo; model to mimic a larger \u0026ldquo;teacher\u0026rdquo; model. The student model learns to approximate the output of the more complex model, reducing computational overhead while maintaining accuracy. This is particularly useful for deploying models on edge devices or mobile applications.\nLow-Rank Factorization – Decomposing large weight matrices into smaller, more efficient representations. By breaking down convolutions and fully connected layers into simpler operations, low-rank factorization can reduce the number of multiplications required, speeding up inference while preserving most of the original model\u0026rsquo;s expressiveness.\n2. Graph \u0026amp; Operator Optimization: Speeding Up Computation Many deep learning frameworks support graph optimizations that fuse or restructure operations for efficiency. These techniques make computations more efficient by reducing redundant operations:\nGraph Fusion – Merging multiple operations into a single, optimized kernel. For example, in deep learning frameworks like TensorFlow and PyTorch, a convolution followed by a batch normalization operation can be fused into a single computation step, reducing memory access overhead and speeding up execution.\nONNX \u0026amp; TorchScript Optimization – Converting models into an optimized intermediate representation like ONNX (Open Neural Network Exchange) or TorchScript can allow further graph-level optimizations and compatibility with efficient runtime engines like ONNX Runtime and TensorRT.\nXLA (Accelerated Linear Algebra) – An optimization framework used in TensorFlow and JAX that compiles deep learning models into highly efficient computation graphs, enabling faster execution by reducing redundant operations and improving memory locality.\n3. Hardware Acceleration: Making the Most of Your Device Neural networks can be significantly accelerated by optimizing for specific hardware capabilities. This involves choosing the right computing resources and leveraging hardware-specific optimizations:\nUsing Specialized Libraries – Libraries like NVIDIA\u0026rsquo;s cuDNN, Intel’s MKL-DNN, and OneDNN optimize matrix multiplications and convolutions to run efficiently on specific hardware. These backends take advantage of SIMD (Single Instruction, Multiple Data) and GPU tensor cores to maximize throughput.\nChoosing the Right Hardware – Depending on your workload, selecting the right processing unit can make a huge difference. GPUs excel at parallelized matrix computations, TPUs (Tensor Processing Units) are optimized for deep learning workloads, and CPUs can still be efficient for low-latency applications, especially with vectorized instructions.\nParallelization – Splitting computations across multiple processing units to improve efficiency. Data parallelism (splitting batches across devices), model parallelism (splitting layers across devices), and tensor parallelism (splitting tensors across devices) are all used in large-scale training and inference.\n4. Efficient Inference Engines: Deploying Models Faster Deep learning frameworks are often designed for flexibility, which can lead to inefficiencies during inference. Using optimized inference engines helps streamline execution:\nTensorRT – NVIDIA’s high-performance deep learning inference engine that applies layer fusion, precision calibration, and kernel tuning to maximize speed on GPUs. It’s widely used in production AI deployments, from self-driving cars to cloud AI.\nOpenVINO – Intel’s optimization framework designed for CPUs and specialized accelerators. It converts models into an intermediate representation optimized for low-latency inference, making it a good choice for deploying models on Intel hardware, including edge devices.\nTVM – An open-source deep learning compiler that enables automatic optimization of deep learning models across different hardware backends. It applies transformations like operator fusion and memory reuse to accelerate inference without modifying the original model.\nTFLite \u0026amp; ONNX Runtime – TensorFlow Lite is optimized for mobile and embedded devices, while ONNX Runtime provides accelerated inference for models converted to the ONNX format. These are crucial for deploying models on lightweight environments with constrained resources.\n5. Batch \u0026amp; Pipeline Optimization: Handling Data Efficiently Beyond optimizing the model itself, efficiently managing input data and execution pipelines is essential for real-time applications:\nDynamic vs. Static Batching – Static batching processes fixed-size input batches, which is faster but less flexible. Dynamic batching, on the other hand, groups incoming requests into batches in real-time, optimizing performance in production settings.\nPreloading \u0026amp; Caching – Data loading can become a bottleneck in high-performance systems. Using data caching and preloading techniques (e.g., TensorFlow\u0026rsquo;s tf.data API or PyTorch’s DataLoader) ensures that the model is never waiting for input data.\nMulti-threaded Execution – Running inference using multiple CPU or GPU threads allows models to process multiple requests in parallel, improving throughput. Frameworks like TensorFlow Serving and TorchServe optimize request handling using these techniques.\nConclusion Optimizing neural networks for speed involves a combination of compression, graph restructuring, hardware tuning, inference engine selection, and data pipeline optimizations. By applying these techniques, you can significantly accelerate inference time, reduce memory footprint, and deploy models efficiently across different platforms.\n","permalink":"https://arikpoz.github.io/posts/2025-03-30-how-to-make-your-neural-network-run-faster-an-overview-of-optimization-techniques/","summary":"\u003cp\u003e\u003cimg alt=\"Illustration of a neural network transforming into a faster, optimized version\" loading=\"lazy\" src=\"/posts/2025-03-30-how-to-make-your-neural-network-run-faster-an-overview-of-optimization-techniques/lead-image.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eNeural networks are becoming increasingly powerful, but speed remains a crucial factor in real-world applications. Whether you\u0026rsquo;re running models on the cloud, edge devices, or personal hardware, optimizing them for speed can lead to faster inference, lower latency, and reduced resource consumption.\u003c/p\u003e\n\u003cp\u003eIn this post, we\u0026rsquo;ll explore various techniques to accelerate neural networks, from model compression to hardware optimizations. This will serve as a foundation for future deep dives into each method.\u003c/p\u003e","title":"How to Make Your Neural Network Run Faster: An Overview of Optimization Techniques"}]