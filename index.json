[{"content":"\nIntroduction In this post, I will demonstrate how to use pruning to significantly reduce a model\u0026rsquo;s size and latency while maintaining minimal accuracy loss. In the example, we achieve a 90% reduction in model size and 5.5x faster inference time, all while preserving the same level of accuracy.\nWe will begin with a brief explanation of what pruning is and why it is important. Then, I‚Äôll provide a hands-on demonstration of applying pruning to a PyTorch model.\nOverview of Pruning Neural network pruning involves removing less important weights, channels, or neurons from a neural network to make it smaller and faster. The goal is to reduce computational costs (such as latency and memory usage) without significantly affecting model accuracy.\nDeep neural networks often contain a lot of redundancy. This redundancy arises because models are typically overparameterized to ensure high accuracy and generalization. During training, many parameters become co-dependent or have little impact on the final output. For example, multiple neurons may learn similar features, or certain filters may remain underutilized. This redundancy makes models robust but also bloated. Pruning helps streamline these models by eliminating parts that contribute the least to the output, resulting in a more efficient network that is easier to deploy on edge devices or in latency-sensitive applications.\nThere are two main types of pruning:\nUnstructured Pruning: Removes individual weights regardless of their position. While it can achieve high sparsity, it often requires specialized hardware or libraries to fully utilize the sparsity. Zeroing out individual weights typically does not improve latency because standard deep learning libraries use dense matrix multiplication regardless of how many weights are zeroed out. To benefit from sparsity, the model must be converted into a sparse format, which is often not well-supported by commodity hardware. In fact, these sparse representations can sometimes be slower than dense operations due to less optimized memory access patterns and lack of hardware acceleration. As a result, unstructured pruning offers theoretical compression but not always practical speedups unless carefully integrated into the deployment pipeline.\nStructured Pruning: Removes entire filters, channels, or layers, leading to real improvements in inference speed on standard hardware. Unlike unstructured pruning, which retains the original dense structure and thus doesn‚Äôt alter compute patterns, structured pruning directly reduces the dimensionality of tensors and layers. This means fewer floating-point operations (FLOPs) and less memory access, as the actual matrices involved in convolutions and linear operations are physically smaller. As a result, inference is faster and more efficient on standard hardware using optimized dense kernels, with no need for specialized sparse computation support.\nPyTorch Pruning API PyTorch provides a built-in pruning utility under torch.nn.utils.prune. This API supports both unstructured pruning (zeroing individual weights by magnitude or custom metrics) and structured pruning (removing entire channels or neurons). The PyTorch pruning tutorial offers a solid introduction using iterative magnitude pruning. However, it is important to note that the PyTorch pruning API does not result in real inference speedups out-of-the-box. This is because it primarily focuses on zeroing out weights rather than removing them. For unstructured pruning, it does not convert the model to a sparse representation, which is necessary to leverage computational gains. For structured pruning, it does not automatically modify the architecture to remove entire channels or filters, which means the computational graph remains unchanged.\nThat said, the PyTorch pruning API is a flexible and useful tool for experimenting with pruning strategies. It provides a simple interface to apply custom pruning criteria, evaluate sparsity effects, and implement iterative pruning and retraining loops. It is especially helpful for research and prototyping where exact hardware efficiency is less critical than functional model behavior.\nWhy Use Torch-Pruning? Structured pruning isn‚Äôt trivial. Removing a channel in one layer often requires modifying downstream layers. Structured pruning often involves complex inter-layer dependencies. For example, if you prune an output channel from a convolutional layer, any layer that consumes its output, such as a batch normalization layer or subsequent convolution, must also be updated to match the new shape. Managing these changes across many layers can be error-prone and tedious when done manually. Torch-Pruning solves this by introducing a graph-based algorithm called DepGraph, which automatically analyzes the model\u0026rsquo;s computation graph, identifies dependencies, and organizes pruning into safe and consistent execution plans.\nPractical Usage Example: Pruning ResNet-18 in PyTorch Let‚Äôs walk through pruning a ResNet-18 model step-by-step using torch-pruning. We\u0026rsquo;ll do this in Google Colab, so you can follow along easily. This example is adapted from the official README of Torch-Pruning.\nRun this code in Google Colab to try it yourself.\nSetup First, install the required library:\n!pip install torch-pruning Then, define the required imports:\nimport os import time import copy import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from torchvision import datasets, transforms, models from torch.utils.data import DataLoader import torch_pruning as tp device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) print(f\u0026#34;{device=}\u0026#34;) Get CIFAR-10 Train and Test Sets transform = transforms.Compose([ transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ]) train_loader = DataLoader( datasets.CIFAR10(root=\u0026#34;./data\u0026#34;, train=True, download=True, transform=transform), batch_size=128, shuffle=True ) test_loader = DataLoader( datasets.CIFAR10(root=\u0026#34;./data\u0026#34;, train=False, download=True, transform=transform), batch_size=256 ) Adjust ResNet-18 Network for CIFAR-10 Dataset def get_resnet18_for_cifar10(): model = models.resnet18(weights=None, num_classes=10) model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False) model.maxpool = nn.Identity() return model.to(device) full_model = get_resnet18_for_cifar10() Define Train and Evaluate Functions def train(model, loader, epochs, lr=0.01, save_path=\u0026#34;model.pth\u0026#34;, silent=False): if os.path.exists(save_path): if not silent: print(f\u0026#34;Model already trained. Loading from {save_path}\u0026#34;) model.load_state_dict(torch.load(save_path)) return criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9) model.train() for epoch in range(epochs): for x, y in loader: x, y = x.to(device), y.to(device) optimizer.zero_grad() loss = criterion(model(x), y) loss.backward() optimizer.step() if not silent: print(f\u0026#34;Epoch {epoch+1}: loss={loss.item():.4f}\u0026#34;) torch.save(model.state_dict(), save_path) if not silent: print(f\u0026#34;Training complete. Model saved to {save_path}\u0026#34;) def evaluate(model): model.eval() correct = total = 0 with torch.no_grad(): for x, y in test_loader: x, y = x.to(device), y.to(device) preds = model(x).argmax(1) correct += (preds == y).sum().item() total += y.size(0) return correct / total Define Helper Functions to Measure Latency class Timer: def __init__(self): self.use_cuda = torch.cuda.is_available() if self.use_cuda: self.starter = torch.cuda.Event(enable_timing=True) self.ender = torch.cuda.Event(enable_timing=True) def start(self): if self.use_cuda: self.starter.record() else: self.start_time = time.time() def stop(self): if self.use_cuda: self.ender.record() torch.cuda.synchronize() return self.starter.elapsed_time(self.ender) # ms else: return (time.time() - self.start_time) * 1000 # ms def estimate_latency(model, example_inputs, repetitions=50): timer = Timer() timings = np.zeros((repetitions, 1)) # Warm-up for _ in range(5): _ = model(example_inputs) with torch.no_grad(): for rep in range(repetitions): timer.start() _ = model(example_inputs) elapsed = timer.stop() timings[rep] = elapsed return np.mean(timings), np.std(timings) Train and Evaluate the Full Model train(full_model, train_loader, epochs=10, save_path=\u0026#34;full_model.pth\u0026#34;) accuracy_full = evaluate(full_model) example_input = torch.rand(128, 3, 32, 32).to(device) macs, parameters = tp.utils.count_ops_and_params(full_model, example_input) latency_mu, latency_std = estimate_latency(full_model, example_input) print(f\u0026#34;[full model] \\t\\tMACs: {macs/1e9:.2f} G, \\tParameters: {parameters/1e6:.2f} M, \\tLatency: {latency_mu:.2f} ¬± {latency_std:.2f} ms \\tAccuracy: {accuracy_full*100:.2f}%\u0026#34;) To save you some time, here are the results for the fully trained model:\n[full model] MACs: 0.56 G, Parameters: 11.17 M, Latency: 16.52 ¬± 0.03 ms Accuracy: 76.85% Prune by L2 Magnitude # Clone full model before pruning pruned_model = copy.deepcopy(full_model) pruned_model = pruned_model.to(device) # Set which layers to skip pruning. Important to keep the final classifier layer ignored_layers = [] for m in pruned_model.modules(): if isinstance(m, torch.nn.Linear) and m.out_features == 10: ignored_layers.append(m) # Iterative pruning iterative_steps = 20 pruner = tp.pruner.MagnitudePruner( model=pruned_model, example_inputs=example_input, importance=tp.importance.MagnitudeImportance(p=2), pruning_ratio=1, iterative_steps=iterative_steps, ignored_layers=ignored_layers, round_to=2, ) for iter in range(iterative_steps): # Prune pruner.step() # Evaluate after pruning acc_before = evaluate(pruned_model) # Fine-tune pruned model train(pruned_model, train_loader, epochs=1, save_path=f\u0026#34;pruned_model_{iter}.pth\u0026#34;, silent=True) # Evaluate after fine-tuning acc_after = evaluate(pruned_model) # Count MACs and parameters macs, parameters = tp.utils.count_ops_and_params(pruned_model, example_input) latency_mu, latency_std = estimate_latency(pruned_model, example_input) current_pruning_ratio = 1 / iterative_steps * (iter + 1) print(f\u0026#34;[pruned model] \\tPruning ratio: {current_pruning_ratio:.2f}, \\tMACs: {macs/1e9:.2f} G, \\tParameters: {parameters/1e6:.2f} M, \\tLatency: {latency_mu:.2f} ¬± {latency_std:.2f} ms \\tAccuracy pruned: {acc_before*100:.2f}%\\tFinetuned: {acc_after*100:.2f}%\u0026#34;) The pruning results show the model\u0026rsquo;s accuracy immediately after pruning and again after fine-tuning the smaller, pruned model. While accuracy initially drops following pruning, it recovers significantly after just one epoch of fine-tuning.\n[pruned model] Pruning ratio: 0.05, MACs: 0.49 G, Parameters: 10.03 M, Latency: 17.64 ¬± 0.04 ms Accuracy pruned: 63.60%\tFinetuned: 72.17% [pruned model] Pruning ratio: 0.10, MACs: 0.44 G, Parameters: 9.00 M, Latency: 16.12 ¬± 0.04 ms Accuracy pruned: 44.51%\tFinetuned: 76.51% [pruned model] Pruning ratio: 0.15, MACs: 0.40 G, Parameters: 8.01 M, Latency: 16.40 ¬± 0.04 ms Accuracy pruned: 66.98%\tFinetuned: 75.18% [pruned model] Pruning ratio: 0.20, MACs: 0.35 G, Parameters: 7.09 M, Latency: 16.33 ¬± 0.04 ms Accuracy pruned: 51.83%\tFinetuned: 74.64% [pruned model] Pruning ratio: 0.25, MACs: 0.31 G, Parameters: 6.29 M, Latency: 14.40 ¬± 0.05 ms Accuracy pruned: 63.51%\tFinetuned: 76.73% [pruned model] Pruning ratio: 0.30, MACs: 0.27 G, Parameters: 5.44 M, Latency: 14.07 ¬± 0.03 ms Accuracy pruned: 49.36%\tFinetuned: 74.64% [pruned model] Pruning ratio: 0.35, MACs: 0.23 G, Parameters: 4.69 M, Latency: 12.27 ¬± 0.03 ms Accuracy pruned: 58.74%\tFinetuned: 77.56% [pruned model] Pruning ratio: 0.40, MACs: 0.20 G, Parameters: 3.98 M, Latency: 12.28 ¬± 0.03 ms Accuracy pruned: 63.98%\tFinetuned: 78.29% [pruned model] Pruning ratio: 0.45, MACs: 0.16 G, Parameters: 3.34 M, Latency: 11.41 ¬± 0.02 ms Accuracy pruned: 45.66%\tFinetuned: 78.58% [pruned model] Pruning ratio: 0.50, MACs: 0.14 G, Parameters: 2.80 M, Latency: 7.06 ¬± 0.03 ms Accuracy pruned: 49.91%\tFinetuned: 72.77% [pruned model] Pruning ratio: 0.55, MACs: 0.11 G, Parameters: 2.24 M, Latency: 6.82 ¬± 0.05 ms Accuracy pruned: 38.72%\tFinetuned: 76.13% [pruned model] Pruning ratio: 0.60, MACs: 0.09 G, Parameters: 1.77 M, Latency: 5.96 ¬± 0.05 ms Accuracy pruned: 42.84%\tFinetuned: 79.09% [pruned model] Pruning ratio: 0.65, MACs: 0.07 G, Parameters: 1.34 M, Latency: 4.88 ¬± 0.09 ms Accuracy pruned: 33.88%\tFinetuned: 75.54% [pruned model] Pruning ratio: 0.70, MACs: 0.05 G, Parameters: 0.99 M, Latency: 4.17 ¬± 0.01 ms Accuracy pruned: 22.50%\tFinetuned: 75.60% [pruned model] Pruning ratio: 0.75, MACs: 0.04 G, Parameters: 0.70 M, Latency: 2.96 ¬± 0.08 ms Accuracy pruned: 34.23%\tFinetuned: 78.91% [pruned model] Pruning ratio: 0.80, MACs: 0.02 G, Parameters: 0.44 M, Latency: 2.70 ¬± 0.02 ms Accuracy pruned: 15.91%\tFinetuned: 75.55% [pruned model] Pruning ratio: 0.85, MACs: 0.01 G, Parameters: 0.25 M, Latency: 2.69 ¬± 0.04 ms Accuracy pruned: 14.16%\tFinetuned: 75.01% [pruned model] Pruning ratio: 0.90, MACs: 0.01 G, Parameters: 0.11 M, Latency: 2.63 ¬± 0.01 ms Accuracy pruned: 10.00%\tFinetuned: 68.87% [pruned model] Pruning ratio: 0.95, MACs: 0.00 G, Parameters: 0.03 M, Latency: 2.59 ¬± 0.02 ms Accuracy pruned: 10.00%\tFinetuned: 53.36% [pruned model] Pruning ratio: 1.00, MACs: 0.00 G, Parameters: 0.03 M, Latency: 2.57 ¬± 0.01 ms Accuracy pruned: 53.36%\tFinetuned: 54.91% Note that one of the final models achives same accuracy (even higher, 78.91%) while having 15x less parameters (0.7M vs. 11.17M), and is 5.5x faster than original (2.96 ms vs. 16.52 ms).\nSummary Pruning is a powerful technique to make deep networks lighter and faster. In this blog post, we:\nExplored what pruning is and why it matters Compared the native PyTorch pruning API with Torch-Pruning Used torch-pruning to prune a ResNet-18 model in PyTorch Evaluated model size, inference latency, and top-1 prediction accuracy using CIFAR-10 data By applying structured pruning, you can make your models more efficient with minimal impact on performance, a valuable step in any model optimization workflow.\n","permalink":"https://arikpoz.github.io/posts/2025-04-10-neural-network-pruning-how-to-accelerate-inference-with-minimal-accuracy-loss/","summary":"\u003cp\u003e\u003cimg alt=\"Cartoon of a person in an \u0026ldquo;NN Pruning\u0026rdquo; shirt trimming a large robot labeled \u0026ldquo;Neural Network\u0026rdquo; into a slim, fast robot labeled \u0026ldquo;Pruned Network.\u0026rdquo;\" loading=\"lazy\" src=\"/posts/2025-04-10-neural-network-pruning-how-to-accelerate-inference-with-minimal-accuracy-loss/lead-image.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this post, I will demonstrate how to use pruning to \u003cstrong\u003esignificantly reduce a model\u0026rsquo;s size and latency\u003c/strong\u003e while maintaining minimal accuracy loss. In the example, we achieve a \u003cstrong\u003e90% reduction in model size\u003c/strong\u003e and \u003cstrong\u003e5.5x faster inference time\u003c/strong\u003e, all while preserving the same level of accuracy.\u003c/p\u003e","title":"Neural Network Pruning: How to Accelerate Inference with Minimal Accuracy Loss"},{"content":"\nIntroduction In deep learning pipelines, especially those involving image data, data loading and preprocessing often become major bottlenecks. Traditionally, image decoding is performed using libraries like OpenCV or Pillow, which rely on CPU-based processing. After decoding, the data must be transferred to GPU memory for further operations. But what if the decoding process itself could be performed directly on the GPU? Could this lead to faster performance?\nIn this post, I demonstrate how to use nvImageCodec to achieve a 2.18x speedup in JPEG loading by decoding directly on the GPU. Learn more about nvImageCodec in its documentation or on GitHub.\nüîç What is nvImageCodec? nvImageCodec is a high-performance image codec optimized for GPU acceleration. It is designed for scenarios like model training and batch inference, where decoding thousands of images quickly is critical. The library supports decoding (bytes to pixels) and encoding (pixels to bytes) for various common image formats. However, not all formats are fully supported on the GPU. Some, like PNG and WebP, fall back to CPU-based decoding. Below is a summary of supported formats:\n‚úÖ Format Support: Format GPU Decode GPU Encode Notes JPEG ‚úÖ Yes ‚úÖ Yes Fastest, hardware-accelerated JPEG 2000 ‚úÖ Yes ‚úÖ Yes TIFF ‚úÖ Yes ‚ùå No (planned) CUDA decoder PNG ‚ùå No (planned) ‚ùå No (planned) CPU only WebP ‚ùå No ‚ùå No CPU only üåü What was Benchmarked? We compared the performance of:\nOpenCV: CPU-based decoding followed by PIL transformations. nvImageCodec: GPU-based decoding with tensor transformations. Benchmark Details: Dataset: 1000 JPEG images from the ImageNet Sample Images dataset (credit: Eli Schwartz). Model: ResNet18 for inference. Transform Pipeline: Resize and crop applied to all images. Each benchmark was run 10 times (plus 1 warmup iteration), and the average times were recorded for:\nüß™ Loading: Decoding, resizing, and tensor conversion. ‚ö° Inference: Model forward pass. ‚è±Ô∏è Total: Combined loading and inference time. All benchmarks were conducted in Google Colab using a T4 GPU instance.\nRun this code in Google Colab to try it yourself.\nüõ†Ô∏è Setup in Colab Install Dependencies and Load Dataset !pip install nvidia-nvimgcodec-cu11 opencv-python-headless !git clone https://github.com/EliSchwartz/imagenet-sample-images.git Prepare the Images import os, shutil from pathlib import Path source_dir = Path(\u0026#34;imagenet-sample-images\u0026#34;) dest_dir = Path(\u0026#34;benchmark_images\u0026#34;) dest_dir.mkdir(exist_ok=True) all_images = list(source_dir.glob(\u0026#34;*.JPEG\u0026#34;)) for img in all_images: shutil.copy(img, dest_dir / img.name) image_paths = sorted(list(dest_dir.glob(\u0026#34;*.JPEG\u0026#34;))) print(f\u0026#34;Prepared {len(image_paths)} images.\u0026#34;) Define Model and Preprocessing import torch import torchvision.transforms as transforms import torchvision.models as models from torchvision.transforms import Resize, CenterCrop device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = models.resnet18(pretrained=True).to(device).eval() transform = transforms.Compose([ Resize(256), CenterCrop(224), ]) üß≤ Benchmark Functions (10x Repeated Runs) OpenCV Benchmark def run_opencv_inference(image_paths, runs=10): import time, numpy as np from PIL import Image load_times, infer_times = [], [] for run_idx in range(runs + 1): imgs = [] t0 = time.time() for path in image_paths: img = cv2.imread(str(path)) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img = Image.fromarray(img) img = transform(img) img = transforms.ToTensor()(img) imgs.append(img) batch = torch.stack(imgs).to(device) load_time = time.time() - t0 t1 = time.time() with torch.no_grad(): model(batch) infer_time = time.time() - t1 if run_idx == 0: print(f\u0026#34;Run {run_idx + 1}: Warmup iteration (not included in mean). Loading Time = {load_time:.4f}s, Inference Time = {infer_time:.4f}s\u0026#34;) else: load_times.append(load_time) infer_times.append(infer_time) print(f\u0026#34;Run {run_idx + 1}: Loading Time = {load_time:.4f}s, Inference Time = {infer_time:.4f}s\u0026#34;) return np.mean(load_times), np.mean(infer_times) opencv_load, opencv_infer = run_opencv_inference(image_paths) nvImageCodec Benchmark def run_nvimagecodec_inference(image_paths, runs=10): import time, numpy as np decoder = nvimgcodec.Decoder(device_id=0) load_times, infer_times = [], [] for run_idx in range(runs + 1): imgs = [] t0 = time.time() for path in image_paths: with open(path, \u0026#39;rb\u0026#39;) as f: data = f.read() nv_img = decoder.decode(data) img = torch.as_tensor(nv_img.cuda()).permute(2, 0, 1).float().div(255) img = transform(img) imgs.append(img) batch = torch.stack(imgs).to(device) load_time = time.time() - t0 t1 = time.time() with torch.no_grad(): model(batch) infer_time = time.time() - t1 if run_idx == 0: print(f\u0026#34;Run {run_idx + 1}: Warmup iteration (not included in mean). Loading Time = {load_time:.4f}s, Inference Time = {infer_time:.4f}s\u0026#34;) else: load_times.append(load_time) infer_times.append(infer_time) print(f\u0026#34;Run {run_idx + 1}: Loading Time = {load_time:.4f}s, Inference Time = {infer_time:.4f}s\u0026#34;) return np.mean(load_times), np.mean(infer_times) nv_load, nv_infer = run_nvimagecodec_inference(image_paths) üìä Results \u0026amp; Visualization import pandas as pd import matplotlib.pyplot as plt results = pd.DataFrame({ \u0026#34;Method\u0026#34;: [\u0026#34;OpenCV\u0026#34;, \u0026#34;nvImageCodec\u0026#34;], \u0026#34;Loading Time (s)\u0026#34;: [opencv_load, nv_load], \u0026#34;Inference Time (s)\u0026#34;: [opencv_infer, nv_infer], \u0026#34;Total Time (s)\u0026#34;: [ opencv_load + opencv_infer, nv_load + nv_infer ], }) print(results) results.plot(x=\u0026#34;Method\u0026#34;, y=[\u0026#34;Loading Time (s)\u0026#34;, \u0026#34;Inference Time (s)\u0026#34;, \u0026#34;Total Time (s)\u0026#34;], kind=\u0026#34;bar\u0026#34;, figsize=(10, 6)) plt.title(\u0026#34;OpenCV vs. nvImageCodec on 1000 ImageNet JPEGs (10-run average)\u0026#34;) plt.ylabel(\u0026#34;Seconds\u0026#34;) plt.grid(True) plt.show() ‚úÖ Summary Method Loading Time (s) Inference Time (s) Total Time (s) OpenCV 6.08343 0.00349 6.08693 nvImageCodec 2.78262 0.00323 2.78585 By leveraging the T4 GPU, nvImageCodec achieves a 2.18x speedup in JPEG loading times by performing decoding directly on the GPU. This eliminates CPU bottlenecks and enables a more efficient data pipeline.\nFor workflows heavily reliant on JPEGs, integrating nvImageCodec into your training or inference pipeline can deliver substantial performance improvements with minimal effort.\nTip: Before integrating, ensure that loading time is indeed a bottleneck in your pipeline. For example, test by preloading a single image or skipping loading altogether to simulate random data. In training pipelines, prefetching images in parallel with GPU processing is also a common optimization strategy.\n","permalink":"https://arikpoz.github.io/posts/2025-04-07-fast-image-loading-with-nvidia-nvimagecodec/","summary":"\u003cp\u003e\u003cimg alt=\"OpenCV turtle loses the race with nvImageCodec rabbit\" loading=\"lazy\" src=\"/posts/2025-04-07-fast-image-loading-with-nvidia-nvimagecodec/lead-image.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eIn deep learning pipelines, especially those involving image data, \u003cstrong\u003edata loading and preprocessing\u003c/strong\u003e often become major bottlenecks. Traditionally, image decoding is performed using libraries like \u003ca href=\"https://docs.opencv.org/4.x/index.html\"\u003eOpenCV\u003c/a\u003e or \u003ca href=\"https://pillow.readthedocs.io/en/stable/\"\u003ePillow\u003c/a\u003e, which rely on CPU-based processing. After decoding, the data must be transferred to GPU memory for further operations. But what if the decoding process itself could be performed directly on the GPU? Could this lead to faster performance?\u003c/p\u003e","title":"Fast Image Loading with NVIDIA nvImageCodec"},{"content":"\nIntroduction Deep learning models have grown increasingly large and complex, enabling state-of-the-art performance in tasks such as image recognition, natural language processing, and generative AI. However, these large models often come with high computational costs, making them slow to run on edge devices, embedded systems, or even in cloud environments with strict latency requirements.\nModel compression techniques aim to reduce the size and computational requirements of neural networks while maintaining their accuracy. This enables faster inference, lower power consumption, and better deployment flexibility. In this post, we‚Äôll explore why model compression is essential and provide an overview of four key techniques: pruning, quantization, knowledge distillation, and low-rank factorization.\nWhy Compress Neural Networks? Compression is not just about saving memory - it significantly improves inference speed and enables deployment on a wider range of hardware. Here are some key benefits:\nFaster Inference: Smaller models require fewer computations, reducing latency in real-time applications. Lower Memory Footprint: Compressed models take up less storage, making them ideal for mobile and edge devices. Reduced Power Consumption: Lower computation means lower energy usage, which is critical for battery-powered devices. Easier Deployment: Efficient models can be deployed on a broader range of hardware, including microcontrollers and IoT devices. Cost Savings: Running optimized models on lower-end hardware reduces infrastructure costs in cloud-based AI applications. Now, let\u0026rsquo;s explore the four primary methods of model compression.\n1. Pruning: Cutting Down Redundant Weights What is Pruning? Pruning removes unnecessary weights or neurons from a neural network, reducing its size without significantly impacting performance. The idea is that many parameters contribute little to the final output, and eliminating them can speed up computation.\nTypes of Pruning: Unstructured Pruning: Removes individual weights that have minimal impact on the network. Structured Pruning: Removes entire neurons, channels, or layers for a more hardware-friendly compression. Global vs. Layer-wise Pruning: Some methods prune across the entire model, while others prune within each layer independently. Use Cases and Benefits: Works well for over-parameterized models. Can be applied iteratively during training or post-training. Reduces memory usage and speeds up inference. 2. Quantization: Reducing Precision for Faster Computation What is Quantization? Quantization lowers the precision of a model‚Äôs weights and activations, reducing memory usage and enabling faster execution, particularly on specialized hardware like GPUs, TPUs, and mobile processors.\nTypes of Quantization: Post-Training Quantization: Converts a trained FP32 model to a lower precision (e.g., INT8) after training. Quantization-Aware Training: Trains the model with quantization effects simulated to minimize accuracy loss. Dynamic vs. Static Quantization: Determines whether quantization is applied per batch dynamically or precomputed for inference. Use Cases and Benefits: Significant speedup on hardware optimized for lower precision (TensorRT, OpenVINO, TFLite). Works well for inference-time optimization. Reduces model size while maintaining accuracy in many cases. 3. Knowledge Distillation: Training Small Models Using Large Models What is Knowledge Distillation? Knowledge distillation trains a smaller ‚Äústudent‚Äù model to mimic the behavior of a larger ‚Äúteacher‚Äù model. Instead of learning directly from labeled data, the student learns from the teacher‚Äôs output distribution, capturing nuanced knowledge that direct training may miss.\nTypes of Knowledge Distillation: Logit-based Distillation: The student learns from the softened output probabilities of the teacher. Feature-based Distillation: The student mimics intermediate feature representations from the teacher. Self-Distillation: A single model is trained in stages, where later iterations learn from earlier iterations. Use Cases and Benefits: Enables smaller models to achieve near teacher-level accuracy. Useful for transferring knowledge from large pretrained models (e.g., BERT ‚Üí DistilBERT). Can be used in conjunction with other compression techniques. 4. Low-Rank Factorization: Decomposing Weights for Efficiency What is Low-Rank Factorization? Low-rank factorization techniques decompose large weight matrices into smaller ones that approximate the original matrix, reducing computational cost without major accuracy loss.\nMethods of Factorization: Singular Value Decomposition (SVD): Breaks down weight matrices into simpler components. Tensor Decomposition: Extends matrix factorization to multi-dimensional tensors for convolutional layers. Factorized Convolutions: Reduces convolutional kernel complexity (e.g., depthwise separable convolutions in MobileNet). Use Cases and Benefits: Particularly useful for CNNs and Transformer models. Reduces FLOPs (floating point operations) in matrix multiplications. Can be combined with pruning and quantization for additional gains. Choosing the Right Compression Method Each compression technique has trade-offs, and the best choice depends on your target hardware and application:\nCompression Method Best For Key Benefit Potential Drawback Pruning Over-parameterized models Reduces model size May require fine-tuning Quantization Hardware acceleration Significant speedup Some accuracy loss possible Knowledge Distillation Efficient small models Retains knowledge from large models Requires a good teacher model Low-Rank Factorization CNNs, Transformers Reduces computation Approximation can impact accuracy Conclusion Model compression is a critical step in optimizing deep learning models for speed and efficiency. Each method: pruning, quantization, knowledge distillation, and low-rank factorization, offers unique advantages depending on the application.\n","permalink":"https://arikpoz.github.io/posts/2025-04-02-introduction-to-model-compression-why-and-how-to-shrink-neural-networks-for-speed/","summary":"\u003cp\u003e\u003cimg alt=\"Glowing digital brain shrinking in size, symbolizing faster, smaller neural networks\" loading=\"lazy\" src=\"/posts/2025-04-02-introduction-to-model-compression-why-and-how-to-shrink-neural-networks-for-speed/lead-image.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eDeep learning models have grown increasingly large and complex, enabling state-of-the-art performance in tasks such as image recognition, natural language processing, and generative AI. However, these large models often come with high computational costs, making them slow to run on edge devices, embedded systems, or even in cloud environments with strict latency requirements.\u003c/p\u003e\n\u003cp\u003eModel compression techniques aim to reduce the size and computational requirements of neural networks while maintaining their accuracy. This enables faster inference, lower power consumption, and better deployment flexibility. In this post, we‚Äôll explore why model compression is essential and provide an overview of four key techniques: \u003cstrong\u003epruning, quantization, knowledge distillation, and low-rank factorization\u003c/strong\u003e.\u003c/p\u003e","title":"Introduction to Model Compression: Why and How to Shrink Neural Networks for Speed"},{"content":"\nIntroduction Neural networks are becoming increasingly powerful, but speed remains a crucial factor in real-world applications. Whether you\u0026rsquo;re running models on the cloud, edge devices, or personal hardware, optimizing them for speed can lead to faster inference, lower latency, and reduced resource consumption.\nIn this post, we\u0026rsquo;ll explore various techniques to accelerate neural networks, from model compression to hardware optimizations. This will serve as a foundation for future deep dives into each method.\n1. Model Compression: Shrinking the Network Without Losing Power One of the most effective ways to speed up a neural network is by reducing its size while maintaining performance. This can be achieved through:\nPruning ‚Äì Removing unnecessary weights and neurons that contribute little to the model‚Äôs output. This reduces the number of computations needed during inference, improving speed without significantly affecting accuracy. Techniques include structured and unstructured pruning, where entire neurons or just individual weights are removed.\nQuantization ‚Äì Lowering the precision of weights and activations, typically from 32-bit floating point (FP32) to 16-bit (FP16) or even 8-bit integers (INT8). Since lower precision numbers require fewer bits to store and process, inference can be significantly accelerated, especially on hardware optimized for integer operations like NVIDIA TensorRT or TensorFlow Lite.\nKnowledge Distillation ‚Äì Training a smaller \u0026ldquo;student\u0026rdquo; model to mimic a larger \u0026ldquo;teacher\u0026rdquo; model. The student model learns to approximate the output of the more complex model, reducing computational overhead while maintaining accuracy. This is particularly useful for deploying models on edge devices or mobile applications.\nLow-Rank Factorization ‚Äì Decomposing large weight matrices into smaller, more efficient representations. By breaking down convolutions and fully connected layers into simpler operations, low-rank factorization can reduce the number of multiplications required, speeding up inference while preserving most of the original model\u0026rsquo;s expressiveness.\n2. Graph \u0026amp; Operator Optimization: Speeding Up Computation Many deep learning frameworks support graph optimizations that fuse or restructure operations for efficiency. These techniques make computations more efficient by reducing redundant operations:\nGraph Fusion ‚Äì Merging multiple operations into a single, optimized kernel. For example, in deep learning frameworks like TensorFlow and PyTorch, a convolution followed by a batch normalization operation can be fused into a single computation step, reducing memory access overhead and speeding up execution.\nONNX \u0026amp; TorchScript Optimization ‚Äì Converting models into an optimized intermediate representation like ONNX (Open Neural Network Exchange) or TorchScript can allow further graph-level optimizations and compatibility with efficient runtime engines like ONNX Runtime and TensorRT.\nXLA (Accelerated Linear Algebra) ‚Äì An optimization framework used in TensorFlow and JAX that compiles deep learning models into highly efficient computation graphs, enabling faster execution by reducing redundant operations and improving memory locality.\n3. Hardware Acceleration: Making the Most of Your Device Neural networks can be significantly accelerated by optimizing for specific hardware capabilities. This involves choosing the right computing resources and leveraging hardware-specific optimizations:\nUsing Specialized Libraries ‚Äì Libraries like NVIDIA\u0026rsquo;s cuDNN, Intel‚Äôs MKL-DNN, and OneDNN optimize matrix multiplications and convolutions to run efficiently on specific hardware. These backends take advantage of SIMD (Single Instruction, Multiple Data) and GPU tensor cores to maximize throughput.\nChoosing the Right Hardware ‚Äì Depending on your workload, selecting the right processing unit can make a huge difference. GPUs excel at parallelized matrix computations, TPUs (Tensor Processing Units) are optimized for deep learning workloads, and CPUs can still be efficient for low-latency applications, especially with vectorized instructions.\nParallelization ‚Äì Splitting computations across multiple processing units to improve efficiency. Data parallelism (splitting batches across devices), model parallelism (splitting layers across devices), and tensor parallelism (splitting tensors across devices) are all used in large-scale training and inference.\n4. Efficient Inference Engines: Deploying Models Faster Deep learning frameworks are often designed for flexibility, which can lead to inefficiencies during inference. Using optimized inference engines helps streamline execution:\nTensorRT ‚Äì NVIDIA‚Äôs high-performance deep learning inference engine that applies layer fusion, precision calibration, and kernel tuning to maximize speed on GPUs. It‚Äôs widely used in production AI deployments, from self-driving cars to cloud AI.\nOpenVINO ‚Äì Intel‚Äôs optimization framework designed for CPUs and specialized accelerators. It converts models into an intermediate representation optimized for low-latency inference, making it a good choice for deploying models on Intel hardware, including edge devices.\nTVM ‚Äì An open-source deep learning compiler that enables automatic optimization of deep learning models across different hardware backends. It applies transformations like operator fusion and memory reuse to accelerate inference without modifying the original model.\nTFLite \u0026amp; ONNX Runtime ‚Äì TensorFlow Lite is optimized for mobile and embedded devices, while ONNX Runtime provides accelerated inference for models converted to the ONNX format. These are crucial for deploying models on lightweight environments with constrained resources.\n5. Batch \u0026amp; Pipeline Optimization: Handling Data Efficiently Beyond optimizing the model itself, efficiently managing input data and execution pipelines is essential for real-time applications:\nDynamic vs. Static Batching ‚Äì Static batching processes fixed-size input batches, which is faster but less flexible. Dynamic batching, on the other hand, groups incoming requests into batches in real-time, optimizing performance in production settings.\nPreloading \u0026amp; Caching ‚Äì Data loading can become a bottleneck in high-performance systems. Using data caching and preloading techniques (e.g., TensorFlow\u0026rsquo;s tf.data API or PyTorch‚Äôs DataLoader) ensures that the model is never waiting for input data.\nMulti-threaded Execution ‚Äì Running inference using multiple CPU or GPU threads allows models to process multiple requests in parallel, improving throughput. Frameworks like TensorFlow Serving and TorchServe optimize request handling using these techniques.\nConclusion Optimizing neural networks for speed involves a combination of compression, graph restructuring, hardware tuning, inference engine selection, and data pipeline optimizations. By applying these techniques, you can significantly accelerate inference time, reduce memory footprint, and deploy models efficiently across different platforms.\n","permalink":"https://arikpoz.github.io/posts/2025-03-30-how-to-make-your-neural-network-run-faster-an-overview-of-optimization-techniques/","summary":"\u003cp\u003e\u003cimg alt=\"Illustration of a neural network transforming into a faster, optimized version\" loading=\"lazy\" src=\"/posts/2025-03-30-how-to-make-your-neural-network-run-faster-an-overview-of-optimization-techniques/lead-image.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eNeural networks are becoming increasingly powerful, but speed remains a crucial factor in real-world applications. Whether you\u0026rsquo;re running models on the cloud, edge devices, or personal hardware, optimizing them for speed can lead to faster inference, lower latency, and reduced resource consumption.\u003c/p\u003e\n\u003cp\u003eIn this post, we\u0026rsquo;ll explore various techniques to accelerate neural networks, from model compression to hardware optimizations. This will serve as a foundation for future deep dives into each method.\u003c/p\u003e","title":"How to Make Your Neural Network Run Faster: An Overview of Optimization Techniques"}]